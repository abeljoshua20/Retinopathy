{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:37:54.689774Z",
     "start_time": "2021-01-11T06:37:54.687296Z"
    }
   },
   "outputs": [],
   "source": [
    "#Temporary for Kisko Usage\n",
    "settings_aws = {'key': 'AKIAT5B24LA5P7AA26F5',\n",
    "           'secret':'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'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:37:57.488059Z",
     "start_time": "2021-01-11T06:37:54.694398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras.applications.inception_resnet_v2 as inception_resnet\n",
    "from keras import backend\n",
    "from keras.models import Model, Input, Sequential, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Input, Conv2D\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau)\n",
    "from PIL import Image\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:37:57.491148Z",
     "start_time": "2021-01-11T06:37:57.489463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "# print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "# gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "# print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:37:57.503528Z",
     "start_time": "2021-01-11T06:37:57.492740Z"
    }
   },
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:37:57.508565Z",
     "start_time": "2021-01-11T06:37:57.504735Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display, clear_output\n",
    "# from IPython.display import Audio\n",
    "# from IPython.core.display import HTML\n",
    "# import numpy as np\n",
    "\n",
    "# def alert(duration=2):\n",
    "#     \"\"\" makes sound on client using javascript (works with remote server) \"\"\"      \n",
    "#     framerate = 44100\n",
    "#     freq=300\n",
    "#     t = np.linspace(0,duration,framerate*duration)\n",
    "#     data = np.sin(2*np.pi*freq*t)\n",
    "#     display(Audio(data,rate=framerate, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:37:57.513871Z",
     "start_time": "2021-01-11T06:37:57.509591Z"
    }
   },
   "outputs": [],
   "source": [
    "BUCKET = 'kapeles'\n",
    "DATA_PATH ='BDCC_Dataset/downloads/manual'\n",
    "TRAIN_PATH =  os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "SAMPLE_PATH = os.path.join(DATA_PATH, 'sample')\n",
    "\n",
    "VECTOR_PATH = 'BDCC_Dataset/inception_resnet_vectors'\n",
    "TARGET_SIZE = (1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract image vectors based on InceptionResNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inception Resnset V2 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:06.237541Z",
     "start_time": "2021-01-11T06:37:57.514897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_resnet_v2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1024, 1024,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 511, 511, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 511, 511, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 511, 511, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 509, 509, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 509, 509, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 509, 509, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 509, 509, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 509, 509, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 509, 509, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 254, 254, 64) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 254, 254, 80) 5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 254, 254, 80) 240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 254, 254, 80) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 252, 252, 192 138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 252, 252, 192 576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 252, 252, 192 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 125, 125, 192 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 125, 125, 64) 12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 125, 125, 64) 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 125, 125, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 125, 125, 48) 9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 125, 125, 96) 55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 125, 125, 48) 144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 125, 125, 96) 288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 125, 125, 48) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 125, 125, 96) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 125, 125, 192 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 125, 125, 96) 18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 125, 125, 64) 76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 125, 125, 96) 82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 125, 125, 64) 12288       average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 125, 125, 96) 288         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 125, 125, 64) 192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 125, 125, 96) 288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 125, 125, 64) 192         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 125, 125, 96) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 125, 125, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 125, 125, 96) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 125, 125, 64) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed_5b (Concatenate)          (None, 125, 125, 320 0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 125, 125, 32) 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 125, 125, 32) 96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 125, 125, 32) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 125, 125, 32) 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 125, 125, 48) 13824       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 125, 125, 32) 96          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 125, 125, 48) 144         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 125, 125, 32) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 125, 125, 48) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 125, 125, 32) 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 125, 125, 32) 9216        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 125, 125, 64) 27648       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 125, 125, 32) 96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 125, 125, 32) 96          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 125, 125, 64) 192         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 125, 125, 32) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 125, 125, 32) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 125, 125, 64) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_mixed (Concatenate)   (None, 125, 125, 128 0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_conv (Conv2D)         (None, 125, 125, 320 41280       block35_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_1 (Lambda)              (None, 125, 125, 320 0           mixed_5b[0][0]                   \n",
      "                                                                 block35_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_ac (Activation)       (None, 125, 125, 320 0           block35_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 125, 125, 32) 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 125, 125, 32) 96          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 125, 125, 32) 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 125, 125, 32) 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 125, 125, 48) 13824       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 125, 125, 32) 96          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 125, 125, 48) 144         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 125, 125, 32) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 125, 125, 48) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 125, 125, 32) 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 125, 125, 32) 9216        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 125, 125, 64) 27648       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 125, 125, 32) 96          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 125, 125, 32) 96          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 125, 125, 64) 192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 125, 125, 32) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 125, 125, 32) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 125, 125, 64) 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_mixed (Concatenate)   (None, 125, 125, 128 0           activation_18[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_conv (Conv2D)         (None, 125, 125, 320 41280       block35_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_2 (Lambda)              (None, 125, 125, 320 0           block35_1_ac[0][0]               \n",
      "                                                                 block35_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_ac (Activation)       (None, 125, 125, 320 0           block35_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 125, 125, 32) 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 125, 125, 32) 96          conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 125, 125, 32) 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 125, 125, 32) 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 125, 125, 48) 13824       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 125, 125, 32) 96          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 125, 125, 48) 144         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 125, 125, 32) 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 125, 125, 48) 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 125, 125, 32) 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 125, 125, 32) 9216        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 125, 125, 64) 27648       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 125, 125, 32) 96          conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 125, 125, 32) 96          conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 125, 125, 64) 192         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 125, 125, 32) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 125, 125, 32) 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 125, 125, 64) 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_mixed (Concatenate)   (None, 125, 125, 128 0           activation_24[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_conv (Conv2D)         (None, 125, 125, 320 41280       block35_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_3 (Lambda)              (None, 125, 125, 320 0           block35_2_ac[0][0]               \n",
      "                                                                 block35_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_ac (Activation)       (None, 125, 125, 320 0           block35_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 125, 125, 32) 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 125, 125, 32) 96          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 125, 125, 32) 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 125, 125, 32) 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 125, 125, 48) 13824       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 125, 125, 32) 96          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 125, 125, 48) 144         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 125, 125, 32) 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 125, 125, 48) 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 125, 125, 32) 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 125, 125, 32) 9216        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 125, 125, 64) 27648       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 125, 125, 32) 96          conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 125, 125, 32) 96          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 125, 125, 64) 192         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 125, 125, 32) 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 125, 125, 32) 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 125, 125, 64) 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_mixed (Concatenate)   (None, 125, 125, 128 0           activation_30[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "                                                                 activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_conv (Conv2D)         (None, 125, 125, 320 41280       block35_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_4 (Lambda)              (None, 125, 125, 320 0           block35_3_ac[0][0]               \n",
      "                                                                 block35_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_ac (Activation)       (None, 125, 125, 320 0           block35_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 125, 125, 32) 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 125, 125, 32) 96          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 125, 125, 32) 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 125, 125, 32) 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 125, 125, 48) 13824       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 125, 125, 32) 96          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 125, 125, 48) 144         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 125, 125, 32) 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 125, 125, 48) 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 125, 125, 32) 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 125, 125, 32) 9216        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 125, 125, 64) 27648       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 125, 125, 32) 96          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 125, 125, 32) 96          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 125, 125, 64) 192         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 125, 125, 32) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 125, 125, 32) 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 125, 125, 64) 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_mixed (Concatenate)   (None, 125, 125, 128 0           activation_36[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_conv (Conv2D)         (None, 125, 125, 320 41280       block35_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_5 (Lambda)              (None, 125, 125, 320 0           block35_4_ac[0][0]               \n",
      "                                                                 block35_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_ac (Activation)       (None, 125, 125, 320 0           block35_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 125, 125, 32) 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 125, 125, 32) 96          conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 125, 125, 32) 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 125, 125, 32) 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 125, 125, 48) 13824       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 125, 125, 32) 96          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 125, 125, 48) 144         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 125, 125, 32) 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 125, 125, 48) 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 125, 125, 32) 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 125, 125, 32) 9216        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 125, 125, 64) 27648       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 125, 125, 32) 96          conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 125, 125, 32) 96          conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 125, 125, 64) 192         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 125, 125, 32) 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 125, 125, 32) 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 125, 125, 64) 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_mixed (Concatenate)   (None, 125, 125, 128 0           activation_42[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_conv (Conv2D)         (None, 125, 125, 320 41280       block35_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_6 (Lambda)              (None, 125, 125, 320 0           block35_5_ac[0][0]               \n",
      "                                                                 block35_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_ac (Activation)       (None, 125, 125, 320 0           block35_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 125, 125, 32) 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 125, 125, 32) 96          conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 125, 125, 32) 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 125, 125, 32) 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 125, 125, 48) 13824       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 125, 125, 32) 96          conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 125, 125, 48) 144         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 125, 125, 32) 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 125, 125, 48) 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 125, 125, 32) 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 125, 125, 32) 9216        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 125, 125, 64) 27648       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 125, 125, 32) 96          conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 125, 125, 32) 96          conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 125, 125, 64) 192         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 125, 125, 32) 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 125, 125, 32) 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 125, 125, 64) 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_mixed (Concatenate)   (None, 125, 125, 128 0           activation_48[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_conv (Conv2D)         (None, 125, 125, 320 41280       block35_7_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_7 (Lambda)              (None, 125, 125, 320 0           block35_6_ac[0][0]               \n",
      "                                                                 block35_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_ac (Activation)       (None, 125, 125, 320 0           block35_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 125, 125, 32) 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 125, 125, 32) 96          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 125, 125, 32) 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 125, 125, 32) 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 125, 125, 48) 13824       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 125, 125, 32) 96          conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 125, 125, 48) 144         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 125, 125, 32) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 125, 125, 48) 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 125, 125, 32) 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 125, 125, 32) 9216        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 125, 125, 64) 27648       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 125, 125, 32) 96          conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 125, 125, 32) 96          conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 125, 125, 64) 192         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 125, 125, 32) 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 125, 125, 32) 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 125, 125, 64) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_mixed (Concatenate)   (None, 125, 125, 128 0           activation_54[0][0]              \n",
      "                                                                 activation_56[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_conv (Conv2D)         (None, 125, 125, 320 41280       block35_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_8 (Lambda)              (None, 125, 125, 320 0           block35_7_ac[0][0]               \n",
      "                                                                 block35_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_ac (Activation)       (None, 125, 125, 320 0           block35_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 125, 125, 32) 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 125, 125, 32) 96          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 125, 125, 32) 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 125, 125, 32) 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 125, 125, 48) 13824       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 125, 125, 32) 96          conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 125, 125, 48) 144         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 125, 125, 32) 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 125, 125, 48) 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 125, 125, 32) 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 125, 125, 32) 9216        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 125, 125, 64) 27648       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 125, 125, 32) 96          conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 125, 125, 32) 96          conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 125, 125, 64) 192         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 125, 125, 32) 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 125, 125, 32) 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 125, 125, 64) 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_mixed (Concatenate)   (None, 125, 125, 128 0           activation_60[0][0]              \n",
      "                                                                 activation_62[0][0]              \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_conv (Conv2D)         (None, 125, 125, 320 41280       block35_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_9 (Lambda)              (None, 125, 125, 320 0           block35_8_ac[0][0]               \n",
      "                                                                 block35_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_ac (Activation)       (None, 125, 125, 320 0           block35_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 125, 125, 32) 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 125, 125, 32) 96          conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 125, 125, 32) 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 125, 125, 32) 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 125, 125, 48) 13824       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 125, 125, 32) 96          conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 125, 125, 48) 144         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 125, 125, 32) 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 125, 125, 48) 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 125, 125, 32) 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 125, 125, 32) 9216        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 125, 125, 64) 27648       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 125, 125, 32) 96          conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 125, 125, 32) 96          conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 125, 125, 64) 192         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 125, 125, 32) 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 125, 125, 32) 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 125, 125, 64) 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_mixed (Concatenate)  (None, 125, 125, 128 0           activation_66[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_conv (Conv2D)        (None, 125, 125, 320 41280       block35_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block35_10 (Lambda)             (None, 125, 125, 320 0           block35_9_ac[0][0]               \n",
      "                                                                 block35_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_ac (Activation)      (None, 125, 125, 320 0           block35_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 125, 125, 256 81920       block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 125, 125, 256 768         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 125, 125, 256 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 125, 125, 256 589824      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 125, 125, 256 768         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 125, 125, 256 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 62, 62, 384)  1105920     block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 62, 62, 384)  884736      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 62, 62, 384)  1152        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 62, 62, 384)  1152        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 62, 62, 384)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 62, 62, 384)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 62, 62, 320)  0           block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mixed_6a (Concatenate)          (None, 62, 62, 1088) 0           activation_72[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 62, 62, 128)  139264      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 62, 62, 128)  384         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 62, 62, 128)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 62, 62, 160)  143360      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 62, 62, 160)  480         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 62, 62, 160)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 62, 62, 192)  208896      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 62, 62, 192)  215040      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 62, 62, 192)  576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 62, 62, 192)  576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 62, 62, 192)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 62, 62, 192)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_76[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_1 (Lambda)              (None, 62, 62, 1088) 0           mixed_6a[0][0]                   \n",
      "                                                                 block17_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_ac (Activation)       (None, 62, 62, 1088) 0           block17_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 62, 62, 128)  139264      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 62, 62, 128)  384         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 62, 62, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 62, 62, 160)  143360      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 62, 62, 160)  480         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 62, 62, 160)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 62, 62, 192)  208896      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 62, 62, 192)  215040      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 62, 62, 192)  576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 62, 62, 192)  576         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 62, 62, 192)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 62, 62, 192)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_80[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_2 (Lambda)              (None, 62, 62, 1088) 0           block17_1_ac[0][0]               \n",
      "                                                                 block17_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_ac (Activation)       (None, 62, 62, 1088) 0           block17_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 62, 62, 128)  139264      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 62, 62, 128)  384         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 62, 62, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 62, 62, 160)  143360      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 62, 62, 160)  480         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 62, 62, 160)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 62, 62, 192)  208896      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 62, 62, 192)  215040      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 62, 62, 192)  576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 62, 62, 192)  576         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 62, 62, 192)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 62, 62, 192)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_84[0][0]              \n",
      "                                                                 activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_3 (Lambda)              (None, 62, 62, 1088) 0           block17_2_ac[0][0]               \n",
      "                                                                 block17_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_ac (Activation)       (None, 62, 62, 1088) 0           block17_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 62, 62, 128)  139264      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 62, 62, 128)  384         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 62, 62, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 62, 62, 160)  143360      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 62, 62, 160)  480         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 62, 62, 160)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 62, 62, 192)  208896      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 62, 62, 192)  215040      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 62, 62, 192)  576         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 62, 62, 192)  576         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 62, 62, 192)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 62, 62, 192)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_88[0][0]              \n",
      "                                                                 activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_4 (Lambda)              (None, 62, 62, 1088) 0           block17_3_ac[0][0]               \n",
      "                                                                 block17_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_ac (Activation)       (None, 62, 62, 1088) 0           block17_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 62, 62, 128)  139264      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 62, 62, 128)  384         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 62, 62, 128)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 62, 62, 160)  143360      activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 62, 62, 160)  480         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 62, 62, 160)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 62, 62, 192)  208896      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 62, 62, 192)  215040      activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 62, 62, 192)  576         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 62, 62, 192)  576         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 62, 62, 192)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 62, 62, 192)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_92[0][0]              \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_5 (Lambda)              (None, 62, 62, 1088) 0           block17_4_ac[0][0]               \n",
      "                                                                 block17_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_ac (Activation)       (None, 62, 62, 1088) 0           block17_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 62, 62, 128)  139264      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 62, 62, 128)  384         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 62, 62, 128)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 62, 62, 160)  143360      activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 62, 62, 160)  480         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 62, 62, 160)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 62, 62, 192)  208896      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 62, 62, 192)  215040      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 62, 62, 192)  576         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 62, 62, 192)  576         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 62, 62, 192)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 62, 62, 192)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_96[0][0]              \n",
      "                                                                 activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_6 (Lambda)              (None, 62, 62, 1088) 0           block17_5_ac[0][0]               \n",
      "                                                                 block17_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_ac (Activation)       (None, 62, 62, 1088) 0           block17_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 62, 62, 128)  139264      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 62, 62, 128)  384         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 62, 62, 128)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 62, 62, 160)  143360      activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 62, 62, 160)  480         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 62, 62, 160)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 62, 62, 192)  208896      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 62, 62, 192)  215040      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 62, 62, 192)  576         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 62, 62, 192)  576         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 62, 62, 192)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 62, 62, 192)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_100[0][0]             \n",
      "                                                                 activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_7_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_7 (Lambda)              (None, 62, 62, 1088) 0           block17_6_ac[0][0]               \n",
      "                                                                 block17_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_ac (Activation)       (None, 62, 62, 1088) 0           block17_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 62, 62, 128)  139264      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 62, 62, 128)  384         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 62, 62, 128)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 62, 62, 160)  143360      activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 62, 62, 160)  480         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 62, 62, 160)  0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 62, 62, 192)  208896      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 62, 62, 192)  215040      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 62, 62, 192)  576         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 62, 62, 192)  576         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 62, 62, 192)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 62, 62, 192)  0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_104[0][0]             \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_8 (Lambda)              (None, 62, 62, 1088) 0           block17_7_ac[0][0]               \n",
      "                                                                 block17_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_ac (Activation)       (None, 62, 62, 1088) 0           block17_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 62, 62, 128)  139264      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 62, 62, 128)  384         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 62, 62, 128)  0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 62, 62, 160)  143360      activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 62, 62, 160)  480         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 62, 62, 160)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 62, 62, 192)  208896      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 62, 62, 192)  215040      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 62, 62, 192)  576         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 62, 62, 192)  576         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 62, 62, 192)  0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 62, 62, 192)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_mixed (Concatenate)   (None, 62, 62, 384)  0           activation_108[0][0]             \n",
      "                                                                 activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_conv (Conv2D)         (None, 62, 62, 1088) 418880      block17_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_9 (Lambda)              (None, 62, 62, 1088) 0           block17_8_ac[0][0]               \n",
      "                                                                 block17_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_ac (Activation)       (None, 62, 62, 1088) 0           block17_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 62, 62, 128)  139264      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 62, 62, 128)  384         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 62, 62, 128)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 62, 62, 160)  143360      activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 62, 62, 160)  480         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 62, 62, 160)  0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 62, 62, 192)  208896      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 62, 62, 192)  215040      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 62, 62, 192)  576         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 62, 62, 192)  576         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 62, 62, 192)  0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 62, 62, 192)  0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_112[0][0]             \n",
      "                                                                 activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_10 (Lambda)             (None, 62, 62, 1088) 0           block17_9_ac[0][0]               \n",
      "                                                                 block17_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_ac (Activation)      (None, 62, 62, 1088) 0           block17_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 62, 62, 128)  139264      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 62, 62, 128)  384         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 62, 62, 128)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 62, 62, 160)  143360      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 62, 62, 160)  480         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 62, 62, 160)  0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 62, 62, 192)  208896      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 62, 62, 192)  215040      activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 62, 62, 192)  576         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 62, 62, 192)  576         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 62, 62, 192)  0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 62, 62, 192)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_116[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_11_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_11 (Lambda)             (None, 62, 62, 1088) 0           block17_10_ac[0][0]              \n",
      "                                                                 block17_11_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_ac (Activation)      (None, 62, 62, 1088) 0           block17_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 62, 62, 128)  139264      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 62, 62, 128)  384         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 62, 62, 128)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 62, 62, 160)  143360      activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 62, 62, 160)  480         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 62, 62, 160)  0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 62, 62, 192)  208896      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 62, 62, 192)  215040      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 62, 62, 192)  576         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 62, 62, 192)  576         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 62, 62, 192)  0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 62, 62, 192)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_120[0][0]             \n",
      "                                                                 activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_12_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_12 (Lambda)             (None, 62, 62, 1088) 0           block17_11_ac[0][0]              \n",
      "                                                                 block17_12_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_ac (Activation)      (None, 62, 62, 1088) 0           block17_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 62, 62, 128)  139264      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 62, 62, 128)  384         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 62, 62, 128)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 62, 62, 160)  143360      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 62, 62, 160)  480         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 62, 62, 160)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 62, 62, 192)  208896      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 62, 62, 192)  215040      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 62, 62, 192)  576         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 62, 62, 192)  576         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 62, 62, 192)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 62, 62, 192)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_124[0][0]             \n",
      "                                                                 activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_13_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_13 (Lambda)             (None, 62, 62, 1088) 0           block17_12_ac[0][0]              \n",
      "                                                                 block17_13_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_ac (Activation)      (None, 62, 62, 1088) 0           block17_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 62, 62, 128)  139264      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 62, 62, 128)  384         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 62, 62, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 62, 62, 160)  143360      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 62, 62, 160)  480         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 62, 62, 160)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 62, 62, 192)  208896      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 62, 62, 192)  215040      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 62, 62, 192)  576         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 62, 62, 192)  576         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 62, 62, 192)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 62, 62, 192)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_128[0][0]             \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_14_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_14 (Lambda)             (None, 62, 62, 1088) 0           block17_13_ac[0][0]              \n",
      "                                                                 block17_14_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_ac (Activation)      (None, 62, 62, 1088) 0           block17_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 62, 62, 128)  139264      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 62, 62, 128)  384         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 62, 62, 128)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 62, 62, 160)  143360      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 62, 62, 160)  480         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 62, 62, 160)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 62, 62, 192)  208896      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 62, 62, 192)  215040      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 62, 62, 192)  576         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 62, 62, 192)  576         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 62, 62, 192)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 62, 62, 192)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_132[0][0]             \n",
      "                                                                 activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_15_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_15 (Lambda)             (None, 62, 62, 1088) 0           block17_14_ac[0][0]              \n",
      "                                                                 block17_15_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_ac (Activation)      (None, 62, 62, 1088) 0           block17_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 62, 62, 128)  139264      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 62, 62, 128)  384         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 62, 62, 128)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 62, 62, 160)  143360      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 62, 62, 160)  480         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 62, 62, 160)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 62, 62, 192)  208896      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 62, 62, 192)  215040      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 62, 62, 192)  576         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 62, 62, 192)  576         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 62, 62, 192)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 62, 62, 192)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_136[0][0]             \n",
      "                                                                 activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_16_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_16 (Lambda)             (None, 62, 62, 1088) 0           block17_15_ac[0][0]              \n",
      "                                                                 block17_16_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_ac (Activation)      (None, 62, 62, 1088) 0           block17_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 62, 62, 128)  139264      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 62, 62, 128)  384         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 62, 62, 128)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 62, 62, 160)  143360      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 62, 62, 160)  480         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 62, 62, 160)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 62, 62, 192)  208896      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 62, 62, 192)  215040      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 62, 62, 192)  576         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 62, 62, 192)  576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 62, 62, 192)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 62, 62, 192)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_140[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_17_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_17 (Lambda)             (None, 62, 62, 1088) 0           block17_16_ac[0][0]              \n",
      "                                                                 block17_17_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_ac (Activation)      (None, 62, 62, 1088) 0           block17_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 62, 62, 128)  139264      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 62, 62, 128)  384         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 62, 62, 128)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 62, 62, 160)  143360      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 62, 62, 160)  480         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 62, 62, 160)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 62, 62, 192)  208896      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 62, 62, 192)  215040      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 62, 62, 192)  576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 62, 62, 192)  576         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 62, 62, 192)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 62, 62, 192)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_144[0][0]             \n",
      "                                                                 activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_18_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_18 (Lambda)             (None, 62, 62, 1088) 0           block17_17_ac[0][0]              \n",
      "                                                                 block17_18_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_ac (Activation)      (None, 62, 62, 1088) 0           block17_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 62, 62, 128)  139264      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 62, 62, 128)  384         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 62, 62, 128)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 62, 62, 160)  143360      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 62, 62, 160)  480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 62, 62, 160)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 62, 62, 192)  208896      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 62, 62, 192)  215040      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 62, 62, 192)  576         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 62, 62, 192)  576         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 62, 62, 192)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 62, 62, 192)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_148[0][0]             \n",
      "                                                                 activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_19_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_19 (Lambda)             (None, 62, 62, 1088) 0           block17_18_ac[0][0]              \n",
      "                                                                 block17_19_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_ac (Activation)      (None, 62, 62, 1088) 0           block17_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 62, 62, 128)  139264      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 62, 62, 128)  384         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 62, 62, 128)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 62, 62, 160)  143360      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 62, 62, 160)  480         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 62, 62, 160)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 62, 62, 192)  208896      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 62, 62, 192)  215040      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 62, 62, 192)  576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 62, 62, 192)  576         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 62, 62, 192)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 62, 62, 192)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_mixed (Concatenate)  (None, 62, 62, 384)  0           activation_152[0][0]             \n",
      "                                                                 activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_conv (Conv2D)        (None, 62, 62, 1088) 418880      block17_20_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_20 (Lambda)             (None, 62, 62, 1088) 0           block17_19_ac[0][0]              \n",
      "                                                                 block17_20_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_ac (Activation)      (None, 62, 62, 1088) 0           block17_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 62, 62, 256)  278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 62, 62, 256)  768         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 62, 62, 256)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 62, 62, 256)  278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 62, 62, 256)  278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 62, 62, 288)  663552      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 62, 62, 256)  768         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 62, 62, 256)  768         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 62, 62, 288)  864         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 62, 62, 256)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 62, 62, 256)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 62, 62, 288)  0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 30, 30, 384)  884736      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 30, 30, 288)  663552      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 30, 30, 320)  829440      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 30, 30, 384)  1152        conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 30, 30, 288)  864         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 30, 30, 320)  960         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 30, 30, 384)  0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 30, 30, 288)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 30, 30, 320)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 30, 30, 1088) 0           block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mixed_7a (Concatenate)          (None, 30, 30, 2080) 0           activation_157[0][0]             \n",
      "                                                                 activation_159[0][0]             \n",
      "                                                                 activation_162[0][0]             \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 30, 30, 192)  399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 30, 30, 192)  576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 30, 30, 192)  0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 30, 30, 224)  129024      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 30, 30, 224)  672         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 30, 30, 224)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 30, 30, 192)  399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 30, 30, 256)  172032      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 30, 30, 192)  576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 30, 30, 256)  768         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 30, 30, 192)  0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 30, 30, 256)  0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_163[0][0]             \n",
      "                                                                 activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_1_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1 (Lambda)               (None, 30, 30, 2080) 0           mixed_7a[0][0]                   \n",
      "                                                                 block8_1_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_ac (Activation)        (None, 30, 30, 2080) 0           block8_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 30, 30, 192)  399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 30, 30, 192)  576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 30, 30, 192)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 30, 30, 224)  129024      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 30, 30, 224)  672         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 30, 30, 224)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 30, 30, 192)  399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 30, 30, 256)  172032      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 30, 30, 192)  576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 30, 30, 256)  768         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 30, 30, 192)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 30, 30, 256)  0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_167[0][0]             \n",
      "                                                                 activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_2_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2 (Lambda)               (None, 30, 30, 2080) 0           block8_1_ac[0][0]                \n",
      "                                                                 block8_2_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_ac (Activation)        (None, 30, 30, 2080) 0           block8_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 30, 30, 192)  399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 30, 30, 192)  576         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 30, 30, 192)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 30, 30, 224)  129024      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 30, 30, 224)  672         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 30, 30, 224)  0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 30, 30, 192)  399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 30, 30, 256)  172032      activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 30, 30, 192)  576         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 30, 30, 256)  768         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 30, 30, 192)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 30, 30, 256)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_171[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_3_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3 (Lambda)               (None, 30, 30, 2080) 0           block8_2_ac[0][0]                \n",
      "                                                                 block8_3_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_ac (Activation)        (None, 30, 30, 2080) 0           block8_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 30, 30, 192)  399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 30, 30, 192)  576         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 30, 30, 192)  0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 30, 30, 224)  129024      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 30, 30, 224)  672         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 30, 30, 224)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 30, 30, 192)  399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 30, 30, 256)  172032      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 30, 30, 192)  576         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 30, 30, 256)  768         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 30, 30, 192)  0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 30, 30, 256)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_175[0][0]             \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_4_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4 (Lambda)               (None, 30, 30, 2080) 0           block8_3_ac[0][0]                \n",
      "                                                                 block8_4_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_ac (Activation)        (None, 30, 30, 2080) 0           block8_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 30, 30, 192)  399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 30, 30, 192)  576         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 30, 30, 192)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 30, 30, 224)  129024      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 30, 30, 224)  672         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 30, 30, 224)  0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 30, 30, 192)  399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 30, 30, 256)  172032      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 30, 30, 192)  576         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 30, 30, 256)  768         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 30, 30, 192)  0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 30, 30, 256)  0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_179[0][0]             \n",
      "                                                                 activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_5_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5 (Lambda)               (None, 30, 30, 2080) 0           block8_4_ac[0][0]                \n",
      "                                                                 block8_5_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_ac (Activation)        (None, 30, 30, 2080) 0           block8_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 30, 30, 192)  399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 30, 30, 192)  576         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 30, 30, 192)  0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 30, 30, 224)  129024      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 30, 30, 224)  672         conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 30, 30, 224)  0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 30, 30, 192)  399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 30, 30, 256)  172032      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 30, 30, 192)  576         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 30, 30, 256)  768         conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 30, 30, 192)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 30, 30, 256)  0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_183[0][0]             \n",
      "                                                                 activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_6_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6 (Lambda)               (None, 30, 30, 2080) 0           block8_5_ac[0][0]                \n",
      "                                                                 block8_6_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_ac (Activation)        (None, 30, 30, 2080) 0           block8_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 30, 30, 192)  399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 30, 30, 192)  576         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 30, 30, 192)  0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 30, 30, 224)  129024      activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 30, 30, 224)  672         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 30, 30, 224)  0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 30, 30, 192)  399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 30, 30, 256)  172032      activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 30, 30, 192)  576         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 30, 30, 256)  768         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 30, 30, 192)  0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 30, 30, 256)  0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_187[0][0]             \n",
      "                                                                 activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_7_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7 (Lambda)               (None, 30, 30, 2080) 0           block8_6_ac[0][0]                \n",
      "                                                                 block8_7_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_ac (Activation)        (None, 30, 30, 2080) 0           block8_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 30, 30, 192)  399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 30, 30, 192)  576         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 30, 30, 192)  0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 30, 30, 224)  129024      activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 30, 30, 224)  672         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 30, 30, 224)  0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 30, 30, 192)  399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 30, 30, 256)  172032      activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 30, 30, 192)  576         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 30, 30, 256)  768         conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 30, 30, 192)  0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 30, 30, 256)  0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_191[0][0]             \n",
      "                                                                 activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_8_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8 (Lambda)               (None, 30, 30, 2080) 0           block8_7_ac[0][0]                \n",
      "                                                                 block8_8_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_ac (Activation)        (None, 30, 30, 2080) 0           block8_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 30, 30, 192)  399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 30, 30, 192)  576         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 30, 30, 192)  0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 30, 30, 224)  129024      activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 30, 30, 224)  672         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 30, 30, 224)  0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 30, 30, 192)  399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 30, 30, 256)  172032      activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 30, 30, 192)  576         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 30, 30, 256)  768         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 30, 30, 192)  0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 30, 30, 256)  0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_mixed (Concatenate)    (None, 30, 30, 448)  0           activation_195[0][0]             \n",
      "                                                                 activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_conv (Conv2D)          (None, 30, 30, 2080) 933920      block8_9_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9 (Lambda)               (None, 30, 30, 2080) 0           block8_8_ac[0][0]                \n",
      "                                                                 block8_9_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_ac (Activation)        (None, 30, 30, 2080) 0           block8_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 30, 30, 192)  399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 30, 30, 192)  576         conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 30, 30, 192)  0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 30, 30, 224)  129024      activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 30, 30, 224)  672         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 30, 30, 224)  0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 30, 30, 192)  399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 30, 30, 256)  172032      activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 30, 30, 192)  576         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 30, 30, 256)  768         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 30, 30, 192)  0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 30, 30, 256)  0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_mixed (Concatenate)   (None, 30, 30, 448)  0           activation_199[0][0]             \n",
      "                                                                 activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_conv (Conv2D)         (None, 30, 30, 2080) 933920      block8_10_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_10 (Lambda)              (None, 30, 30, 2080) 0           block8_9_ac[0][0]                \n",
      "                                                                 block8_10_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b (Conv2D)                (None, 30, 30, 1536) 3194880     block8_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_bn (BatchNormalization) (None, 30, 30, 1536) 4608        conv_7b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_ac (Activation)         (None, 30, 30, 1536) 0           conv_7b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1536)         0           conv_7b_ac[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 54,336,736\n",
      "Trainable params: 0\n",
      "Non-trainable params: 54,336,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inception Resnet V2 base without top layer and specified input shape\n",
    "base = inception_resnet.InceptionResNetV2(weights='imagenet',\n",
    "                                          include_top=False,\n",
    "                                          input_shape=(TARGET_SIZE +\n",
    "                                                       tuple([3])),\n",
    "                                          pooling='avg')\n",
    "# Freeze layers\n",
    "for layer in base.layers:\n",
    "    layer.trainable=False\n",
    "base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict each images using Inception Resnet V2 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:06.520744Z",
     "start_time": "2021-01-11T06:38:06.238835Z"
    }
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3', region_name='us-east-2')\n",
    "bucket = s3.Bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:06.524031Z",
     "start_time": "2021-01-11T06:38:06.522019Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_image(key, bucket):\n",
    "#     \"\"\"Return numpy array of image from the s3 bucket\"\"\"\n",
    "#     obj = bucket.Object(key)\n",
    "#     response = obj.get()\n",
    "#     file_stream = response['Body']\n",
    "#     im = Image.open(file_stream).resize(TARGET_SIZE)\n",
    "#     return np.array(im)\n",
    "    \n",
    "\n",
    "# def write_json_file(data, filename):\n",
    "#     \"\"\"Write json file in the VECTOR_PATH\"\"\"\n",
    "#     key = os.path.join(VECTOR_PATH, filename)\n",
    "#     (s3.Object(BUCKET, key)\n",
    "#      .put(Body=bytes(json.dumps(data).encode('UTF-8'))))\n",
    "    \n",
    "\n",
    "# def read_json_file(filename):\n",
    "#     \"\"\"Reads json file in the VECTOR_PATH\"\"\"\n",
    "#     obj = bucket.Object(os.path.join(VECTOR_PATH, filename))\n",
    "#     response = obj.get()\n",
    "#     file_content = response['Body'].read()\n",
    "#     return json.loads(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:06.530526Z",
     "start_time": "2021-01-11T06:38:06.525992Z"
    }
   },
   "outputs": [],
   "source": [
    "# file_no = 100\n",
    "# train_images = list(bucket.objects.filter(Prefix=TRAIN_PATH))\n",
    "# count = len(train_images)\n",
    "# print('Extracting image vectors for {} images'.format(count))\n",
    "# images = []\n",
    "# for idx in tqdm(range(count)):\n",
    "#     obj = train_images[idx]\n",
    "#     try:\n",
    "#         # Get the numpy matrix of the image\n",
    "#         dct = dict()\n",
    "#         img = get_image(obj.key, bucket)\n",
    "        \n",
    "#         # Preprocess input based on the training of convolutional base\n",
    "#         nimage = inception_resnet.preprocess_input(img)\n",
    "#         nimage = np.expand_dims(nimage, axis=0)\n",
    "        \n",
    "#         # Extracted image vectors\n",
    "#         image_vector = base.predict(nimage)\n",
    "#         image_vector = np.reshape(image_vector, image_vector.shape[1:])\n",
    "        \n",
    "#         dct[\"name\"] = os.path.basename(obj.key)\n",
    "#         dct[\"value\"] = image_vector.tolist()\n",
    "#         images.append(dct)\n",
    "        \n",
    "#         # Check the image is in the hundredth or the last image\n",
    "#         if (not file_no // (idx + 1)) or count == (idx + 1):\n",
    "#             file_name = f'train_{file_no//100}.json'\n",
    "#             write_json_file(images, file_name)\n",
    "#             file_no += 100\n",
    "#             images = []\n",
    "#     except Exception as e:\n",
    "#         alert(5)\n",
    "#         print('Unable to read {}:{}'.format(idx, obj.key)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:06.535142Z",
     "start_time": "2021-01-11T06:38:06.531788Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file_no = 100\n",
    "# test_images = list(bucket.objects.filter(Prefix=TEST_PATH))\n",
    "# count = len(test_images)\n",
    "# print('Extracting image vectors for {} images'.format(count))\n",
    "# images = []\n",
    "# for idx in tqdm(range(count)):\n",
    "#     obj = test_images[idx]\n",
    "#     try:\n",
    "#         # Get the numpy matrix of the image\n",
    "#         dct = dict()\n",
    "#         img = get_image(obj.key, bucket)\n",
    "        \n",
    "#         # Preprocess input based on the training of convolutional base\n",
    "#         nimage = inception_resnet.preprocess_input(img)\n",
    "#         nimage = np.expand_dims(nimage, axis=0)\n",
    "        \n",
    "#         # Extracted image vectors\n",
    "#         image_vector = base.predict(nimage)\n",
    "#         image_vector = np.reshape(image_vector, image_vector.shape[1:])\n",
    "        \n",
    "#         dct[\"name\"] = os.path.basename(obj.key)\n",
    "#         dct[\"value\"] = image_vector.tolist()\n",
    "#         images.append(dct)\n",
    "        \n",
    "#         # Check the image is in the hundredth or the last image\n",
    "#         if (not file_no // (idx + 1)) or count == (idx + 1):\n",
    "#             file_name = f'test_{file_no//100}.json'\n",
    "#             write_json_file(images, file_name)\n",
    "#             file_no += 100\n",
    "#             images = []\n",
    "#     except Exception as e:\n",
    "#         alert(5)\n",
    "#         print('Unable to read {}:{}'.format(idx, obj.key)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:06.541654Z",
     "start_time": "2021-01-11T06:38:06.536227Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file_no = 100\n",
    "# sample_images = list(bucket.objects.filter(Prefix=SAMPLE_PATH))\n",
    "# count = len(sample_images)\n",
    "# print('Extracting image vectors for {} images'.format(count))\n",
    "# images = []\n",
    "# for idx in tqdm(range(count)):\n",
    "#     obj = sample_images[idx]\n",
    "#     try:\n",
    "#         # Get the numpy matrix of the image\n",
    "#         dct = dict()\n",
    "#         img = get_image(obj.key, bucket)\n",
    "        \n",
    "#         # Preprocess input based on the training of convolutional base\n",
    "#         nimage = inception_resnet.preprocess_input(img)\n",
    "#         nimage = np.expand_dims(nimage, axis=0)\n",
    "        \n",
    "#         # Extracted image vectors\n",
    "#         image_vector = base.predict(nimage)\n",
    "#         image_vector = np.reshape(image_vector, image_vector.shape[1:])\n",
    "        \n",
    "#         dct[\"name\"] = os.path.basename(obj.key)\n",
    "#         dct[\"value\"] = image_vector.tolist()\n",
    "#         images.append(dct)\n",
    "        \n",
    "#         # Check the image is in the hundredth or the last image\n",
    "#         if (not file_no // (idx + 1)) or count == (idx + 1):\n",
    "#             file_name = f'sample_{file_no//100}.json'\n",
    "#             write_json_file(images, file_name)\n",
    "#             file_no += 100\n",
    "#             images = []\n",
    "#     except Exception as e:\n",
    "#         alert(5)\n",
    "#         print('Unable to read {}:{}'.format(idx, obj.key)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:07.698632Z",
     "start_time": "2021-01-11T06:38:06.542636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:39147</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>33.19 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:39147' processes=4 threads=8, memory=33.19 GB>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dask cluster\n",
    "#client = Client('172.31.31.30:8786')\n",
    "client = Client()\n",
    "\n",
    "# Register progress bar\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "# Get AWS Config\n",
    "# with open('setting_aws.pkl', 'rb') as f:\n",
    "#     settings_aws = pickle.load(f)\n",
    "\n",
    "# Show cluster\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.739592Z",
     "start_time": "2021-01-11T06:38:07.699975Z"
    }
   },
   "outputs": [],
   "source": [
    "test_label_path = ('s3://kapeles/BDCC_Dataset/downloads/'\n",
    "                   'kagg-foru-mess-atta_9052_2877_reti_solupq'\n",
    "                   '89ZoBOgYh9qJuSYbz7faT1us7bBw8bXbg_qHmrYMk.csv')\n",
    "train_label_path = ('s3://kapeles/BDCC_Dataset/downloads/manual'\n",
    "                    '/trainLabels.csv')\n",
    "train_label = dd.read_csv(train_label_path, storage_options=settings_aws)\n",
    "test_label = dd.read_csv(test_label_path,\n",
    "                         usecols=['image', 'level'],\n",
    "                         storage_options=settings_aws)\n",
    "\n",
    "all_labels = train_label.append(test_label).compute()\n",
    "all_labels.image = all_labels.image.apply(lambda x: str(x)+'.jpeg')\n",
    "del train_label_path, test_label_path, train_label, test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset 60 20 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.779260Z",
     "start_time": "2021-01-11T06:38:08.740888Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(all_labels.image,\n",
    "                                                    all_labels.level,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=2021)\n",
    "train_labels = all_labels.loc[all_labels.image.isin(x_train)]\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_labels.image,\n",
    "                                                  train_labels.level,\n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state=2021)\n",
    "\n",
    "del all_labels, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T04:19:58.261772Z",
     "start_time": "2021-01-10T04:17:01.364Z"
    }
   },
   "source": [
    "## Get class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.800216Z",
     "start_time": "2021-01-11T06:38:08.780667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.27143367164788085,\n",
       " 1: 2.8597528210639442,\n",
       " 2: 1.3536818008393743,\n",
       " 3: 8.322126661454261,\n",
       " 4: 9.320490367775832}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_integers = y_train.to_list()\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "dict_class_weights = dict(enumerate(class_weights))\n",
    "dict_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform sampling to solve imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.806628Z",
     "start_time": "2021-01-11T06:38:08.801454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    39214\n",
       "2     7863\n",
       "1     3722\n",
       "3     1279\n",
       "4     1142\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.858383Z",
     "start_time": "2021-01-11T06:38:08.807760Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = int(y_train.value_counts().max())\n",
    "x_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "new_x_train, new_y_train = pd.Series(dtype='object'), pd.Series(dtype='int32')\n",
    "for cls in y_train.unique():\n",
    "    idx = y_train[y_train==cls].index\n",
    "    x = x_train.loc[idx]\n",
    "    y = y_train.loc[idx]\n",
    "    \n",
    "    new_x, new_y = resample(x, y, replace=True, n_samples=n_samples,\n",
    "                            random_state=2021)\n",
    "    new_x_train = new_x_train.append(new_x)\n",
    "    new_y_train = new_y_train.append(new_y)\n",
    "\n",
    "x_train, y_train = new_x_train, new_y_train\n",
    "del new_x_train, new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.865625Z",
     "start_time": "2021-01-11T06:38:08.859649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    39214\n",
       "3    39214\n",
       "2    39214\n",
       "1    39214\n",
       "0    39214\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data generator to get batches from train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.871601Z",
     "start_time": "2021-01-11T06:38:08.866861Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_generator(captions, data, batch_size):\n",
    "    \"\"\"\n",
    "    Create a generator that for 2 inputs (image vector, caption sequence)\n",
    "    output is the next word in the sequence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        captions : dict\n",
    "            image filename as key and level as value\n",
    "        data : dask bag\n",
    "            image filaname as key and vectors from convolutional base as value\n",
    "        img_map : list or iterator\n",
    "            list or iterator to include, exclude data not in the img_map\n",
    "        batch_size : int\n",
    "            batch_size\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        result : 2 tuples\n",
    "            first element is the 2 inputs image vector from the convolutional\n",
    "            base and text to sequence of the caption\n",
    "            second element is the output is the next word in the sequence\n",
    "            ([image_vector, sequence], next_word)\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for image_name, vector in data.items():\n",
    "            # Get image vector based on the convolutional base\n",
    "            target = captions[image_name]\n",
    "            # Set the value of target to binary\n",
    "            target = to_categorical([target], num_classes=5)[0]\n",
    "            x.append(vector)\n",
    "            y.append(target)\n",
    "            n += 1\n",
    "            if n == batch_size:\n",
    "                yield np.array(x), np.array(y)\n",
    "                x, y = [], []\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data using Dask Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:08.876295Z",
     "start_time": "2021-01-11T06:38:08.872808Z"
    }
   },
   "outputs": [],
   "source": [
    "def join_dict(x, y):\n",
    "    \"\"\"Update dictionary x with values of dictionary y\"\"\"\n",
    "    x.update(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:38:09.276199Z",
     "start_time": "2021-01-11T06:38:08.877525Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read training data from S3\n",
    "data = (db.read_text(os.path.join('s3://', BUCKET, VECTOR_PATH, '*.json'),\n",
    "                     storage_options=settings_aws)\n",
    "        .map(json.loads)\n",
    "        .flatten()\n",
    "        .persist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:45:09.528359Z",
     "start_time": "2021-01-11T06:38:09.278216Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = (data.filter(lambda x: x['name'] in x_train.values)\n",
    "              .map(lambda x: {x['name']: np.asarray(x['value'])})\n",
    "              .fold(binop=lambda x, y: join_dict(x, y),\n",
    "                    combine=lambda x, y: join_dict(x, y))\n",
    "              .compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:46:08.028301Z",
     "start_time": "2021-01-11T06:45:09.530010Z"
    }
   },
   "outputs": [],
   "source": [
    "val_data = (data.filter(lambda x: x['name'] in x_val.values)\n",
    "            .map(lambda x: {x['name']: np.asarray(x['value'])})\n",
    "            .fold(binop=lambda x, y: join_dict(x, y),\n",
    "                  combine=lambda x, y: join_dict(x, y))\n",
    "            .compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:46:08.094466Z",
     "start_time": "2021-01-11T06:46:08.029943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make train labels to dictionary for easier implementation\n",
    "train_labels = pd.Series(y_train.values,index=x_train.values).to_dict()\n",
    "val_labels = pd.Series(y_val.values, index=x_val.values).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST Remove first Dropout layer and Add Dense layer, Reduce Batch size perform sampling using max as n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:27:52.296049Z",
     "start_time": "2021-01-03T19:27:52.291395Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_toplayers():\n",
    "    \"\"\"Create top layer using inceptionb resnet v2 base\"\"\"\n",
    "    # Create input layer based on the output of the convolutional base\n",
    "    lyr_input = Input(shape=(base.layers[-1].output.shape.as_list()[1], ))\n",
    "\n",
    "    # Add Dense\n",
    "    lyr_dense1 = Dense(1024, activation='relu')(lyr_input)\n",
    "    lyr_dense2 = Dense(512, activation='relu')(lyr_dense1)\n",
    "    lyr_dense3 = Dense(256, activation='relu')(lyr_dense2)\n",
    "\n",
    "\n",
    "    # Create output layer\n",
    "    output = Dense(5, activation='softmax')(lyr_dense3)\n",
    "\n",
    "    model = Model(inputs=[lyr_input], outputs=[output])\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:27:52.300217Z",
     "start_time": "2021-01-03T19:27:52.297194Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_gen = data_generator(train_labels, train_data, batch_size)\n",
    "val_gen = data_generator(val_labels, val_data, batch_size)\n",
    "train_steps = len(train_labels) // batch_size\n",
    "val_steps = len(val_labels) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:27:52.305010Z",
     "start_time": "2021-01-03T19:27:52.301380Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_fp = 'tune/add_more_dense4.h5'\n",
    "checkpoint = ModelCheckpoint(model_fp,\n",
    "                             monitor='val_categorical_accuracy',\n",
    "                             verbose=1, save_best_only=True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_categorical_accuracy',\n",
    "                                   factor=0.8,\n",
    "                                   patience=3,\n",
    "                                   cooldown=3,\n",
    "                                   verbose=1,\n",
    "                                   min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_categorical_accuracy\", \n",
    "                      verbose=1,\n",
    "                      patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:31:18.379061Z",
     "start_time": "2021-01-03T19:27:52.306285Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 1.5841 - categorical_accuracy: 0.5432 - val_loss: 0.8206 - val_categorical_accuracy: 0.7367\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.73667, saving model to tune/add_more_dense4.h5\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 1.0638 - categorical_accuracy: 0.6090 - val_loss: 0.8003 - val_categorical_accuracy: 0.7356\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.73667\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 1.0051 - categorical_accuracy: 0.6402 - val_loss: 0.7915 - val_categorical_accuracy: 0.7374\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.73667 to 0.73742, saving model to tune/add_more_dense4.h5\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 1.0010 - categorical_accuracy: 0.6396 - val_loss: 0.7832 - val_categorical_accuracy: 0.7376\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.73742 to 0.73765, saving model to tune/add_more_dense4.h5\n",
      "Epoch 5/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9654 - categorical_accuracy: 0.6400 - val_loss: 0.8370 - val_categorical_accuracy: 0.7376\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.73765\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9568 - categorical_accuracy: 0.6434 - val_loss: 0.7812 - val_categorical_accuracy: 0.7394\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.73765 to 0.73943, saving model to tune/add_more_dense4.h5\n",
      "Epoch 7/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9521 - categorical_accuracy: 0.6439 - val_loss: 0.8380 - val_categorical_accuracy: 0.7375\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.73943\n",
      "Epoch 8/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9361 - categorical_accuracy: 0.6426 - val_loss: 0.7513 - val_categorical_accuracy: 0.7418\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.73943 to 0.74179, saving model to tune/add_more_dense4.h5\n",
      "Epoch 9/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9384 - categorical_accuracy: 0.6475 - val_loss: 0.7964 - val_categorical_accuracy: 0.7413\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.74179\n",
      "Epoch 10/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9310 - categorical_accuracy: 0.6550 - val_loss: 0.7908 - val_categorical_accuracy: 0.7490\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.74179 to 0.74902, saving model to tune/add_more_dense4.h5\n",
      "Epoch 11/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.9322 - categorical_accuracy: 0.6511 - val_loss: 0.7569 - val_categorical_accuracy: 0.7436\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.74902\n",
      "Epoch 12/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9185 - categorical_accuracy: 0.6563 - val_loss: 0.7555 - val_categorical_accuracy: 0.7457\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.74902\n",
      "Epoch 13/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.9084 - categorical_accuracy: 0.6618 - val_loss: 0.8243 - val_categorical_accuracy: 0.7382\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.74902\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 14/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9276 - categorical_accuracy: 0.6581 - val_loss: 0.8286 - val_categorical_accuracy: 0.7061\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.74902\n",
      "Epoch 15/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.9054 - categorical_accuracy: 0.6658 - val_loss: 0.7642 - val_categorical_accuracy: 0.7455\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.74902\n",
      "Epoch 16/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8922 - categorical_accuracy: 0.6707 - val_loss: 0.7989 - val_categorical_accuracy: 0.7189\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.74902\n",
      "Epoch 17/200\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.8887 - categorical_accuracy: 0.6698 - val_loss: 0.7288 - val_categorical_accuracy: 0.7529\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.74902 to 0.75287, saving model to tune/add_more_dense4.h5\n",
      "Epoch 18/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8757 - categorical_accuracy: 0.6760 - val_loss: 0.7398 - val_categorical_accuracy: 0.7595\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy improved from 0.75287 to 0.75948, saving model to tune/add_more_dense4.h5\n",
      "Epoch 19/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8635 - categorical_accuracy: 0.6816 - val_loss: 0.7314 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.75948\n",
      "Epoch 20/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8535 - categorical_accuracy: 0.6806 - val_loss: 0.7274 - val_categorical_accuracy: 0.7575\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.75948\n",
      "Epoch 21/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8496 - categorical_accuracy: 0.6814 - val_loss: 0.7139 - val_categorical_accuracy: 0.7640\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy improved from 0.75948 to 0.76396, saving model to tune/add_more_dense4.h5\n",
      "Epoch 22/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8591 - categorical_accuracy: 0.6762 - val_loss: 0.7197 - val_categorical_accuracy: 0.7623\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.76396\n",
      "Epoch 23/200\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 0.8488 - categorical_accuracy: 0.6856 - val_loss: 0.7209 - val_categorical_accuracy: 0.7533\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.76396\n",
      "Epoch 24/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8466 - categorical_accuracy: 0.6862 - val_loss: 0.7649 - val_categorical_accuracy: 0.7383\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.76396\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 25/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8368 - categorical_accuracy: 0.6877 - val_loss: 0.6943 - val_categorical_accuracy: 0.7668\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy improved from 0.76396 to 0.76683, saving model to tune/add_more_dense4.h5\n",
      "Epoch 26/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8426 - categorical_accuracy: 0.6872 - val_loss: 0.7041 - val_categorical_accuracy: 0.7602\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.76683\n",
      "Epoch 27/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8383 - categorical_accuracy: 0.6919 - val_loss: 0.6930 - val_categorical_accuracy: 0.7692\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy improved from 0.76683 to 0.76919, saving model to tune/add_more_dense4.h5\n",
      "Epoch 28/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8278 - categorical_accuracy: 0.6958 - val_loss: 0.7251 - val_categorical_accuracy: 0.7538\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.76919\n",
      "Epoch 29/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8471 - categorical_accuracy: 0.6879 - val_loss: 0.6916 - val_categorical_accuracy: 0.7642\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.76919\n",
      "Epoch 30/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8233 - categorical_accuracy: 0.6944 - val_loss: 0.7124 - val_categorical_accuracy: 0.7580\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.76919\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 31/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8171 - categorical_accuracy: 0.6987 - val_loss: 0.7110 - val_categorical_accuracy: 0.7568\n",
      "\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.76919\n",
      "Epoch 32/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8177 - categorical_accuracy: 0.6979 - val_loss: 0.6971 - val_categorical_accuracy: 0.7632\n",
      "\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.76919\n",
      "Epoch 33/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.8157 - categorical_accuracy: 0.6963 - val_loss: 0.7134 - val_categorical_accuracy: 0.7602\n",
      "\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.76919\n",
      "Epoch 34/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8039 - categorical_accuracy: 0.7011 - val_loss: 0.6855 - val_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.76919\n",
      "Epoch 35/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8056 - categorical_accuracy: 0.6991 - val_loss: 0.7090 - val_categorical_accuracy: 0.7664\n",
      "\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.76919\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 36/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7875 - categorical_accuracy: 0.7064 - val_loss: 0.6928 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00036: val_categorical_accuracy improved from 0.76919 to 0.77378, saving model to tune/add_more_dense4.h5\n",
      "Epoch 37/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7891 - categorical_accuracy: 0.7064 - val_loss: 0.7053 - val_categorical_accuracy: 0.7698\n",
      "\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.77378\n",
      "Epoch 38/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7883 - categorical_accuracy: 0.7058 - val_loss: 0.6925 - val_categorical_accuracy: 0.7663\n",
      "\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.77378\n",
      "Epoch 39/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7901 - categorical_accuracy: 0.7047 - val_loss: 0.6754 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00039: val_categorical_accuracy improved from 0.77378 to 0.77591, saving model to tune/add_more_dense4.h5\n",
      "Epoch 40/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.8029 - categorical_accuracy: 0.6975 - val_loss: 0.6738 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.77591\n",
      "Epoch 41/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7872 - categorical_accuracy: 0.7073 - val_loss: 0.6898 - val_categorical_accuracy: 0.7653\n",
      "\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.77591\n",
      "Epoch 42/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7928 - categorical_accuracy: 0.7032 - val_loss: 0.6850 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.77591\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 43/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7703 - categorical_accuracy: 0.7131 - val_loss: 0.7311 - val_categorical_accuracy: 0.7487\n",
      "\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.77591\n",
      "Epoch 44/200\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.7729 - categorical_accuracy: 0.7125 - val_loss: 0.6744 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.77591\n",
      "Epoch 45/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7713 - categorical_accuracy: 0.7125 - val_loss: 0.6723 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.77591\n",
      "Epoch 46/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7642 - categorical_accuracy: 0.7139 - val_loss: 0.6852 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.77591\n",
      "Epoch 47/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7629 - categorical_accuracy: 0.7153 - val_loss: 0.6673 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00047: val_categorical_accuracy improved from 0.77591 to 0.77688, saving model to tune/add_more_dense4.h5\n",
      "Epoch 48/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7648 - categorical_accuracy: 0.7142 - val_loss: 0.7011 - val_categorical_accuracy: 0.7665\n",
      "\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.77688\n",
      "Epoch 49/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7620 - categorical_accuracy: 0.7168 - val_loss: 0.6744 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00049: val_categorical_accuracy improved from 0.77688 to 0.77889, saving model to tune/add_more_dense4.h5\n",
      "Epoch 50/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7652 - categorical_accuracy: 0.7144 - val_loss: 0.6987 - val_categorical_accuracy: 0.7647\n",
      "\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.77889\n",
      "Epoch 51/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7667 - categorical_accuracy: 0.7135 - val_loss: 0.6742 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.77889\n",
      "Epoch 52/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7657 - categorical_accuracy: 0.7145 - val_loss: 0.6702 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.77889\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 53/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7478 - categorical_accuracy: 0.7204 - val_loss: 0.7116 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.77889\n",
      "Epoch 54/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7477 - categorical_accuracy: 0.7216 - val_loss: 0.6807 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.77889\n",
      "Epoch 55/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7553 - categorical_accuracy: 0.7182 - val_loss: 0.7472 - val_categorical_accuracy: 0.7423\n",
      "\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.77889\n",
      "Epoch 56/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7470 - categorical_accuracy: 0.7247 - val_loss: 0.6685 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.77889\n",
      "Epoch 57/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7443 - categorical_accuracy: 0.7231 - val_loss: 0.6723 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.77889\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 58/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7401 - categorical_accuracy: 0.7263 - val_loss: 0.6627 - val_categorical_accuracy: 0.7810\n",
      "\n",
      "Epoch 00058: val_categorical_accuracy improved from 0.77889 to 0.78096, saving model to tune/add_more_dense4.h5\n",
      "Epoch 59/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7335 - categorical_accuracy: 0.7270 - val_loss: 0.6720 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 60/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7303 - categorical_accuracy: 0.7299 - val_loss: 0.6683 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 61/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7221 - categorical_accuracy: 0.7324 - val_loss: 0.6904 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00061: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 62/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7249 - categorical_accuracy: 0.7319 - val_loss: 0.6665 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.78096\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 63/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7189 - categorical_accuracy: 0.7339 - val_loss: 0.6818 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00063: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 64/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7196 - categorical_accuracy: 0.7313 - val_loss: 0.6962 - val_categorical_accuracy: 0.7689\n",
      "\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 65/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7184 - categorical_accuracy: 0.7328 - val_loss: 0.6750 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 66/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7171 - categorical_accuracy: 0.7336 - val_loss: 0.6787 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 67/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7072 - categorical_accuracy: 0.7374 - val_loss: 0.6666 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00067: val_categorical_accuracy did not improve from 0.78096\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 68/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7052 - categorical_accuracy: 0.7392 - val_loss: 0.6901 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 69/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7063 - categorical_accuracy: 0.7375 - val_loss: 0.6791 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.78096\n",
      "Epoch 70/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7032 - categorical_accuracy: 0.7402 - val_loss: 0.6666 - val_categorical_accuracy: 0.7815\n",
      "\n",
      "Epoch 00070: val_categorical_accuracy improved from 0.78096 to 0.78154, saving model to tune/add_more_dense4.h5\n",
      "Epoch 71/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.7014 - categorical_accuracy: 0.7402 - val_loss: 0.6769 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00071: val_categorical_accuracy did not improve from 0.78154\n",
      "Epoch 72/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6980 - categorical_accuracy: 0.7415 - val_loss: 0.6718 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00072: val_categorical_accuracy did not improve from 0.78154\n",
      "Epoch 73/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6956 - categorical_accuracy: 0.7431 - val_loss: 0.6680 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00073: val_categorical_accuracy did not improve from 0.78154\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 74/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6956 - categorical_accuracy: 0.7427 - val_loss: 0.6656 - val_categorical_accuracy: 0.7817\n",
      "\n",
      "Epoch 00074: val_categorical_accuracy improved from 0.78154 to 0.78165, saving model to tune/add_more_dense4.h5\n",
      "Epoch 75/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6935 - categorical_accuracy: 0.7439 - val_loss: 0.6753 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00075: val_categorical_accuracy did not improve from 0.78165\n",
      "Epoch 76/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6929 - categorical_accuracy: 0.7430 - val_loss: 0.6675 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00076: val_categorical_accuracy did not improve from 0.78165\n",
      "Epoch 77/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6879 - categorical_accuracy: 0.7434 - val_loss: 0.6857 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00077: val_categorical_accuracy did not improve from 0.78165\n",
      "Epoch 78/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6890 - categorical_accuracy: 0.7427 - val_loss: 0.6593 - val_categorical_accuracy: 0.7831\n",
      "\n",
      "Epoch 00078: val_categorical_accuracy improved from 0.78165 to 0.78309, saving model to tune/add_more_dense4.h5\n",
      "Epoch 79/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6875 - categorical_accuracy: 0.7421 - val_loss: 0.7055 - val_categorical_accuracy: 0.7617\n",
      "\n",
      "Epoch 00079: val_categorical_accuracy did not improve from 0.78309\n",
      "Epoch 80/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6880 - categorical_accuracy: 0.7440 - val_loss: 0.6732 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00080: val_categorical_accuracy did not improve from 0.78309\n",
      "Epoch 81/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.6863 - categorical_accuracy: 0.7444 - val_loss: 0.6812 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00081: val_categorical_accuracy did not improve from 0.78309\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 82/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6809 - categorical_accuracy: 0.7461 - val_loss: 0.6616 - val_categorical_accuracy: 0.7841\n",
      "\n",
      "Epoch 00082: val_categorical_accuracy improved from 0.78309 to 0.78406, saving model to tune/add_more_dense4.h5\n",
      "Epoch 83/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6867 - categorical_accuracy: 0.7445 - val_loss: 0.6941 - val_categorical_accuracy: 0.7646\n",
      "\n",
      "Epoch 00083: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 84/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6866 - categorical_accuracy: 0.7456 - val_loss: 0.6642 - val_categorical_accuracy: 0.7814\n",
      "\n",
      "Epoch 00084: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 85/200\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 0.6807 - categorical_accuracy: 0.7479 - val_loss: 0.6682 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00085: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 86/200\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.6770 - categorical_accuracy: 0.7485 - val_loss: 0.6664 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00086: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 87/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6717 - categorical_accuracy: 0.7508 - val_loss: 0.6820 - val_categorical_accuracy: 0.7713\n",
      "\n",
      "Epoch 00087: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 88/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6716 - categorical_accuracy: 0.7502 - val_loss: 0.6852 - val_categorical_accuracy: 0.7720\n",
      "\n",
      "Epoch 00088: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 89/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6696 - categorical_accuracy: 0.7514 - val_loss: 0.6656 - val_categorical_accuracy: 0.7806\n",
      "\n",
      "Epoch 00089: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 90/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6674 - categorical_accuracy: 0.7519 - val_loss: 0.6834 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00090: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 91/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6683 - categorical_accuracy: 0.7534 - val_loss: 0.6756 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00091: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 92/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6709 - categorical_accuracy: 0.7524 - val_loss: 0.7060 - val_categorical_accuracy: 0.7673\n",
      "\n",
      "Epoch 00092: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 93/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6741 - categorical_accuracy: 0.7506 - val_loss: 0.6765 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00093: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 94/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6773 - categorical_accuracy: 0.7471 - val_loss: 0.6817 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00094: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 95/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6765 - categorical_accuracy: 0.7475 - val_loss: 0.6727 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00095: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 96/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6708 - categorical_accuracy: 0.7496 - val_loss: 0.6838 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00096: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 97/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6678 - categorical_accuracy: 0.7518 - val_loss: 0.6680 - val_categorical_accuracy: 0.7821\n",
      "\n",
      "Epoch 00097: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 98/200\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.6670 - categorical_accuracy: 0.7524 - val_loss: 0.6945 - val_categorical_accuracy: 0.7661\n",
      "\n",
      "Epoch 00098: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 99/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6679 - categorical_accuracy: 0.7528 - val_loss: 0.7200 - val_categorical_accuracy: 0.7566\n",
      "\n",
      "Epoch 00099: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 100/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6679 - categorical_accuracy: 0.7551 - val_loss: 0.6739 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00100: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 101/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.6706 - categorical_accuracy: 0.7505 - val_loss: 0.6853 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00101: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 102/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6674 - categorical_accuracy: 0.7536 - val_loss: 0.6755 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00102: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 103/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6614 - categorical_accuracy: 0.7557 - val_loss: 0.6828 - val_categorical_accuracy: 0.7722\n",
      "\n",
      "Epoch 00103: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 104/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6614 - categorical_accuracy: 0.7543 - val_loss: 0.6743 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00104: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 105/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.6593 - categorical_accuracy: 0.7553 - val_loss: 0.6912 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00105: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 106/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6579 - categorical_accuracy: 0.7567 - val_loss: 0.6746 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00106: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 107/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6550 - categorical_accuracy: 0.7580 - val_loss: 0.7335 - val_categorical_accuracy: 0.7525\n",
      "\n",
      "Epoch 00107: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 108/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6555 - categorical_accuracy: 0.7575 - val_loss: 0.6723 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00108: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 109/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6532 - categorical_accuracy: 0.7584 - val_loss: 0.6721 - val_categorical_accuracy: 0.7823\n",
      "\n",
      "Epoch 00109: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 110/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.6532 - categorical_accuracy: 0.7586 - val_loss: 0.6827 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00110: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 111/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6469 - categorical_accuracy: 0.7613 - val_loss: 0.6898 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00111: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 112/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6430 - categorical_accuracy: 0.7615 - val_loss: 0.6882 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00112: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 113/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6460 - categorical_accuracy: 0.7615 - val_loss: 0.6843 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00113: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 114/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6416 - categorical_accuracy: 0.7647 - val_loss: 0.6985 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00114: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 115/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6405 - categorical_accuracy: 0.7635 - val_loss: 0.6737 - val_categorical_accuracy: 0.7828\n",
      "\n",
      "Epoch 00115: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 116/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6402 - categorical_accuracy: 0.7645 - val_loss: 0.6790 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00116: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 117/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6386 - categorical_accuracy: 0.7632 - val_loss: 0.6780 - val_categorical_accuracy: 0.7810\n",
      "\n",
      "Epoch 00117: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 118/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6358 - categorical_accuracy: 0.7638 - val_loss: 0.6937 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00118: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 119/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6362 - categorical_accuracy: 0.7627 - val_loss: 0.6770 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00119: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 120/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6320 - categorical_accuracy: 0.7636 - val_loss: 0.7139 - val_categorical_accuracy: 0.7584\n",
      "\n",
      "Epoch 00120: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 121/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6304 - categorical_accuracy: 0.7661 - val_loss: 0.6684 - val_categorical_accuracy: 0.7837\n",
      "\n",
      "Epoch 00121: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 122/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6333 - categorical_accuracy: 0.7639 - val_loss: 0.7054 - val_categorical_accuracy: 0.7622\n",
      "\n",
      "Epoch 00122: val_categorical_accuracy did not improve from 0.78406\n",
      "Epoch 123/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6318 - categorical_accuracy: 0.7660 - val_loss: 0.6698 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00123: val_categorical_accuracy improved from 0.78406 to 0.78567, saving model to tune/add_more_dense4.h5\n",
      "Epoch 124/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6312 - categorical_accuracy: 0.7646 - val_loss: 0.7245 - val_categorical_accuracy: 0.7557\n",
      "\n",
      "Epoch 00124: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 125/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6274 - categorical_accuracy: 0.7691 - val_loss: 0.6747 - val_categorical_accuracy: 0.7845\n",
      "\n",
      "Epoch 00125: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 126/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.6308 - categorical_accuracy: 0.7651 - val_loss: 0.7469 - val_categorical_accuracy: 0.7420\n",
      "\n",
      "Epoch 00126: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 127/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6318 - categorical_accuracy: 0.7640 - val_loss: 0.6733 - val_categorical_accuracy: 0.7829\n",
      "\n",
      "Epoch 00127: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 128/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6278 - categorical_accuracy: 0.7660 - val_loss: 0.6780 - val_categorical_accuracy: 0.7812\n",
      "\n",
      "Epoch 00128: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 129/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6280 - categorical_accuracy: 0.7683 - val_loss: 0.6796 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00129: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 130/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6196 - categorical_accuracy: 0.7695 - val_loss: 0.7397 - val_categorical_accuracy: 0.7456\n",
      "\n",
      "Epoch 00130: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 131/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6207 - categorical_accuracy: 0.7703 - val_loss: 0.6773 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00131: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 132/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6173 - categorical_accuracy: 0.7721 - val_loss: 0.6728 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00132: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 133/200\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.6118 - categorical_accuracy: 0.7725 - val_loss: 0.7183 - val_categorical_accuracy: 0.7561\n",
      "\n",
      "Epoch 00133: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 134/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6132 - categorical_accuracy: 0.7729 - val_loss: 0.6813 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00134: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 135/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6161 - categorical_accuracy: 0.7715 - val_loss: 0.7108 - val_categorical_accuracy: 0.7615\n",
      "\n",
      "Epoch 00135: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 136/200\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 0.6214 - categorical_accuracy: 0.7707 - val_loss: 0.7253 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00136: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 137/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6232 - categorical_accuracy: 0.7695 - val_loss: 0.7109 - val_categorical_accuracy: 0.7642\n",
      "\n",
      "Epoch 00137: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 138/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6207 - categorical_accuracy: 0.7685 - val_loss: 0.6752 - val_categorical_accuracy: 0.7837\n",
      "\n",
      "Epoch 00138: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 139/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6216 - categorical_accuracy: 0.7674 - val_loss: 0.6874 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00139: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 140/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6143 - categorical_accuracy: 0.7715 - val_loss: 0.7179 - val_categorical_accuracy: 0.7621\n",
      "\n",
      "Epoch 00140: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 141/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6132 - categorical_accuracy: 0.7720 - val_loss: 0.6803 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00141: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 142/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6204 - categorical_accuracy: 0.7674 - val_loss: 0.7340 - val_categorical_accuracy: 0.7509\n",
      "\n",
      "Epoch 00142: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 143/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6147 - categorical_accuracy: 0.7729 - val_loss: 0.6880 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00143: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 144/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6186 - categorical_accuracy: 0.7706 - val_loss: 0.7095 - val_categorical_accuracy: 0.7663\n",
      "\n",
      "Epoch 00144: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 145/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6165 - categorical_accuracy: 0.7712 - val_loss: 0.6785 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00145: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 146/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6124 - categorical_accuracy: 0.7732 - val_loss: 0.6941 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00146: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 147/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6123 - categorical_accuracy: 0.7719 - val_loss: 0.7166 - val_categorical_accuracy: 0.7629\n",
      "\n",
      "Epoch 00147: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 148/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6071 - categorical_accuracy: 0.7771 - val_loss: 0.7093 - val_categorical_accuracy: 0.7686\n",
      "\n",
      "Epoch 00148: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 149/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6043 - categorical_accuracy: 0.7783 - val_loss: 0.7327 - val_categorical_accuracy: 0.7545\n",
      "\n",
      "Epoch 00149: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 150/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6137 - categorical_accuracy: 0.7729 - val_loss: 0.7678 - val_categorical_accuracy: 0.7408\n",
      "\n",
      "Epoch 00150: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 151/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6074 - categorical_accuracy: 0.7740 - val_loss: 0.7181 - val_categorical_accuracy: 0.7637\n",
      "\n",
      "Epoch 00151: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 152/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6029 - categorical_accuracy: 0.7743 - val_loss: 0.6945 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00152: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 153/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6052 - categorical_accuracy: 0.7771 - val_loss: 0.7229 - val_categorical_accuracy: 0.7634\n",
      "\n",
      "Epoch 00153: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 154/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.6004 - categorical_accuracy: 0.7775 - val_loss: 0.7077 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00154: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 155/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.5996 - categorical_accuracy: 0.7783 - val_loss: 0.7242 - val_categorical_accuracy: 0.7611\n",
      "\n",
      "Epoch 00155: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 156/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5999 - categorical_accuracy: 0.7791 - val_loss: 0.7216 - val_categorical_accuracy: 0.7638\n",
      "\n",
      "Epoch 00156: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 157/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5975 - categorical_accuracy: 0.7804 - val_loss: 0.7205 - val_categorical_accuracy: 0.7644\n",
      "\n",
      "Epoch 00157: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 158/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5982 - categorical_accuracy: 0.7768 - val_loss: 0.7011 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00158: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 159/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5910 - categorical_accuracy: 0.7820 - val_loss: 0.6897 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00159: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 160/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5889 - categorical_accuracy: 0.7773 - val_loss: 0.6982 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00160: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 161/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5878 - categorical_accuracy: 0.7808 - val_loss: 0.7177 - val_categorical_accuracy: 0.7651\n",
      "\n",
      "Epoch 00161: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 162/200\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.5883 - categorical_accuracy: 0.7819 - val_loss: 0.7066 - val_categorical_accuracy: 0.7720\n",
      "\n",
      "Epoch 00162: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 163/200\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.5866 - categorical_accuracy: 0.7820 - val_loss: 0.7326 - val_categorical_accuracy: 0.7548\n",
      "\n",
      "Epoch 00163: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 164/200\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.5823 - categorical_accuracy: 0.7841 - val_loss: 0.7079 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00164: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 165/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5804 - categorical_accuracy: 0.7834 - val_loss: 0.6969 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00165: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 166/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5796 - categorical_accuracy: 0.7848 - val_loss: 0.7091 - val_categorical_accuracy: 0.7691\n",
      "\n",
      "Epoch 00166: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 167/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5790 - categorical_accuracy: 0.7875 - val_loss: 0.6963 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00167: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 168/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5814 - categorical_accuracy: 0.7833 - val_loss: 0.7350 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00168: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 169/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5824 - categorical_accuracy: 0.7817 - val_loss: 0.7937 - val_categorical_accuracy: 0.7247\n",
      "\n",
      "Epoch 00169: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 170/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5876 - categorical_accuracy: 0.7807 - val_loss: 0.7081 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00170: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 171/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5813 - categorical_accuracy: 0.7834 - val_loss: 0.7106 - val_categorical_accuracy: 0.7819\n",
      "\n",
      "Epoch 00171: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 172/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5815 - categorical_accuracy: 0.7829 - val_loss: 0.6872 - val_categorical_accuracy: 0.7806\n",
      "\n",
      "Epoch 00172: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 173/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5693 - categorical_accuracy: 0.7906 - val_loss: 0.7024 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00173: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 174/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5713 - categorical_accuracy: 0.7892 - val_loss: 0.7563 - val_categorical_accuracy: 0.7465\n",
      "\n",
      "Epoch 00174: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 175/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5726 - categorical_accuracy: 0.7860 - val_loss: 0.7425 - val_categorical_accuracy: 0.7536\n",
      "\n",
      "Epoch 00175: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 176/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5693 - categorical_accuracy: 0.7895 - val_loss: 0.6962 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00176: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 177/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5681 - categorical_accuracy: 0.7912 - val_loss: 0.7582 - val_categorical_accuracy: 0.7455\n",
      "\n",
      "Epoch 00177: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 178/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.5662 - categorical_accuracy: 0.7938 - val_loss: 0.7189 - val_categorical_accuracy: 0.7660\n",
      "\n",
      "Epoch 00178: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 179/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.5695 - categorical_accuracy: 0.7910 - val_loss: 0.7479 - val_categorical_accuracy: 0.7547\n",
      "\n",
      "Epoch 00179: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 180/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5711 - categorical_accuracy: 0.7910 - val_loss: 0.7049 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00180: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 181/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5806 - categorical_accuracy: 0.7829 - val_loss: 0.7263 - val_categorical_accuracy: 0.7640\n",
      "\n",
      "Epoch 00181: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 182/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5727 - categorical_accuracy: 0.7872 - val_loss: 0.7013 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00182: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 183/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5674 - categorical_accuracy: 0.7888 - val_loss: 0.7037 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00183: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 184/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5754 - categorical_accuracy: 0.7845 - val_loss: 0.6996 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00184: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 185/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5692 - categorical_accuracy: 0.7896 - val_loss: 0.7896 - val_categorical_accuracy: 0.7373\n",
      "\n",
      "Epoch 00185: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 186/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5722 - categorical_accuracy: 0.7875 - val_loss: 0.7263 - val_categorical_accuracy: 0.7659\n",
      "\n",
      "Epoch 00186: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 187/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5744 - categorical_accuracy: 0.7891 - val_loss: 0.7114 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00187: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 188/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5739 - categorical_accuracy: 0.7879 - val_loss: 0.7092 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00188: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 189/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5666 - categorical_accuracy: 0.7919 - val_loss: 0.7442 - val_categorical_accuracy: 0.7570\n",
      "\n",
      "Epoch 00189: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 190/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5658 - categorical_accuracy: 0.7930 - val_loss: 0.7143 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00190: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 191/200\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.5640 - categorical_accuracy: 0.7933 - val_loss: 0.7267 - val_categorical_accuracy: 0.7690\n",
      "\n",
      "Epoch 00191: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 192/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5678 - categorical_accuracy: 0.7900 - val_loss: 0.7477 - val_categorical_accuracy: 0.7567\n",
      "\n",
      "Epoch 00192: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 193/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5634 - categorical_accuracy: 0.7917 - val_loss: 0.7157 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00193: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 194/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5672 - categorical_accuracy: 0.7895 - val_loss: 0.7916 - val_categorical_accuracy: 0.7344\n",
      "\n",
      "Epoch 00194: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 195/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5598 - categorical_accuracy: 0.7929 - val_loss: 0.7467 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00195: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 196/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5667 - categorical_accuracy: 0.7901 - val_loss: 0.7253 - val_categorical_accuracy: 0.7810\n",
      "\n",
      "Epoch 00196: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 197/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5570 - categorical_accuracy: 0.7957 - val_loss: 0.7318 - val_categorical_accuracy: 0.7692\n",
      "\n",
      "Epoch 00197: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 198/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5499 - categorical_accuracy: 0.7971 - val_loss: 0.7350 - val_categorical_accuracy: 0.7696\n",
      "\n",
      "Epoch 00198: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 199/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5512 - categorical_accuracy: 0.7962 - val_loss: 0.7227 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00199: val_categorical_accuracy did not improve from 0.78567\n",
      "Epoch 200/200\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.5564 - categorical_accuracy: 0.7927 - val_loss: 0.7267 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00200: val_categorical_accuracy did not improve from 0.78567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa934377a50>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_toplayers()\n",
    "model.fit(train_gen,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_data = val_gen,\n",
    "          validation_steps = val_steps,\n",
    "          epochs=200,\n",
    "          callbacks=[checkpoint, reduceLROnPlat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:31:19.226242Z",
     "start_time": "2021-01-03T19:31:18.380632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 18ms/step - loss: 0.6405 - categorical_accuracy: 0.7580\n",
      "train loss 0.6404966711997986 accuracy 0.7579708695411682\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_fp)\n",
    "print('train loss {} accuracy {}'.format(*model.evaluate(train_gen, steps=train_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:31:19.733771Z",
     "start_time": "2021-01-03T19:31:19.227708Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6686 - categorical_accuracy: 0.7862\n",
      "train loss 0.668616771697998 accuracy 0.7861902713775635\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_fp)\n",
    "print('train loss {} accuracy {}'.format(*model.evaluate(val_gen, steps=val_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Tweaking for Recall using top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:29:21.220352Z",
     "start_time": "2021-01-10T16:29:21.176072Z"
    }
   },
   "outputs": [],
   "source": [
    "# val_data = (data.filter(lambda x: x['name'] in x_val.values)\n",
    "#             .map(lambda x: {x['name']: np.asarray(x['value'])})\n",
    "#             .fold(binop=lambda x, y: join_dict(x, y),\n",
    "#                   combine=lambda x, y: join_dict(x, y))\n",
    "#             .compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:29:21.223476Z",
     "start_time": "2021-01-10T16:29:21.221796Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Make train labels to dictionary for easier implementation\n",
    "# train_labels = pd.Series(y_train.values,index=x_train.values).to_dict()\n",
    "# val_labels = pd.Series(y_val.values, index=x_val.values).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:47:09.142345Z",
     "start_time": "2021-01-11T06:47:09.133814Z"
    }
   },
   "outputs": [],
   "source": [
    "y_val_labels = [val_labels[i] for i in val_data.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:29:21.318431Z",
     "start_time": "2021-01-10T16:29:21.234798Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_toplayers():\n",
    "    \"\"\"Create top layer using inceptionb resnet v2 base\"\"\"\n",
    "    # Create input layer based on the output of the convolutional base\n",
    "    lyr_input = Input(shape=(base.layers[-1].output.shape.as_list()[1], ))\n",
    "\n",
    "    # Add Dense\n",
    "    lyr_dense1 = Dense(1024, activation='relu')(lyr_input)\n",
    "    lyr_dense2 = Dense(512, activation='relu')(lyr_dense1)\n",
    "    lyr_dense3 = Dense(256, activation='relu')(lyr_dense2)\n",
    "\n",
    "\n",
    "    # Create output layer\n",
    "    output = Dense(5, activation='softmax')(lyr_dense3)\n",
    "\n",
    "    model = Model(inputs=[lyr_input], outputs=[output])\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model_fp = 'tune/add_more_dense4.h5'\n",
    "model = create_toplayers()\n",
    "model.load_weights(model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using val data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:29:23.880298Z",
     "start_time": "2021-01-10T16:29:21.319670Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - 2s 3ms/step - loss: 0.6685 - categorical_accuracy: 0.7861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6684679388999939, 0.7860968708992004]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(list(val_data.values())), to_categorical(y_val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking treshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:47:26.353037Z",
     "start_time": "2021-01-11T06:47:25.055384Z"
    }
   },
   "outputs": [],
   "source": [
    "#Filter out those 1s counted as 0s to be 1s\n",
    "##check 1s softmax score distribution of 1s that were tagged as 0s to define threshold.\n",
    "y_pred_softmax = model.predict(np.array(list(val_data.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:47:26.360215Z",
     "start_time": "2021-01-11T06:47:26.355539Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true_softmax = to_categorical(y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:47:27.263870Z",
     "start_time": "2021-01-11T06:47:27.257530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 13063, 2: 2677, 1: 1252, 4: 370, 3: 375})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(np.argmax(y_true_softmax, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:47:29.980456Z",
     "start_time": "2021-01-11T06:47:29.941897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7860968596718724"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_pred_softmax, axis=1)\n",
    "y_true = np.argmax(y_true_softmax, axis=1)\n",
    "sum(y_pred == y_true)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:59:36.159498Z",
     "start_time": "2021-01-11T06:59:36.155247Z"
    }
   },
   "outputs": [],
   "source": [
    "def y_pred_threshold(y_pred_softmax):\n",
    "    y_pred = []\n",
    "    for i in y_pred_softmax:\n",
    "        if np.argmax(i) in [2,3,4]:\n",
    "             y_pred.append(np.argmax(i))\n",
    "        elif i[0] > 0.6:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:59:36.727300Z",
     "start_time": "2021-01-11T06:59:36.514976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11865   835   308    18    37]\n",
      " [  981   183    86     1     1]\n",
      " [ 1189   389   954    78    67]\n",
      " [   48    37   150   107    33]\n",
      " [   45    20    81    36   188]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.840     0.908     0.873     13063\n",
      "           1      0.125     0.146     0.135      1252\n",
      "           2      0.604     0.356     0.448      2677\n",
      "           3      0.446     0.285     0.348       375\n",
      "           4      0.577     0.508     0.540       370\n",
      "\n",
      "    accuracy                          0.750     17737\n",
      "   macro avg      0.518     0.441     0.469     17737\n",
      "weighted avg      0.740     0.750     0.739     17737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, \n",
    "                               y_pred_threshold(y_pred_softmax)))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, \n",
    "                                    y_pred_threshold(y_pred_softmax), \n",
    "                                    digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:03:00.154493Z",
     "start_time": "2021-01-10T17:03:00.116472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(0, 4)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "columns=[0,1,2,3,4]\n",
    "df_features = pd.DataFrame(y_pred_softmax, columns=columns)\n",
    "\n",
    "for i in combinations(df_features.columns, 2):\n",
    "    print(i)\n",
    "    df_features['add_'+str(i)] = df_features.loc[:,i[0]] + df_features.loc[:,i[1]]\n",
    "    df_features['diff_'+str(i)] = df_features.loc[:,i[0]] - df_features.loc[:,i[1]]\n",
    "    df_features['div_'+str(i)] = df_features.loc[:,i[0]] / df_features.loc[:,i[1]]\n",
    "    df_features['mul_'+str(i)] = df_features.loc[:,i[0]] * df_features.loc[:,i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:20:54.858606Z",
     "start_time": "2021-01-10T17:03:30.625467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "XX = df_features\n",
    "y = y_val_labels\n",
    "\n",
    "lahat_training = pd.DataFrame()\n",
    "lahat_test = pd.DataFrame()\n",
    "for seedN in tqdm(range(1,20,1)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, \n",
    "                                                        test_size=0.25, \n",
    "                                                        random_state=seedN,\n",
    " #                                                      stratify=True\n",
    "                                                       )\n",
    "    training_accuracy = []  \n",
    "    test_accuracy = []\n",
    "    maxdepth_settings = range(1, 15) # try n_neighbors from 1 to 10\n",
    "\n",
    "    for depth in maxdepth_settings:   \n",
    "        reg = RandomForestClassifier(random_state=0, max_depth=depth) # build the model \n",
    "        reg.fit(X_train, y_train) #clf = KNeighborsClassifier(n_neighbors=n_neighbors    \n",
    "        training_accuracy.append(reg.score(X_train, y_train)) # record training set accuracy  \n",
    "        test_accuracy.append(reg.score(X_test, y_test)) # record generalization accuracy    \n",
    "    lahat_training[seedN]=training_accuracy\n",
    "    lahat_test[seedN] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:21:52.279830Z",
     "start_time": "2021-01-10T17:21:52.073156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Average Test Set Achieved = 0.785486\n",
      "Max_Depth = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.errorbar(maxdepth_settings, lahat_training.mean(axis=1),\n",
    "             yerr=lahat_training.std(axis=1), label=\"training accuracy\")\n",
    "plt.errorbar(maxdepth_settings, lahat_test.mean(axis=1),\n",
    "             yerr=lahat_test.std(axis=1), label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Max_Depth\")\n",
    "plt.legend()\n",
    "bestdepth=np.argmax(lahat_test.mean(axis=1))+1\n",
    "print(\"Highest Average Test Set Achieved = %f\" % np.amax(lahat_test.mean(axis=1)))\n",
    "print(\"Max_Depth = %d\" %bestdepth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:28:18.795157Z",
     "start_time": "2021-01-10T17:26:14.975386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "XX = df_features\n",
    "y = y_val_labels\n",
    "\n",
    "lahat_training = pd.DataFrame()\n",
    "lahat_test = pd.DataFrame()\n",
    "for seedN in tqdm(range(1,20,1)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, \n",
    "                                                        test_size=0.25, \n",
    "                                                        random_state=seedN,\n",
    " #                                                      stratify=True\n",
    "                                                       )\n",
    "    training_accuracy = []  \n",
    "    test_accuracy = []\n",
    "    maxdepth_settings = range(1, 5) # try n_neighbors from 1 to 10\n",
    "\n",
    "    for depth in maxdepth_settings:   \n",
    "        reg = RandomForestClassifier(random_state=0, max_depth=depth) # build the model \n",
    "        reg.fit(X_train, y_train) #clf = KNeighborsClassifier(n_neighbors=n_neighbors    \n",
    "        training_accuracy.append(reg.score(X_train, y_train)) # record training set accuracy  \n",
    "        test_accuracy.append(reg.score(X_test, y_test)) # record generalization accuracy    \n",
    "    lahat_training[seedN]=training_accuracy\n",
    "    lahat_test[seedN] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:28:18.981492Z",
     "start_time": "2021-01-10T17:28:18.796482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Average Test Set Achieved = 0.785486\n",
      "Max_Depth = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.errorbar(maxdepth_settings, lahat_training.mean(axis=1),\n",
    "             yerr=lahat_training.std(axis=1), label=\"training accuracy\")\n",
    "plt.errorbar(maxdepth_settings, lahat_test.mean(axis=1),\n",
    "             yerr=lahat_test.std(axis=1), label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Max_Depth\")\n",
    "plt.legend()\n",
    "bestdepth=np.argmax(lahat_test.mean(axis=1))+1\n",
    "print(\"Highest Average Test Set Achieved = %f\" % np.amax(lahat_test.mean(axis=1)))\n",
    "print(\"Max_Depth = %d\" %bestdepth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:30:06.652008Z",
     "start_time": "2021-01-10T17:30:06.648129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([            0,             1,             2,             3,\n",
       "                   4,  'add_(0, 1)', 'diff_(0, 1)',  'div_(0, 1)',\n",
       "        'mul_(0, 1)',  'add_(0, 2)', 'diff_(0, 2)',  'div_(0, 2)',\n",
       "        'mul_(0, 2)',  'add_(0, 3)', 'diff_(0, 3)',  'div_(0, 3)',\n",
       "        'mul_(0, 3)',  'add_(0, 4)', 'diff_(0, 4)',  'div_(0, 4)',\n",
       "        'mul_(0, 4)',  'add_(1, 2)', 'diff_(1, 2)',  'div_(1, 2)',\n",
       "        'mul_(1, 2)',  'add_(1, 3)', 'diff_(1, 3)',  'div_(1, 3)',\n",
       "        'mul_(1, 3)',  'add_(1, 4)', 'diff_(1, 4)',  'div_(1, 4)',\n",
       "        'mul_(1, 4)',  'add_(2, 3)', 'diff_(2, 3)',  'div_(2, 3)',\n",
       "        'mul_(2, 3)',  'add_(2, 4)', 'diff_(2, 4)',  'div_(2, 4)',\n",
       "        'mul_(2, 4)',  'add_(3, 4)', 'diff_(3, 4)',  'div_(3, 4)',\n",
       "        'mul_(3, 4)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:32:00.015583Z",
     "start_time": "2021-01-10T17:31:59.997805Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.11983990640239658, 'diff_(0, 4)'),\n",
       " (0.09674850161794067, 'diff_(0, 2)'),\n",
       " (0.09327456084304493, 'add_(0, 1)'),\n",
       " (0.08300858573250132, 'div_(0, 2)'),\n",
       " (0.07429904869013532, 'diff_(0, 3)'),\n",
       " (0.07246665135509288, 0),\n",
       " (0.047287512744826204, 'add_(2, 3)'),\n",
       " (0.04619690819247218, 'add_(2, 4)'),\n",
       " (0.03456212961558206, 'diff_(0, 1)'),\n",
       " (0.03130174719367346, 'add_(0, 3)'),\n",
       " (0.0308814336550682, 'add_(0, 4)'),\n",
       " (0.03086639185753681, 'div_(0, 3)'),\n",
       " (0.02271106256682852, 'add_(3, 4)'),\n",
       " (0.022608269200231478, 'mul_(2, 3)'),\n",
       " (0.017399986541733706, 'div_(0, 4)'),\n",
       " (0.015718709336691703, 'diff_(2, 4)'),\n",
       " (0.015677130941726517, 2),\n",
       " (0.014698123835024429, 'diff_(1, 4)'),\n",
       " (0.012700699040938674, 'mul_(3, 4)'),\n",
       " (0.01258203724746857, 'add_(1, 2)'),\n",
       " (0.00939953467209968, 'diff_(2, 3)'),\n",
       " (0.008337780770352824, 'div_(1, 4)'),\n",
       " (0.008244817753215249, 'add_(1, 4)'),\n",
       " (0.008082312112017275, 'diff_(1, 3)'),\n",
       " (0.007958960467830968, 4),\n",
       " (0.007911676382958381, 'div_(2, 4)'),\n",
       " (0.00783030775443604, 'diff_(1, 2)'),\n",
       " (0.00646282471836943, 'diff_(3, 4)'),\n",
       " (0.00623755657378046, 'mul_(0, 2)'),\n",
       " (0.005830401780153926, 'mul_(1, 2)'),\n",
       " (0.004479745877546375, 'mul_(2, 4)'),\n",
       " (0.0036299512461427357, 'add_(0, 2)'),\n",
       " (0.0030116359317819454, 3),\n",
       " (0.002842878636544219, 'div_(0, 1)'),\n",
       " (0.002531735119301075, 'mul_(0, 1)'),\n",
       " (0.0024083227646113634, 'add_(1, 3)'),\n",
       " (0.001996813466450614, 'div_(1, 3)'),\n",
       " (0.0016999990213750946, 'div_(3, 4)'),\n",
       " (0.0013477249299325134, 'mul_(0, 3)'),\n",
       " (0.0011012127202537285, 'mul_(0, 4)'),\n",
       " (0.0009804764867899379, 'mul_(1, 3)'),\n",
       " (0.0008966991272048311, 1),\n",
       " (0.0007429403098939373, 'div_(2, 3)'),\n",
       " (0.0006514448993461909, 'mul_(1, 4)'),\n",
       " (0.0005528498666969823, 'div_(1, 2)')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(i,j) for i,j in zip(reg.feature_importances_, df_features.columns)], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:39:00.050055Z",
     "start_time": "2021-01-10T17:38:59.932877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:37:17.312664Z",
     "start_time": "2021-01-10T17:37:17.298687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX = df_features\n",
    "y = y_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:39:40.279856Z",
     "start_time": "2021-01-10T17:39:40.120518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12716     0   329     3    15]\n",
      " [ 1156     0    95     0     1]\n",
      " [ 1574     0  1047    28    28]\n",
      " [   87     0   197    73    18]\n",
      " [   71     0   118    20   161]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.815     0.973     0.887     13063\n",
      "           1      0.000     0.000     0.000      1252\n",
      "           2      0.586     0.391     0.469      2677\n",
      "           3      0.589     0.195     0.293       375\n",
      "           4      0.722     0.435     0.543       370\n",
      "\n",
      "    accuracy                          0.789     17737\n",
      "   macro avg      0.542     0.399     0.438     17737\n",
      "weighted avg      0.716     0.789     0.742     17737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_true = y_val_labels\n",
    "y_pred = reg.predict(df_features)\n",
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untouched Precision Recall F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:04:19.943273Z",
     "start_time": "2021-01-07T09:04:19.902720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7829028983872787, 0.7829028983872787, 0.7829028983872787, None)\n",
      "(0.517461496555691, 0.42250464611229105, 0.44358317115476015, None)\n",
      "(0.7226330765330707, 0.7829028983872787, 0.7381765527261192, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y_test_labels\n",
    "y_pred = y_test_predict\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:04:23.481367Z",
     "start_time": "2021-01-07T09:04:23.436671Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12645    33   328    15    35]\n",
      " [ 1119    11    91     6     4]\n",
      " [ 1536    14   902    95    66]\n",
      " [   91     1   178   122    40]\n",
      " [   57     0    97    44   204]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.819     0.969     0.887     13056\n",
      "           1      0.186     0.009     0.017      1231\n",
      "           2      0.565     0.345     0.429      2613\n",
      "           3      0.433     0.282     0.342       432\n",
      "           4      0.585     0.507     0.543       402\n",
      "\n",
      "    accuracy                          0.783     17734\n",
      "   macro avg      0.517     0.423     0.444     17734\n",
      "weighted avg      0.723     0.783     0.738     17734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of top model BALANCED ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model using different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:55:59.815931Z",
     "start_time": "2021-01-06T16:55:59.811698Z"
    }
   },
   "outputs": [],
   "source": [
    "#PCC\n",
    "#Specificity\n",
    "#Other S measure\n",
    "#Precision\n",
    "#Recall\n",
    "#Weighterd P and R\n",
    "#F1\n",
    "#regarding accuracy, sensitivity, and specificity \n",
    "#are 87.83%, 77.81%, and 93.88%, respectively, \n",
    "#which are better than 86.10%, 73.24%, and 93.81%, "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:06:05.460455Z",
     "start_time": "2021-01-06T16:06:05.457183Z"
    }
   },
   "source": [
    "Prepare test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T08:20:40.197208Z",
     "start_time": "2021-01-07T08:19:42.640284Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test image name and image vector dictionary\n",
    "test_data = (data.filter(lambda x: x['name'] in x_test.values)\n",
    "            .map(lambda x: {x['name']: np.asarray(x['value'])})\n",
    "            .fold(binop=lambda x, y: join_dict(x, y),\n",
    "                  combine=lambda x, y: join_dict(x, y))\n",
    "            .compute())\n",
    "\n",
    "#Test labels\n",
    "test_labels = pd.Series(y_test.values, index=x_test.values).to_dict()\n",
    "y_test_labels = [test_labels[i] for i in test_data.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T08:51:58.532068Z",
     "start_time": "2021-01-07T08:51:58.470845Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_toplayers():\n",
    "    \"\"\"Create top layer using inceptionb resnet v2 base\"\"\"\n",
    "    # Create input layer based on the output of the convolutional base\n",
    "    lyr_input = Input(shape=(base.layers[-1].output.shape.as_list()[1], ))\n",
    "\n",
    "    # Add Dense\n",
    "    lyr_dense1 = Dense(1024, activation='relu')(lyr_input)\n",
    "    lyr_dense2 = Dense(512, activation='relu')(lyr_dense1)\n",
    "    lyr_dense3 = Dense(256, activation='relu')(lyr_dense2)\n",
    "\n",
    "\n",
    "    # Create output layer\n",
    "    output = Dense(5, activation='softmax')(lyr_dense3)\n",
    "\n",
    "    model = Model(inputs=[lyr_input], outputs=[output])\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model_fp = 'tune/add_more_dense4.h5'\n",
    "model = create_toplayers()\n",
    "model.load_weights(model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T08:54:58.675866Z",
     "start_time": "2021-01-07T08:54:56.463167Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - 2s 3ms/step - loss: 0.6702 - categorical_accuracy: 0.7829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6701775789260864, 0.7829028964042664]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(list(test_data.values())), to_categorical(y_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:03:57.475919Z",
     "start_time": "2021-01-07T09:03:56.146647Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_predict = np.argmax(model.predict(np.array(list(test_data.values()))), \n",
    "                           axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T15:35:57.931469Z",
     "start_time": "2021-01-06T15:35:57.929395Z"
    }
   },
   "source": [
    "### PCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T08:55:12.220031Z",
     "start_time": "2021-01-07T08:55:12.215896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pcc : 57.07%\n",
      "Accuracy (1.25Pcc) required : 71.33%\n"
     ]
    }
   ],
   "source": [
    "values = np.bincount(y_train)\n",
    "Pcc = ((values/values.sum())**2).sum()\n",
    "print('Pcc : %.2f'%(Pcc*100) +'%')\n",
    "print('Accuracy (1.25Pcc) required : %.2f'%(Pcc*100*1.25) +'%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Assuming all are 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T08:55:23.028912Z",
     "start_time": "2021-01-07T08:55:23.013244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73682826, 0.14774521, 0.06993611, 0.0214581 , 0.02403232])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "np.array(list(Counter(y_train).values())) / sum(list(Counter(y_train).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Recall F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:04:19.943273Z",
     "start_time": "2021-01-07T09:04:19.902720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7829028983872787, 0.7829028983872787, 0.7829028983872787, None)\n",
      "(0.517461496555691, 0.42250464611229105, 0.44358317115476015, None)\n",
      "(0.7226330765330707, 0.7829028983872787, 0.7381765527261192, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y_test_labels\n",
    "y_pred = y_test_predict\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:04:23.481367Z",
     "start_time": "2021-01-07T09:04:23.436671Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12645    33   328    15    35]\n",
      " [ 1119    11    91     6     4]\n",
      " [ 1536    14   902    95    66]\n",
      " [   91     1   178   122    40]\n",
      " [   57     0    97    44   204]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.819     0.969     0.887     13056\n",
      "           1      0.186     0.009     0.017      1231\n",
      "           2      0.565     0.345     0.429      2613\n",
      "           3      0.433     0.282     0.342       432\n",
      "           4      0.585     0.507     0.543       402\n",
      "\n",
      "    accuracy                          0.783     17734\n",
      "   macro avg      0.517     0.423     0.444     17734\n",
      "weighted avg      0.723     0.783     0.738     17734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:43:22.338617Z",
     "start_time": "2021-01-10T17:43:22.335571Z"
    }
   },
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIGMOID!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST Remove first Dropout layer and Add Dense layer, Reduce Batch size perform sampling using max as n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:14:07.907291Z",
     "start_time": "2021-01-10T19:14:07.902613Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_toplayers():\n",
    "    \"\"\"Create top layer using inceptionb resnet v2 base\"\"\"\n",
    "    # Create input layer based on the output of the convolutional base\n",
    "    lyr_input = Input(shape=(base.layers[-1].output.shape.as_list()[1], ))\n",
    "\n",
    "    # Add Dense\n",
    "    lyr_dense1 = Dense(1024, activation='relu')(lyr_input)\n",
    "    lyr_dense2 = Dense(512, activation='relu')(lyr_dense1)\n",
    "    lyr_dense3 = Dense(256, activation='relu')(lyr_dense2)\n",
    "\n",
    "\n",
    "    # Create output layer\n",
    "    output = Dense(5, activation='sigmoid')(lyr_dense3)\n",
    "\n",
    "    model = Model(inputs=[lyr_input], outputs=[output])\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:14:07.912251Z",
     "start_time": "2021-01-10T19:14:07.908972Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_gen = data_generator(train_labels, train_data, batch_size)\n",
    "val_gen = data_generator(val_labels, val_data, batch_size)\n",
    "train_steps = len(train_labels) // batch_size\n",
    "val_steps = len(val_labels) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:14:07.917529Z",
     "start_time": "2021-01-10T19:14:07.913549Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_fp = 'tune/add_more_dense4_sigmoid.h5'\n",
    "checkpoint = ModelCheckpoint(model_fp,\n",
    "                             monitor='val_categorical_accuracy',\n",
    "                             verbose=1, save_best_only=True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_categorical_accuracy',\n",
    "                                   factor=0.8,\n",
    "                                   patience=3,\n",
    "                                   cooldown=3,\n",
    "                                   verbose=1,\n",
    "                                   min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_categorical_accuracy\", \n",
    "                      verbose=1,\n",
    "                      patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:22:36.246714Z",
     "start_time": "2021-01-10T19:14:07.918847Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 1.1239 - categorical_accuracy: 0.6240\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.73667, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 72ms/step - loss: 1.1220 - categorical_accuracy: 0.6245 - val_loss: 1.0231 - val_categorical_accuracy: 0.7367 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.0167 - categorical_accuracy: 0.6399\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.73667\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 1.0167 - categorical_accuracy: 0.6399 - val_loss: 0.8120 - val_categorical_accuracy: 0.7356 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9898 - categorical_accuracy: 0.6408\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.73667 to 0.73685, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.9898 - categorical_accuracy: 0.6408 - val_loss: 0.8319 - val_categorical_accuracy: 0.7368 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9759 - categorical_accuracy: 0.6421\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.73685 to 0.73868, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.9759 - categorical_accuracy: 0.6421 - val_loss: 0.7804 - val_categorical_accuracy: 0.7387 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.9612 - categorical_accuracy: 0.6425\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.73868\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.9612 - categorical_accuracy: 0.6415 - val_loss: 0.8021 - val_categorical_accuracy: 0.7370 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9515 - categorical_accuracy: 0.6469\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.73868 to 0.73955, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.9515 - categorical_accuracy: 0.6469 - val_loss: 0.7836 - val_categorical_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.9430 - categorical_accuracy: 0.6480\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.73955 to 0.74104, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.9435 - categorical_accuracy: 0.6482 - val_loss: 0.7871 - val_categorical_accuracy: 0.7410 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9337 - categorical_accuracy: 0.6523\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.74104\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.9337 - categorical_accuracy: 0.6523 - val_loss: 0.8120 - val_categorical_accuracy: 0.7359 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9307 - categorical_accuracy: 0.6541\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.74104 to 0.74655, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.9307 - categorical_accuracy: 0.6541 - val_loss: 0.7830 - val_categorical_accuracy: 0.7466 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9212 - categorical_accuracy: 0.6557\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.74655 to 0.74839, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.9212 - categorical_accuracy: 0.6557 - val_loss: 0.7665 - val_categorical_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9169 - categorical_accuracy: 0.6571\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.74839\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.9169 - categorical_accuracy: 0.6571 - val_loss: 0.8526 - val_categorical_accuracy: 0.7305 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.9088 - categorical_accuracy: 0.6613\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.74839\n",
      "37/37 [==============================] - 2s 64ms/step - loss: 0.9101 - categorical_accuracy: 0.6606 - val_loss: 1.0969 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9171 - categorical_accuracy: 0.6556\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.74839 to 0.74960, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 72ms/step - loss: 0.9171 - categorical_accuracy: 0.6556 - val_loss: 0.7445 - val_categorical_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.9093 - categorical_accuracy: 0.6603\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.74960 to 0.75523, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.9071 - categorical_accuracy: 0.6609 - val_loss: 0.7472 - val_categorical_accuracy: 0.7552 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9038 - categorical_accuracy: 0.6655\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.75523\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.9038 - categorical_accuracy: 0.6655 - val_loss: 0.7311 - val_categorical_accuracy: 0.7531 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.9017 - categorical_accuracy: 0.6672\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.75523 to 0.75867, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.8999 - categorical_accuracy: 0.6672 - val_loss: 0.7536 - val_categorical_accuracy: 0.7587 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.8964 - categorical_accuracy: 0.6660\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.75867\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.8947 - categorical_accuracy: 0.6664 - val_loss: 0.7294 - val_categorical_accuracy: 0.7584 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8873 - categorical_accuracy: 0.6699\n",
      "Epoch 00018: val_categorical_accuracy improved from 0.75867 to 0.76201, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 68ms/step - loss: 0.8873 - categorical_accuracy: 0.6699 - val_loss: 0.7172 - val_categorical_accuracy: 0.7620 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8944 - categorical_accuracy: 0.6678\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.76201\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.8944 - categorical_accuracy: 0.6678 - val_loss: 0.7411 - val_categorical_accuracy: 0.7578 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8805 - categorical_accuracy: 0.6735\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.76201\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.8805 - categorical_accuracy: 0.6735 - val_loss: 0.7859 - val_categorical_accuracy: 0.7346 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8805 - categorical_accuracy: 0.6733\n",
      "Epoch 00021: val_categorical_accuracy improved from 0.76201 to 0.76252, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.8805 - categorical_accuracy: 0.6733 - val_loss: 0.7176 - val_categorical_accuracy: 0.7625 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8806 - categorical_accuracy: 0.6695\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.76252\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.8806 - categorical_accuracy: 0.6695 - val_loss: 0.7082 - val_categorical_accuracy: 0.7605 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.8763 - categorical_accuracy: 0.6731\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.76252\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.8756 - categorical_accuracy: 0.6735 - val_loss: 0.7070 - val_categorical_accuracy: 0.7583 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8679 - categorical_accuracy: 0.6790\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.76252\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.8679 - categorical_accuracy: 0.6790 - val_loss: 0.7816 - val_categorical_accuracy: 0.7450 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8471 - categorical_accuracy: 0.6824\n",
      "Epoch 00025: val_categorical_accuracy improved from 0.76252 to 0.76631, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.8471 - categorical_accuracy: 0.6824 - val_loss: 0.7003 - val_categorical_accuracy: 0.7663 - lr: 8.0000e-04\n",
      "Epoch 26/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8489 - categorical_accuracy: 0.6844\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.76631\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.8489 - categorical_accuracy: 0.6844 - val_loss: 0.7155 - val_categorical_accuracy: 0.7638 - lr: 8.0000e-04\n",
      "Epoch 27/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8448 - categorical_accuracy: 0.6831\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.76631\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.8448 - categorical_accuracy: 0.6831 - val_loss: 0.6962 - val_categorical_accuracy: 0.7651 - lr: 8.0000e-04\n",
      "Epoch 28/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8431 - categorical_accuracy: 0.6827\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.76631\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.8431 - categorical_accuracy: 0.6827 - val_loss: 0.7000 - val_categorical_accuracy: 0.7594 - lr: 8.0000e-04\n",
      "Epoch 29/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8390 - categorical_accuracy: 0.6884\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.76631\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.8390 - categorical_accuracy: 0.6884 - val_loss: 0.7373 - val_categorical_accuracy: 0.7577 - lr: 8.0000e-04\n",
      "Epoch 30/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8281 - categorical_accuracy: 0.6919\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.76631\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.8281 - categorical_accuracy: 0.6919 - val_loss: 0.6947 - val_categorical_accuracy: 0.7657 - lr: 6.4000e-04\n",
      "Epoch 31/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8215 - categorical_accuracy: 0.6929\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.76631\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.8215 - categorical_accuracy: 0.6929 - val_loss: 0.7112 - val_categorical_accuracy: 0.7594 - lr: 6.4000e-04\n",
      "Epoch 32/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8190 - categorical_accuracy: 0.6945\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.76631\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.8190 - categorical_accuracy: 0.6945 - val_loss: 0.7309 - val_categorical_accuracy: 0.7506 - lr: 6.4000e-04\n",
      "Epoch 33/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.8182 - categorical_accuracy: 0.6963\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.76631\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.8209 - categorical_accuracy: 0.6952 - val_loss: 0.7704 - val_categorical_accuracy: 0.7493 - lr: 6.4000e-04\n",
      "Epoch 34/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8153 - categorical_accuracy: 0.6963\n",
      "Epoch 00034: val_categorical_accuracy improved from 0.76631 to 0.77085, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.8153 - categorical_accuracy: 0.6963 - val_loss: 0.6894 - val_categorical_accuracy: 0.7709 - lr: 6.4000e-04\n",
      "Epoch 35/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8126 - categorical_accuracy: 0.6957\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.77085\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.8126 - categorical_accuracy: 0.6957 - val_loss: 0.7549 - val_categorical_accuracy: 0.7636 - lr: 6.4000e-04\n",
      "Epoch 36/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.8107 - categorical_accuracy: 0.6983\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.77085\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.8108 - categorical_accuracy: 0.6984 - val_loss: 0.6848 - val_categorical_accuracy: 0.7705 - lr: 6.4000e-04\n",
      "Epoch 37/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.8075 - categorical_accuracy: 0.6986\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.77085\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.8110 - categorical_accuracy: 0.6975 - val_loss: 0.7189 - val_categorical_accuracy: 0.7680 - lr: 6.4000e-04\n",
      "Epoch 38/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7948 - categorical_accuracy: 0.7042\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.77085\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7958 - categorical_accuracy: 0.7037 - val_loss: 0.6907 - val_categorical_accuracy: 0.7680 - lr: 5.1200e-04\n",
      "Epoch 39/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7919 - categorical_accuracy: 0.7034\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.77085\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7919 - categorical_accuracy: 0.7034 - val_loss: 0.6973 - val_categorical_accuracy: 0.7656 - lr: 5.1200e-04\n",
      "Epoch 40/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7930 - categorical_accuracy: 0.7027\n",
      "Epoch 00040: val_categorical_accuracy improved from 0.77085 to 0.77097, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.7920 - categorical_accuracy: 0.7036 - val_loss: 0.6858 - val_categorical_accuracy: 0.7710 - lr: 5.1200e-04\n",
      "Epoch 41/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7910 - categorical_accuracy: 0.7026\n",
      "Epoch 00041: val_categorical_accuracy improved from 0.77097 to 0.77619, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.7910 - categorical_accuracy: 0.7026 - val_loss: 0.6741 - val_categorical_accuracy: 0.7762 - lr: 5.1200e-04\n",
      "Epoch 42/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7882 - categorical_accuracy: 0.7056\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.77619\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.7882 - categorical_accuracy: 0.7056 - val_loss: 0.6922 - val_categorical_accuracy: 0.7615 - lr: 5.1200e-04\n",
      "Epoch 43/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7849 - categorical_accuracy: 0.7051\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.77619\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7849 - categorical_accuracy: 0.7051 - val_loss: 0.7483 - val_categorical_accuracy: 0.7464 - lr: 5.1200e-04\n",
      "Epoch 44/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7841 - categorical_accuracy: 0.7073\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.77619\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.7841 - categorical_accuracy: 0.7073 - val_loss: 0.6930 - val_categorical_accuracy: 0.7663 - lr: 5.1200e-04\n",
      "Epoch 45/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7753 - categorical_accuracy: 0.7083\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.77619\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7752 - categorical_accuracy: 0.7086 - val_loss: 0.6738 - val_categorical_accuracy: 0.7747 - lr: 4.0960e-04\n",
      "Epoch 46/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7715 - categorical_accuracy: 0.7105\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.77619\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.7715 - categorical_accuracy: 0.7105 - val_loss: 0.7196 - val_categorical_accuracy: 0.7561 - lr: 4.0960e-04\n",
      "Epoch 47/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7675 - categorical_accuracy: 0.7118\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.77619\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7675 - categorical_accuracy: 0.7118 - val_loss: 0.6701 - val_categorical_accuracy: 0.7759 - lr: 4.0960e-04\n",
      "Epoch 48/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7668 - categorical_accuracy: 0.7108\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.77619\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.7675 - categorical_accuracy: 0.7103 - val_loss: 0.7110 - val_categorical_accuracy: 0.7634 - lr: 4.0960e-04\n",
      "Epoch 49/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7625 - categorical_accuracy: 0.7142\n",
      "Epoch 00049: val_categorical_accuracy improved from 0.77619 to 0.77826, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7625 - categorical_accuracy: 0.7142 - val_loss: 0.6746 - val_categorical_accuracy: 0.7783 - lr: 4.0960e-04\n",
      "Epoch 50/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7635 - categorical_accuracy: 0.7140\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.77826\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7630 - categorical_accuracy: 0.7143 - val_loss: 0.7030 - val_categorical_accuracy: 0.7660 - lr: 4.0960e-04\n",
      "Epoch 51/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7627 - categorical_accuracy: 0.7130\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.77826\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7627 - categorical_accuracy: 0.7130 - val_loss: 0.6759 - val_categorical_accuracy: 0.7749 - lr: 4.0960e-04\n",
      "Epoch 52/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7611 - categorical_accuracy: 0.7128\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.77826\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7611 - categorical_accuracy: 0.7128 - val_loss: 0.6826 - val_categorical_accuracy: 0.7762 - lr: 4.0960e-04\n",
      "Epoch 53/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7489 - categorical_accuracy: 0.7173\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.77826\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7500 - categorical_accuracy: 0.7171 - val_loss: 0.7049 - val_categorical_accuracy: 0.7657 - lr: 3.2768e-04\n",
      "Epoch 54/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7473 - categorical_accuracy: 0.7179\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.77826\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.7473 - categorical_accuracy: 0.7186 - val_loss: 0.6685 - val_categorical_accuracy: 0.7757 - lr: 3.2768e-04\n",
      "Epoch 55/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7450 - categorical_accuracy: 0.7193\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.77826\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.7450 - categorical_accuracy: 0.7193 - val_loss: 0.7487 - val_categorical_accuracy: 0.7394 - lr: 3.2768e-04\n",
      "Epoch 56/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7418 - categorical_accuracy: 0.7201\n",
      "Epoch 00056: val_categorical_accuracy improved from 0.77826 to 0.78091, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7415 - categorical_accuracy: 0.7201 - val_loss: 0.6631 - val_categorical_accuracy: 0.7809 - lr: 3.2768e-04\n",
      "Epoch 57/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7411 - categorical_accuracy: 0.7206\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.78091\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.7397 - categorical_accuracy: 0.7211 - val_loss: 0.6710 - val_categorical_accuracy: 0.7736 - lr: 3.2768e-04\n",
      "Epoch 58/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7416 - categorical_accuracy: 0.7223\n",
      "Epoch 00058: val_categorical_accuracy improved from 0.78091 to 0.78108, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7416 - categorical_accuracy: 0.7223 - val_loss: 0.6700 - val_categorical_accuracy: 0.7811 - lr: 3.2768e-04\n",
      "Epoch 59/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7369 - categorical_accuracy: 0.7228\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.78108\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.7369 - categorical_accuracy: 0.7228 - val_loss: 0.6731 - val_categorical_accuracy: 0.7765 - lr: 3.2768e-04\n",
      "Epoch 60/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7371 - categorical_accuracy: 0.7209\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.78108\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.7371 - categorical_accuracy: 0.7209 - val_loss: 0.6750 - val_categorical_accuracy: 0.7756 - lr: 3.2768e-04\n",
      "Epoch 61/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7340 - categorical_accuracy: 0.7221\n",
      "Epoch 00061: val_categorical_accuracy did not improve from 0.78108\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "37/37 [==============================] - 3s 75ms/step - loss: 0.7340 - categorical_accuracy: 0.7221 - val_loss: 0.6989 - val_categorical_accuracy: 0.7788 - lr: 3.2768e-04\n",
      "Epoch 62/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7232 - categorical_accuracy: 0.7265\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.78108\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.7232 - categorical_accuracy: 0.7265 - val_loss: 0.6702 - val_categorical_accuracy: 0.7797 - lr: 2.6214e-04\n",
      "Epoch 63/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7229 - categorical_accuracy: 0.7281\n",
      "Epoch 00063: val_categorical_accuracy did not improve from 0.78108\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7229 - categorical_accuracy: 0.7281 - val_loss: 0.6870 - val_categorical_accuracy: 0.7725 - lr: 2.6214e-04\n",
      "Epoch 64/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7201 - categorical_accuracy: 0.7279\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.78108\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7201 - categorical_accuracy: 0.7279 - val_loss: 0.6841 - val_categorical_accuracy: 0.7753 - lr: 2.6214e-04\n",
      "Epoch 65/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7174 - categorical_accuracy: 0.7291\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.78108\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7174 - categorical_accuracy: 0.7291 - val_loss: 0.6798 - val_categorical_accuracy: 0.7776 - lr: 2.6214e-04\n",
      "Epoch 66/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7169 - categorical_accuracy: 0.7285\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.78108\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.7174 - categorical_accuracy: 0.7285 - val_loss: 0.7042 - val_categorical_accuracy: 0.7680 - lr: 2.6214e-04\n",
      "Epoch 67/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7082 - categorical_accuracy: 0.7315\n",
      "Epoch 00067: val_categorical_accuracy improved from 0.78108 to 0.78251, saving model to tune/add_more_dense4_sigmoid.h5\n",
      "37/37 [==============================] - 3s 71ms/step - loss: 0.7082 - categorical_accuracy: 0.7315 - val_loss: 0.6629 - val_categorical_accuracy: 0.7825 - lr: 2.0972e-04\n",
      "Epoch 68/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7056 - categorical_accuracy: 0.7340\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.7056 - categorical_accuracy: 0.7340 - val_loss: 0.7022 - val_categorical_accuracy: 0.7679 - lr: 2.0972e-04\n",
      "Epoch 69/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7059 - categorical_accuracy: 0.7331\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7059 - categorical_accuracy: 0.7331 - val_loss: 0.6886 - val_categorical_accuracy: 0.7755 - lr: 2.0972e-04\n",
      "Epoch 70/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7041 - categorical_accuracy: 0.7349\n",
      "Epoch 00070: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.7041 - categorical_accuracy: 0.7349 - val_loss: 0.6706 - val_categorical_accuracy: 0.7818 - lr: 2.0972e-04\n",
      "Epoch 71/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7016 - categorical_accuracy: 0.7341\n",
      "Epoch 00071: val_categorical_accuracy did not improve from 0.78251\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "37/37 [==============================] - 2s 68ms/step - loss: 0.7016 - categorical_accuracy: 0.7341 - val_loss: 0.6863 - val_categorical_accuracy: 0.7768 - lr: 2.0972e-04\n",
      "Epoch 72/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6950 - categorical_accuracy: 0.7375\n",
      "Epoch 00072: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6950 - categorical_accuracy: 0.7375 - val_loss: 0.6795 - val_categorical_accuracy: 0.7795 - lr: 1.6777e-04\n",
      "Epoch 73/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6918 - categorical_accuracy: 0.7381\n",
      "Epoch 00073: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6914 - categorical_accuracy: 0.7381 - val_loss: 0.6818 - val_categorical_accuracy: 0.7769 - lr: 1.6777e-04\n",
      "Epoch 74/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6901 - categorical_accuracy: 0.7399\n",
      "Epoch 00074: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6901 - categorical_accuracy: 0.7399 - val_loss: 0.6757 - val_categorical_accuracy: 0.7784 - lr: 1.6777e-04\n",
      "Epoch 75/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6891 - categorical_accuracy: 0.7419\n",
      "Epoch 00075: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6885 - categorical_accuracy: 0.7421 - val_loss: 0.6708 - val_categorical_accuracy: 0.7810 - lr: 1.6777e-04\n",
      "Epoch 76/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6881 - categorical_accuracy: 0.7399\n",
      "Epoch 00076: val_categorical_accuracy did not improve from 0.78251\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.6881 - categorical_accuracy: 0.7399 - val_loss: 0.6739 - val_categorical_accuracy: 0.7773 - lr: 1.6777e-04\n",
      "Epoch 77/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6817 - categorical_accuracy: 0.7433\n",
      "Epoch 00077: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.6817 - categorical_accuracy: 0.7433 - val_loss: 0.7046 - val_categorical_accuracy: 0.7604 - lr: 1.3422e-04\n",
      "Epoch 78/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6811 - categorical_accuracy: 0.7422\n",
      "Epoch 00078: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6811 - categorical_accuracy: 0.7422 - val_loss: 0.6646 - val_categorical_accuracy: 0.7825 - lr: 1.3422e-04\n",
      "Epoch 79/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6784 - categorical_accuracy: 0.7426\n",
      "Epoch 00079: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6784 - categorical_accuracy: 0.7426 - val_loss: 0.7159 - val_categorical_accuracy: 0.7553 - lr: 1.3422e-04\n",
      "Epoch 80/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6775 - categorical_accuracy: 0.7440\n",
      "Epoch 00080: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6768 - categorical_accuracy: 0.7441 - val_loss: 0.6799 - val_categorical_accuracy: 0.7756 - lr: 1.3422e-04\n",
      "Epoch 81/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6744 - categorical_accuracy: 0.7452\n",
      "Epoch 00081: val_categorical_accuracy did not improve from 0.78251\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.6744 - categorical_accuracy: 0.7452 - val_loss: 0.6686 - val_categorical_accuracy: 0.7799 - lr: 1.3422e-04\n",
      "Epoch 82/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6693 - categorical_accuracy: 0.7485\n",
      "Epoch 00082: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6693 - categorical_accuracy: 0.7485 - val_loss: 0.6748 - val_categorical_accuracy: 0.7795 - lr: 1.0737e-04\n",
      "Epoch 83/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6689 - categorical_accuracy: 0.7469\n",
      "Epoch 00083: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6689 - categorical_accuracy: 0.7469 - val_loss: 0.6947 - val_categorical_accuracy: 0.7669 - lr: 1.0737e-04\n",
      "Epoch 84/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6674 - categorical_accuracy: 0.7484\n",
      "Epoch 00084: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6674 - categorical_accuracy: 0.7484 - val_loss: 0.6690 - val_categorical_accuracy: 0.7802 - lr: 1.0737e-04\n",
      "Epoch 85/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6676 - categorical_accuracy: 0.7471\n",
      "Epoch 00085: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.6668 - categorical_accuracy: 0.7474 - val_loss: 0.6761 - val_categorical_accuracy: 0.7742 - lr: 1.0737e-04\n",
      "Epoch 86/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6656 - categorical_accuracy: 0.7486\n",
      "Epoch 00086: val_categorical_accuracy did not improve from 0.78251\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6656 - categorical_accuracy: 0.7486 - val_loss: 0.6711 - val_categorical_accuracy: 0.7781 - lr: 1.0737e-04\n",
      "Epoch 87/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6625 - categorical_accuracy: 0.7499\n",
      "Epoch 00087: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.6618 - categorical_accuracy: 0.7502 - val_loss: 0.6825 - val_categorical_accuracy: 0.7729 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6610 - categorical_accuracy: 0.7502\n",
      "Epoch 00088: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6604 - categorical_accuracy: 0.7505 - val_loss: 0.6857 - val_categorical_accuracy: 0.7727 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6589 - categorical_accuracy: 0.7514\n",
      "Epoch 00089: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6589 - categorical_accuracy: 0.7514 - val_loss: 0.6786 - val_categorical_accuracy: 0.7730 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6560 - categorical_accuracy: 0.7521\n",
      "Epoch 00090: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6574 - categorical_accuracy: 0.7513 - val_loss: 0.6938 - val_categorical_accuracy: 0.7672 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6555 - categorical_accuracy: 0.7526\n",
      "Epoch 00091: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6555 - categorical_accuracy: 0.7526 - val_loss: 0.6866 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6535 - categorical_accuracy: 0.7532\n",
      "Epoch 00092: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6535 - categorical_accuracy: 0.7532 - val_loss: 0.7095 - val_categorical_accuracy: 0.7665 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6513 - categorical_accuracy: 0.7547\n",
      "Epoch 00093: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6513 - categorical_accuracy: 0.7547 - val_loss: 0.6852 - val_categorical_accuracy: 0.7742 - lr: 1.0000e-04\n",
      "Epoch 94/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6510 - categorical_accuracy: 0.7544\n",
      "Epoch 00094: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6510 - categorical_accuracy: 0.7544 - val_loss: 0.6871 - val_categorical_accuracy: 0.7733 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6507 - categorical_accuracy: 0.7537\n",
      "Epoch 00095: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6507 - categorical_accuracy: 0.7537 - val_loss: 0.6782 - val_categorical_accuracy: 0.7774 - lr: 1.0000e-04\n",
      "Epoch 96/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6481 - categorical_accuracy: 0.7552\n",
      "Epoch 00096: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6481 - categorical_accuracy: 0.7552 - val_loss: 0.6860 - val_categorical_accuracy: 0.7733 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6465 - categorical_accuracy: 0.7553\n",
      "Epoch 00097: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.6466 - categorical_accuracy: 0.7556 - val_loss: 0.6834 - val_categorical_accuracy: 0.7752 - lr: 1.0000e-04\n",
      "Epoch 98/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6453 - categorical_accuracy: 0.7556\n",
      "Epoch 00098: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6453 - categorical_accuracy: 0.7556 - val_loss: 0.6840 - val_categorical_accuracy: 0.7716 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6429 - categorical_accuracy: 0.7570\n",
      "Epoch 00099: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6429 - categorical_accuracy: 0.7570 - val_loss: 0.7526 - val_categorical_accuracy: 0.7398 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6426 - categorical_accuracy: 0.7576\n",
      "Epoch 00100: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.6426 - categorical_accuracy: 0.7576 - val_loss: 0.6754 - val_categorical_accuracy: 0.7810 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6416 - categorical_accuracy: 0.7576\n",
      "Epoch 00101: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6416 - categorical_accuracy: 0.7576 - val_loss: 0.6976 - val_categorical_accuracy: 0.7670 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6420 - categorical_accuracy: 0.7579\n",
      "Epoch 00102: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6415 - categorical_accuracy: 0.7580 - val_loss: 0.6732 - val_categorical_accuracy: 0.7817 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6378 - categorical_accuracy: 0.7592\n",
      "Epoch 00103: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.6378 - categorical_accuracy: 0.7592 - val_loss: 0.6942 - val_categorical_accuracy: 0.7689 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6373 - categorical_accuracy: 0.7597\n",
      "Epoch 00104: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 64ms/step - loss: 0.6373 - categorical_accuracy: 0.7597 - val_loss: 0.6849 - val_categorical_accuracy: 0.7789 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6365 - categorical_accuracy: 0.7599\n",
      "Epoch 00105: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6363 - categorical_accuracy: 0.7597 - val_loss: 0.6959 - val_categorical_accuracy: 0.7741 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6344 - categorical_accuracy: 0.7602\n",
      "Epoch 00106: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6344 - categorical_accuracy: 0.7602 - val_loss: 0.6813 - val_categorical_accuracy: 0.7806 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6318 - categorical_accuracy: 0.7605\n",
      "Epoch 00107: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6318 - categorical_accuracy: 0.7605 - val_loss: 0.7390 - val_categorical_accuracy: 0.7520 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6318 - categorical_accuracy: 0.7613\n",
      "Epoch 00108: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6318 - categorical_accuracy: 0.7613 - val_loss: 0.6798 - val_categorical_accuracy: 0.7825 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6293 - categorical_accuracy: 0.7621\n",
      "Epoch 00109: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6293 - categorical_accuracy: 0.7621 - val_loss: 0.6824 - val_categorical_accuracy: 0.7798 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6305 - categorical_accuracy: 0.7618\n",
      "Epoch 00110: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.6305 - categorical_accuracy: 0.7618 - val_loss: 0.6965 - val_categorical_accuracy: 0.7722 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6277 - categorical_accuracy: 0.7632\n",
      "Epoch 00111: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.6282 - categorical_accuracy: 0.7630 - val_loss: 0.7010 - val_categorical_accuracy: 0.7695 - lr: 1.0000e-04\n",
      "Epoch 112/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6256 - categorical_accuracy: 0.7639\n",
      "Epoch 00112: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6256 - categorical_accuracy: 0.7639 - val_loss: 0.6965 - val_categorical_accuracy: 0.7726 - lr: 1.0000e-04\n",
      "Epoch 113/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6249 - categorical_accuracy: 0.7636\n",
      "Epoch 00113: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6249 - categorical_accuracy: 0.7636 - val_loss: 0.6995 - val_categorical_accuracy: 0.7723 - lr: 1.0000e-04\n",
      "Epoch 114/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6238 - categorical_accuracy: 0.7633\n",
      "Epoch 00114: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6238 - categorical_accuracy: 0.7633 - val_loss: 0.6966 - val_categorical_accuracy: 0.7737 - lr: 1.0000e-04\n",
      "Epoch 115/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6224 - categorical_accuracy: 0.7654\n",
      "Epoch 00115: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6224 - categorical_accuracy: 0.7654 - val_loss: 0.6842 - val_categorical_accuracy: 0.7791 - lr: 1.0000e-04\n",
      "Epoch 116/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6181 - categorical_accuracy: 0.7666\n",
      "Epoch 00116: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6182 - categorical_accuracy: 0.7663 - val_loss: 0.6933 - val_categorical_accuracy: 0.7742 - lr: 1.0000e-04\n",
      "Epoch 117/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6202 - categorical_accuracy: 0.7665\n",
      "Epoch 00117: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6208 - categorical_accuracy: 0.7659 - val_loss: 0.6854 - val_categorical_accuracy: 0.7775 - lr: 1.0000e-04\n",
      "Epoch 118/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6188 - categorical_accuracy: 0.7665\n",
      "Epoch 00118: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.6179 - categorical_accuracy: 0.7666 - val_loss: 0.7054 - val_categorical_accuracy: 0.7686 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6179 - categorical_accuracy: 0.7674\n",
      "Epoch 00119: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 64ms/step - loss: 0.6180 - categorical_accuracy: 0.7675 - val_loss: 0.6884 - val_categorical_accuracy: 0.7754 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6149 - categorical_accuracy: 0.7672\n",
      "Epoch 00120: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6149 - categorical_accuracy: 0.7672 - val_loss: 0.7260 - val_categorical_accuracy: 0.7578 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6118 - categorical_accuracy: 0.7688\n",
      "Epoch 00121: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.6119 - categorical_accuracy: 0.7690 - val_loss: 0.6832 - val_categorical_accuracy: 0.7806 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6120 - categorical_accuracy: 0.7677\n",
      "Epoch 00122: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6120 - categorical_accuracy: 0.7677 - val_loss: 0.6838 - val_categorical_accuracy: 0.7776 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6124 - categorical_accuracy: 0.7697\n",
      "Epoch 00123: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.6124 - categorical_accuracy: 0.7697 - val_loss: 0.6902 - val_categorical_accuracy: 0.7758 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6095 - categorical_accuracy: 0.7696\n",
      "Epoch 00124: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6095 - categorical_accuracy: 0.7696 - val_loss: 0.6988 - val_categorical_accuracy: 0.7733 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6093 - categorical_accuracy: 0.7696\n",
      "Epoch 00125: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 68ms/step - loss: 0.6093 - categorical_accuracy: 0.7696 - val_loss: 0.6923 - val_categorical_accuracy: 0.7795 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6070 - categorical_accuracy: 0.7704\n",
      "Epoch 00126: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.6083 - categorical_accuracy: 0.7697 - val_loss: 0.7430 - val_categorical_accuracy: 0.7497 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6060 - categorical_accuracy: 0.7714\n",
      "Epoch 00127: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.6060 - categorical_accuracy: 0.7714 - val_loss: 0.7229 - val_categorical_accuracy: 0.7598 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6028 - categorical_accuracy: 0.7730\n",
      "Epoch 00128: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.6028 - categorical_accuracy: 0.7730 - val_loss: 0.6922 - val_categorical_accuracy: 0.7758 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6041 - categorical_accuracy: 0.7702\n",
      "Epoch 00129: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.6041 - categorical_accuracy: 0.7702 - val_loss: 0.7001 - val_categorical_accuracy: 0.7685 - lr: 1.0000e-04\n",
      "Epoch 130/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6017 - categorical_accuracy: 0.7721\n",
      "Epoch 00130: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.6017 - categorical_accuracy: 0.7721 - val_loss: 0.6842 - val_categorical_accuracy: 0.7796 - lr: 1.0000e-04\n",
      "Epoch 131/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6006 - categorical_accuracy: 0.7726\n",
      "Epoch 00131: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 77ms/step - loss: 0.6006 - categorical_accuracy: 0.7726 - val_loss: 0.6909 - val_categorical_accuracy: 0.7769 - lr: 1.0000e-04\n",
      "Epoch 132/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6014 - categorical_accuracy: 0.7729\n",
      "Epoch 00132: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 0.6014 - categorical_accuracy: 0.7729 - val_loss: 0.6960 - val_categorical_accuracy: 0.7706 - lr: 1.0000e-04\n",
      "Epoch 133/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5982 - categorical_accuracy: 0.7738\n",
      "Epoch 00133: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5982 - categorical_accuracy: 0.7738 - val_loss: 0.7252 - val_categorical_accuracy: 0.7563 - lr: 1.0000e-04\n",
      "Epoch 134/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5949 - categorical_accuracy: 0.7753\n",
      "Epoch 00134: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5961 - categorical_accuracy: 0.7750 - val_loss: 0.6922 - val_categorical_accuracy: 0.7759 - lr: 1.0000e-04\n",
      "Epoch 135/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5943 - categorical_accuracy: 0.7744\n",
      "Epoch 00135: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5943 - categorical_accuracy: 0.7744 - val_loss: 0.7148 - val_categorical_accuracy: 0.7656 - lr: 1.0000e-04\n",
      "Epoch 136/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5915 - categorical_accuracy: 0.7763\n",
      "Epoch 00136: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5931 - categorical_accuracy: 0.7756 - val_loss: 0.7238 - val_categorical_accuracy: 0.7609 - lr: 1.0000e-04\n",
      "Epoch 137/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5903 - categorical_accuracy: 0.7782\n",
      "Epoch 00137: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5906 - categorical_accuracy: 0.7782 - val_loss: 0.6968 - val_categorical_accuracy: 0.7760 - lr: 1.0000e-04\n",
      "Epoch 138/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5922 - categorical_accuracy: 0.7756\n",
      "Epoch 00138: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5917 - categorical_accuracy: 0.7758 - val_loss: 0.6917 - val_categorical_accuracy: 0.7784 - lr: 1.0000e-04\n",
      "Epoch 139/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5929 - categorical_accuracy: 0.7766\n",
      "Epoch 00139: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5925 - categorical_accuracy: 0.7768 - val_loss: 0.7084 - val_categorical_accuracy: 0.7684 - lr: 1.0000e-04\n",
      "Epoch 140/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5895 - categorical_accuracy: 0.7780\n",
      "Epoch 00140: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5895 - categorical_accuracy: 0.7780 - val_loss: 0.7221 - val_categorical_accuracy: 0.7597 - lr: 1.0000e-04\n",
      "Epoch 141/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5857 - categorical_accuracy: 0.7783\n",
      "Epoch 00141: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5857 - categorical_accuracy: 0.7783 - val_loss: 0.7024 - val_categorical_accuracy: 0.7781 - lr: 1.0000e-04\n",
      "Epoch 142/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5876 - categorical_accuracy: 0.7776\n",
      "Epoch 00142: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5884 - categorical_accuracy: 0.7770 - val_loss: 0.7287 - val_categorical_accuracy: 0.7565 - lr: 1.0000e-04\n",
      "Epoch 143/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5839 - categorical_accuracy: 0.7787\n",
      "Epoch 00143: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5839 - categorical_accuracy: 0.7786 - val_loss: 0.7035 - val_categorical_accuracy: 0.7711 - lr: 1.0000e-04\n",
      "Epoch 144/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5836 - categorical_accuracy: 0.7798\n",
      "Epoch 00144: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5831 - categorical_accuracy: 0.7801 - val_loss: 0.7220 - val_categorical_accuracy: 0.7644 - lr: 1.0000e-04\n",
      "Epoch 145/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5844 - categorical_accuracy: 0.7797\n",
      "Epoch 00145: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5841 - categorical_accuracy: 0.7800 - val_loss: 0.6935 - val_categorical_accuracy: 0.7819 - lr: 1.0000e-04\n",
      "Epoch 146/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5814 - categorical_accuracy: 0.7802\n",
      "Epoch 00146: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.5814 - categorical_accuracy: 0.7802 - val_loss: 0.7174 - val_categorical_accuracy: 0.7670 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5804 - categorical_accuracy: 0.7811\n",
      "Epoch 00147: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5804 - categorical_accuracy: 0.7811 - val_loss: 0.6989 - val_categorical_accuracy: 0.7802 - lr: 1.0000e-04\n",
      "Epoch 148/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5793 - categorical_accuracy: 0.7807\n",
      "Epoch 00148: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5793 - categorical_accuracy: 0.7807 - val_loss: 0.7163 - val_categorical_accuracy: 0.7668 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5749 - categorical_accuracy: 0.7833\n",
      "Epoch 00149: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5749 - categorical_accuracy: 0.7833 - val_loss: 0.7290 - val_categorical_accuracy: 0.7627 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5791 - categorical_accuracy: 0.7810\n",
      "Epoch 00150: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 64ms/step - loss: 0.5791 - categorical_accuracy: 0.7810 - val_loss: 0.7410 - val_categorical_accuracy: 0.7595 - lr: 1.0000e-04\n",
      "Epoch 151/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5746 - categorical_accuracy: 0.7836\n",
      "Epoch 00151: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5753 - categorical_accuracy: 0.7833 - val_loss: 0.7416 - val_categorical_accuracy: 0.7558 - lr: 1.0000e-04\n",
      "Epoch 152/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5746 - categorical_accuracy: 0.7835\n",
      "Epoch 00152: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5733 - categorical_accuracy: 0.7838 - val_loss: 0.7071 - val_categorical_accuracy: 0.7793 - lr: 1.0000e-04\n",
      "Epoch 153/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5744 - categorical_accuracy: 0.7825\n",
      "Epoch 00153: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5744 - categorical_accuracy: 0.7825 - val_loss: 0.7126 - val_categorical_accuracy: 0.7749 - lr: 1.0000e-04\n",
      "Epoch 154/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5705 - categorical_accuracy: 0.7844\n",
      "Epoch 00154: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5705 - categorical_accuracy: 0.7844 - val_loss: 0.7152 - val_categorical_accuracy: 0.7765 - lr: 1.0000e-04\n",
      "Epoch 155/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5707 - categorical_accuracy: 0.7846\n",
      "Epoch 00155: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5707 - categorical_accuracy: 0.7846 - val_loss: 0.7317 - val_categorical_accuracy: 0.7619 - lr: 1.0000e-04\n",
      "Epoch 156/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5692 - categorical_accuracy: 0.7855\n",
      "Epoch 00156: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5692 - categorical_accuracy: 0.7855 - val_loss: 0.7202 - val_categorical_accuracy: 0.7712 - lr: 1.0000e-04\n",
      "Epoch 157/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5669 - categorical_accuracy: 0.7862\n",
      "Epoch 00157: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5669 - categorical_accuracy: 0.7862 - val_loss: 0.7116 - val_categorical_accuracy: 0.7753 - lr: 1.0000e-04\n",
      "Epoch 158/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5686 - categorical_accuracy: 0.7844\n",
      "Epoch 00158: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5686 - categorical_accuracy: 0.7844 - val_loss: 0.7268 - val_categorical_accuracy: 0.7665 - lr: 1.0000e-04\n",
      "Epoch 159/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5663 - categorical_accuracy: 0.7859\n",
      "Epoch 00159: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5663 - categorical_accuracy: 0.7859 - val_loss: 0.7163 - val_categorical_accuracy: 0.7783 - lr: 1.0000e-04\n",
      "Epoch 160/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5631 - categorical_accuracy: 0.7884\n",
      "Epoch 00160: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.5627 - categorical_accuracy: 0.7883 - val_loss: 0.7280 - val_categorical_accuracy: 0.7683 - lr: 1.0000e-04\n",
      "Epoch 161/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5646 - categorical_accuracy: 0.7862\n",
      "Epoch 00161: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5646 - categorical_accuracy: 0.7862 - val_loss: 0.7534 - val_categorical_accuracy: 0.7550 - lr: 1.0000e-04\n",
      "Epoch 162/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5616 - categorical_accuracy: 0.7891\n",
      "Epoch 00162: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5616 - categorical_accuracy: 0.7891 - val_loss: 0.7118 - val_categorical_accuracy: 0.7744 - lr: 1.0000e-04\n",
      "Epoch 163/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5595 - categorical_accuracy: 0.7886\n",
      "Epoch 00163: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5598 - categorical_accuracy: 0.7882 - val_loss: 0.7787 - val_categorical_accuracy: 0.7378 - lr: 1.0000e-04\n",
      "Epoch 164/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5599 - categorical_accuracy: 0.7893\n",
      "Epoch 00164: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5604 - categorical_accuracy: 0.7890 - val_loss: 0.7717 - val_categorical_accuracy: 0.7444 - lr: 1.0000e-04\n",
      "Epoch 165/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5566 - categorical_accuracy: 0.7904\n",
      "Epoch 00165: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 68ms/step - loss: 0.5566 - categorical_accuracy: 0.7904 - val_loss: 0.7200 - val_categorical_accuracy: 0.7699 - lr: 1.0000e-04\n",
      "Epoch 166/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5585 - categorical_accuracy: 0.7876\n",
      "Epoch 00166: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5585 - categorical_accuracy: 0.7876 - val_loss: 0.7404 - val_categorical_accuracy: 0.7551 - lr: 1.0000e-04\n",
      "Epoch 167/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5568 - categorical_accuracy: 0.7915\n",
      "Epoch 00167: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5568 - categorical_accuracy: 0.7915 - val_loss: 0.7226 - val_categorical_accuracy: 0.7702 - lr: 1.0000e-04\n",
      "Epoch 168/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5552 - categorical_accuracy: 0.7916\n",
      "Epoch 00168: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5552 - categorical_accuracy: 0.7916 - val_loss: 0.7331 - val_categorical_accuracy: 0.7652 - lr: 1.0000e-04\n",
      "Epoch 169/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5520 - categorical_accuracy: 0.7913\n",
      "Epoch 00169: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5531 - categorical_accuracy: 0.7908 - val_loss: 0.8046 - val_categorical_accuracy: 0.7269 - lr: 1.0000e-04\n",
      "Epoch 170/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5525 - categorical_accuracy: 0.7919\n",
      "Epoch 00170: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5536 - categorical_accuracy: 0.7916 - val_loss: 0.7638 - val_categorical_accuracy: 0.7485 - lr: 1.0000e-04\n",
      "Epoch 171/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5476 - categorical_accuracy: 0.7931\n",
      "Epoch 00171: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5476 - categorical_accuracy: 0.7931 - val_loss: 0.7199 - val_categorical_accuracy: 0.7809 - lr: 1.0000e-04\n",
      "Epoch 172/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5526 - categorical_accuracy: 0.7905\n",
      "Epoch 00172: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5526 - categorical_accuracy: 0.7905 - val_loss: 0.7186 - val_categorical_accuracy: 0.7801 - lr: 1.0000e-04\n",
      "Epoch 173/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5491 - categorical_accuracy: 0.7931\n",
      "Epoch 00173: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.5488 - categorical_accuracy: 0.7935 - val_loss: 0.7168 - val_categorical_accuracy: 0.7722 - lr: 1.0000e-04\n",
      "Epoch 174/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5486 - categorical_accuracy: 0.7932\n",
      "Epoch 00174: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5486 - categorical_accuracy: 0.7932 - val_loss: 0.7838 - val_categorical_accuracy: 0.7372 - lr: 1.0000e-04\n",
      "Epoch 175/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5477 - categorical_accuracy: 0.7921\n",
      "Epoch 00175: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5477 - categorical_accuracy: 0.7921 - val_loss: 0.7713 - val_categorical_accuracy: 0.7421 - lr: 1.0000e-04\n",
      "Epoch 176/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5436 - categorical_accuracy: 0.7963\n",
      "Epoch 00176: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5436 - categorical_accuracy: 0.7963 - val_loss: 0.7328 - val_categorical_accuracy: 0.7628 - lr: 1.0000e-04\n",
      "Epoch 177/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5452 - categorical_accuracy: 0.7948\n",
      "Epoch 00177: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.5452 - categorical_accuracy: 0.7948 - val_loss: 0.7319 - val_categorical_accuracy: 0.7682 - lr: 1.0000e-04\n",
      "Epoch 178/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5407 - categorical_accuracy: 0.7968\n",
      "Epoch 00178: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5416 - categorical_accuracy: 0.7961 - val_loss: 0.7137 - val_categorical_accuracy: 0.7794 - lr: 1.0000e-04\n",
      "Epoch 179/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5432 - categorical_accuracy: 0.7961\n",
      "Epoch 00179: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5447 - categorical_accuracy: 0.7952 - val_loss: 0.7518 - val_categorical_accuracy: 0.7583 - lr: 1.0000e-04\n",
      "Epoch 180/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5381 - categorical_accuracy: 0.7988\n",
      "Epoch 00180: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5381 - categorical_accuracy: 0.7988 - val_loss: 0.7353 - val_categorical_accuracy: 0.7630 - lr: 1.0000e-04\n",
      "Epoch 181/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5414 - categorical_accuracy: 0.7965\n",
      "Epoch 00181: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5414 - categorical_accuracy: 0.7965 - val_loss: 0.7406 - val_categorical_accuracy: 0.7610 - lr: 1.0000e-04\n",
      "Epoch 182/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5360 - categorical_accuracy: 0.7989\n",
      "Epoch 00182: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 69ms/step - loss: 0.5360 - categorical_accuracy: 0.7989 - val_loss: 0.7177 - val_categorical_accuracy: 0.7780 - lr: 1.0000e-04\n",
      "Epoch 183/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5369 - categorical_accuracy: 0.7961\n",
      "Epoch 00183: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 64ms/step - loss: 0.5365 - categorical_accuracy: 0.7964 - val_loss: 0.7463 - val_categorical_accuracy: 0.7558 - lr: 1.0000e-04\n",
      "Epoch 184/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5358 - categorical_accuracy: 0.7998\n",
      "Epoch 00184: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 68ms/step - loss: 0.5363 - categorical_accuracy: 0.7995 - val_loss: 0.7316 - val_categorical_accuracy: 0.7732 - lr: 1.0000e-04\n",
      "Epoch 185/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5339 - categorical_accuracy: 0.7989\n",
      "Epoch 00185: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5346 - categorical_accuracy: 0.7986 - val_loss: 0.7494 - val_categorical_accuracy: 0.7619 - lr: 1.0000e-04\n",
      "Epoch 186/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5327 - categorical_accuracy: 0.8016\n",
      "Epoch 00186: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5327 - categorical_accuracy: 0.8016 - val_loss: 0.7534 - val_categorical_accuracy: 0.7558 - lr: 1.0000e-04\n",
      "Epoch 187/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5320 - categorical_accuracy: 0.8008\n",
      "Epoch 00187: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5310 - categorical_accuracy: 0.8017 - val_loss: 0.7383 - val_categorical_accuracy: 0.7674 - lr: 1.0000e-04\n",
      "Epoch 188/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5323 - categorical_accuracy: 0.7995\n",
      "Epoch 00188: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5308 - categorical_accuracy: 0.8005 - val_loss: 0.7398 - val_categorical_accuracy: 0.7659 - lr: 1.0000e-04\n",
      "Epoch 189/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5345 - categorical_accuracy: 0.8010\n",
      "Epoch 00189: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 71ms/step - loss: 0.5345 - categorical_accuracy: 0.8010 - val_loss: 0.7398 - val_categorical_accuracy: 0.7663 - lr: 1.0000e-04\n",
      "Epoch 190/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5275 - categorical_accuracy: 0.8000\n",
      "Epoch 00190: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5278 - categorical_accuracy: 0.8003 - val_loss: 0.7449 - val_categorical_accuracy: 0.7760 - lr: 1.0000e-04\n",
      "Epoch 191/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5283 - categorical_accuracy: 0.8023\n",
      "Epoch 00191: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5283 - categorical_accuracy: 0.8023 - val_loss: 0.7478 - val_categorical_accuracy: 0.7771 - lr: 1.0000e-04\n",
      "Epoch 192/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5289 - categorical_accuracy: 0.8003\n",
      "Epoch 00192: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5287 - categorical_accuracy: 0.8003 - val_loss: 0.7359 - val_categorical_accuracy: 0.7759 - lr: 1.0000e-04\n",
      "Epoch 193/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5229 - categorical_accuracy: 0.8051\n",
      "Epoch 00193: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 66ms/step - loss: 0.5229 - categorical_accuracy: 0.8051 - val_loss: 0.7479 - val_categorical_accuracy: 0.7638 - lr: 1.0000e-04\n",
      "Epoch 194/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5279 - categorical_accuracy: 0.8018\n",
      "Epoch 00194: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 3s 70ms/step - loss: 0.5279 - categorical_accuracy: 0.8018 - val_loss: 0.7880 - val_categorical_accuracy: 0.7411 - lr: 1.0000e-04\n",
      "Epoch 195/200\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5210 - categorical_accuracy: 0.8037\n",
      "Epoch 00195: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 68ms/step - loss: 0.5200 - categorical_accuracy: 0.8044 - val_loss: 0.7678 - val_categorical_accuracy: 0.7651 - lr: 1.0000e-04\n",
      "Epoch 196/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5244 - categorical_accuracy: 0.8026\n",
      "Epoch 00196: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5244 - categorical_accuracy: 0.8026 - val_loss: 0.7991 - val_categorical_accuracy: 0.7321 - lr: 1.0000e-04\n",
      "Epoch 197/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5203 - categorical_accuracy: 0.8052\n",
      "Epoch 00197: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5203 - categorical_accuracy: 0.8052 - val_loss: 0.7899 - val_categorical_accuracy: 0.7408 - lr: 1.0000e-04\n",
      "Epoch 198/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5215 - categorical_accuracy: 0.8039\n",
      "Epoch 00198: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5215 - categorical_accuracy: 0.8039 - val_loss: 0.7629 - val_categorical_accuracy: 0.7583 - lr: 1.0000e-04\n",
      "Epoch 199/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5198 - categorical_accuracy: 0.8066\n",
      "Epoch 00199: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 67ms/step - loss: 0.5198 - categorical_accuracy: 0.8066 - val_loss: 0.7808 - val_categorical_accuracy: 0.7506 - lr: 1.0000e-04\n",
      "Epoch 200/200\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5188 - categorical_accuracy: 0.8038\n",
      "Epoch 00200: val_categorical_accuracy did not improve from 0.78251\n",
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5188 - categorical_accuracy: 0.8038 - val_loss: 0.7566 - val_categorical_accuracy: 0.7636 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc9a817a10>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_toplayers()\n",
    "model.fit(train_gen,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_data = val_gen,\n",
    "          validation_steps = val_steps,\n",
    "          epochs=200,\n",
    "          callbacks=[checkpoint, reduceLROnPlat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:22:37.717525Z",
     "start_time": "2021-01-10T19:22:36.248238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 30ms/step - loss: 0.7150 - categorical_accuracy: 0.7284\n",
      "train loss 0.7149940133094788 accuracy 0.7284364700317383\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_fp)\n",
    "print('train loss {} accuracy {}'.format(*model.evaluate(train_gen, steps=train_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:22:38.520133Z",
     "start_time": "2021-01-10T19:22:37.719042Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 28ms/step - loss: 0.6643 - categorical_accuracy: 0.7821\n",
      "train loss 0.6642634272575378 accuracy 0.7820542454719543\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_fp)\n",
    "print('train loss {} accuracy {}'.format(*model.evaluate(val_gen, steps=val_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:22:38.524377Z",
     "start_time": "2021-01-10T19:22:38.521584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Tweaking for Recall using top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:22:38.528268Z",
     "start_time": "2021-01-10T19:22:38.525619Z"
    }
   },
   "outputs": [],
   "source": [
    "# val_data = (data.filter(lambda x: x['name'] in x_val.values)\n",
    "#             .map(lambda x: {x['name']: np.asarray(x['value'])})\n",
    "#             .fold(binop=lambda x, y: join_dict(x, y),\n",
    "#                   combine=lambda x, y: join_dict(x, y))\n",
    "#             .compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:22:38.532599Z",
     "start_time": "2021-01-10T19:22:38.529940Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Make train labels to dictionary for easier implementation\n",
    "# train_labels = pd.Series(y_train.values,index=x_train.values).to_dict()\n",
    "# val_labels = pd.Series(y_val.values, index=x_val.values).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:00.573321Z",
     "start_time": "2021-01-11T07:04:00.565214Z"
    }
   },
   "outputs": [],
   "source": [
    "y_val_labels = [val_labels[i] for i in val_data.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:01.569419Z",
     "start_time": "2021-01-11T07:04:01.494662Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_toplayers():\n",
    "    \"\"\"Create top layer using inceptionb resnet v2 base\"\"\"\n",
    "    # Create input layer based on the output of the convolutional base\n",
    "    lyr_input = Input(shape=(base.layers[-1].output.shape.as_list()[1], ))\n",
    "\n",
    "    # Add Dense\n",
    "    lyr_dense1 = Dense(1024, activation='relu')(lyr_input)\n",
    "    lyr_dense2 = Dense(512, activation='relu')(lyr_dense1)\n",
    "    lyr_dense3 = Dense(256, activation='relu')(lyr_dense2)\n",
    "\n",
    "\n",
    "    # Create output layer\n",
    "    output = Dense(5, activation='sigmoid')(lyr_dense3)\n",
    "\n",
    "    model = Model(inputs=[lyr_input], outputs=[output])\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model_fp = 'tune/add_more_dense4_sigmoid.h5'\n",
    "model = create_toplayers()\n",
    "model.load_weights(model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using val data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:05.487097Z",
     "start_time": "2021-01-11T07:04:03.254574Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - 2s 3ms/step - loss: 0.6639 - categorical_accuracy: 0.7822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6639451384544373, 0.7822067141532898]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(list(val_data.values())), to_categorical(y_val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:09.782563Z",
     "start_time": "2021-01-11T07:04:08.569774Z"
    }
   },
   "outputs": [],
   "source": [
    "#Filter out those 1s counted as 0s to be 1s\n",
    "##check 1s softmax score distribution of 1s that were tagged as 0s to define threshold.\n",
    "y_pred_softmax = model.predict(np.array(list(val_data.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:09.791293Z",
     "start_time": "2021-01-11T07:04:09.787649Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true_softmax = to_categorical(y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:09.799421Z",
     "start_time": "2021-01-11T07:04:09.792733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 13063, 2: 2677, 1: 1252, 4: 370, 3: 375})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(np.argmax(y_true_softmax, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:11.037351Z",
     "start_time": "2021-01-11T07:04:10.998538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7822066865873598"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_pred_softmax, axis=1)\n",
    "y_true = np.argmax(y_true_softmax, axis=1)\n",
    "sum(y_pred == y_true)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:04:12.338087Z",
     "start_time": "2021-01-11T07:04:12.334466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17737"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:15:28.226053Z",
     "start_time": "2021-01-11T07:15:28.222250Z"
    }
   },
   "outputs": [],
   "source": [
    "def y_pred_threshold(y_pred_softmax):\n",
    "    y_pred = []\n",
    "    for i in y_pred_softmax:\n",
    "        if np.argmax(i) in [2,3,4]:\n",
    "             y_pred.append(np.argmax(i))\n",
    "        elif i[0] > 0.96:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweaked -  Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:15:28.857994Z",
     "start_time": "2021-01-11T07:15:28.636706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7138 5583  308   14   20]\n",
      " [ 532  624   94    1    1]\n",
      " [ 392 1303  850   97   35]\n",
      " [   5   83  123  146   18]\n",
      " [   6   76   80   52  156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.884     0.546     0.675     13063\n",
      "           1      0.081     0.498     0.140      1252\n",
      "           2      0.584     0.318     0.411      2677\n",
      "           3      0.471     0.389     0.426       375\n",
      "           4      0.678     0.422     0.520       370\n",
      "\n",
      "    accuracy                          0.503     17737\n",
      "   macro avg      0.540     0.435     0.435     17737\n",
      "weighted avg      0.769     0.503     0.589     17737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, \n",
    "                              y_pred_threshold(y_pred_softmax)\n",
    "                               #np.argmax(y_pred_softmax, axis=1)\n",
    "                              ))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, \n",
    "                                    y_pred_threshold(y_pred_softmax),\n",
    "                               #np.argmax(y_pred_softmax, axis=1),\n",
    "                                    digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNTOUCHED SIGMOID - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T07:05:37.610046Z",
     "start_time": "2021-01-11T07:05:37.572240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12721     0   308    14    20]\n",
      " [ 1155     1    94     1     1]\n",
      " [ 1695     0   850    97    35]\n",
      " [   88     0   123   146    18]\n",
      " [   82     0    80    52   156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.808     0.974     0.883     13063\n",
      "           1      1.000     0.001     0.002      1252\n",
      "           2      0.584     0.318     0.411      2677\n",
      "           3      0.471     0.389     0.426       375\n",
      "           4      0.678     0.422     0.520       370\n",
      "\n",
      "    accuracy                          0.782     17737\n",
      "   macro avg      0.708     0.421     0.449     17737\n",
      "weighted avg      0.778     0.782     0.733     17737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, \n",
    "                              # y_pred_threshold(y_pred_softmax)\n",
    "                               np.argmax(y_pred_softmax, axis=1)\n",
    "                              ))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, \n",
    "                                    # y_pred_threshold(y_pred_softmax)\n",
    "                               np.argmax(y_pred_softmax, axis=1),\n",
    "                                    digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:22:42.635849Z",
     "start_time": "2021-01-10T19:22:42.599972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(0, 4)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "columns=[0,1,2,3,4]\n",
    "df_features = pd.DataFrame(y_pred_softmax, columns=columns)\n",
    "\n",
    "for i in combinations(df_features.columns, 2):\n",
    "    print(i)\n",
    "    df_features['add_'+str(i)] = df_features.loc[:,i[0]] + df_features.loc[:,i[1]]\n",
    "    df_features['diff_'+str(i)] = df_features.loc[:,i[0]] - df_features.loc[:,i[1]]\n",
    "    df_features['div_'+str(i)] = df_features.loc[:,i[0]] / df_features.loc[:,i[1]]\n",
    "    df_features['mul_'+str(i)] = df_features.loc[:,i[0]] * df_features.loc[:,i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:40:01.575352Z",
     "start_time": "2021-01-10T19:22:42.637292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549303d0096a4127b569bf0cb6233ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "XX = df_features\n",
    "y = y_val_labels\n",
    "\n",
    "lahat_training = pd.DataFrame()\n",
    "lahat_test = pd.DataFrame()\n",
    "for seedN in tqdm(range(1,20,1)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, \n",
    "                                                        test_size=0.25, \n",
    "                                                        random_state=seedN,\n",
    " #                                                      stratify=True\n",
    "                                                       )\n",
    "    training_accuracy = []  \n",
    "    test_accuracy = []\n",
    "    maxdepth_settings = range(1, 15) # try n_neighbors from 1 to 10\n",
    "\n",
    "    for depth in maxdepth_settings:   \n",
    "        reg = RandomForestClassifier(random_state=0, max_depth=depth) # build the model \n",
    "        reg.fit(X_train, y_train) #clf = KNeighborsClassifier(n_neighbors=n_neighbors    \n",
    "        training_accuracy.append(reg.score(X_train, y_train)) # record training set accuracy  \n",
    "        test_accuracy.append(reg.score(X_test, y_test)) # record generalization accuracy    \n",
    "    lahat_training[seedN]=training_accuracy\n",
    "    lahat_test[seedN] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:40:01.784107Z",
     "start_time": "2021-01-10T19:40:01.576546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Average Test Set Achieved = 0.780146\n",
      "Max_Depth = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.errorbar(maxdepth_settings, lahat_training.mean(axis=1),\n",
    "             yerr=lahat_training.std(axis=1), label=\"training accuracy\")\n",
    "plt.errorbar(maxdepth_settings, lahat_test.mean(axis=1),\n",
    "             yerr=lahat_test.std(axis=1), label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Max_Depth\")\n",
    "plt.legend()\n",
    "bestdepth=np.argmax(lahat_test.mean(axis=1))+1\n",
    "print(\"Highest Average Test Set Achieved = %f\" % np.amax(lahat_test.mean(axis=1)))\n",
    "print(\"Max_Depth = %d\" %bestdepth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:42:05.457503Z",
     "start_time": "2021-01-10T19:40:01.785399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1836eaa083ae42308dcffa48ee6db1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "XX = df_features\n",
    "y = y_val_labels\n",
    "\n",
    "lahat_training = pd.DataFrame()\n",
    "lahat_test = pd.DataFrame()\n",
    "for seedN in tqdm(range(1,20,1)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, \n",
    "                                                        test_size=0.25, \n",
    "                                                        random_state=seedN,\n",
    " #                                                      stratify=True\n",
    "                                                       )\n",
    "    training_accuracy = []  \n",
    "    test_accuracy = []\n",
    "    maxdepth_settings = range(1, 5) # try n_neighbors from 1 to 10\n",
    "\n",
    "    for depth in maxdepth_settings:   \n",
    "        reg = RandomForestClassifier(random_state=0, max_depth=depth) # build the model \n",
    "        reg.fit(X_train, y_train) #clf = KNeighborsClassifier(n_neighbors=n_neighbors    \n",
    "        training_accuracy.append(reg.score(X_train, y_train)) # record training set accuracy  \n",
    "        test_accuracy.append(reg.score(X_test, y_test)) # record generalization accuracy    \n",
    "    lahat_training[seedN]=training_accuracy\n",
    "    lahat_test[seedN] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:42:05.645782Z",
     "start_time": "2021-01-10T19:42:05.459251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Average Test Set Achieved = 0.780146\n",
      "Max_Depth = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4UAAAF0CAYAAACUgZw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXzc1X3v/9fRvsxoJM1IsizZkq2Rd8m2LBtjOWE3Zg1Lwxq4kMuSpHBz8/vBL6S3hTT9tTePhF+SJm3SkPwgKU0bUrK2wQmhQFNjB2wDscE2SPIqedFm7evMnPvHdzSjzVg2lkbL+/l4+CHNfM93dMar3j6fcz7GWouIiIiIiIjMTnGxnoCIiIiIiIjEjkKhiIiIiIjILKZQKCIiIiIiMospFIqIiIiIiMxiCoUiIiIiIiKzmEKhiIiIiIjILJYQ6wlMBp/PZ4uLi2M9DRERERERkZjYtWtXk7U2Z6xrsyIUFhcXs3PnzlhPQ0REREREJCaMMYdPd03loyIiIiIiIrOYQqGIiIiIiMgsplAoIiIiIiIyi82KPYUiIiIiIrPdwMAAdXV19Pb2xnoqMoFSUlIoLCwkMTFx3PcoFIqIiIiIzAJ1dXW43W6Ki4sxxsR6OjIBrLU0NzdTV1fHggULxn2fykdFRERERGaB3t5evF6vAuEMZozB6/We9WqwQqGIiIiIyCxxtoHw1u9u59bvbp+g2chEOJfQr1AoIiIiIiITrrW1lW9/+9vndO/VV19Na2vrB455/PHHeemll87p9Wc7hUIREREREZlwHxQKg8HgB977wgsvkJmZ+YFjvvSlL3H55Zef8/xiIRAIxHoKgEKhiIiIiIhMgscee4za2lpWrVrFo48+yquvvsoll1zCHXfcQVlZGQA33HADa9asYfny5Tz11FORe4uLi2lqauLQoUMsXbqU+++/n+XLl7Np0yZ6enoAuOeee3j++ecj45944gkqKiooKytj//79ADQ2NnLFFVdQUVHBgw8+SFFREU1NTaPm+ulPf5rKykqWL1/OE088EXl+x44dbNiwgZUrV7Ju3To6OjoIBoM88sgjlJWVUV5ezre+9a1hcwbYuXMnF198MQBf/OIXeeCBB9i0aRN33303hw4d4iMf+QgVFRVUVFSwbdu2yNf7yle+QllZGStXroz8/FVUVESuV1dXs2bNmg/9a6PTR0VEREREZpm//Ld32Xus/Yzj9h53xoxnX+GyuRk8cd3y017/8pe/zDvvvMPbb78NwKuvvsobb7zBO++8Ezkp8+mnnyY7O5uenh7Wrl3LzTffjNfrHfY61dXV/Mu//Avf+973uOWWW/jpT3/KJz7xiVFfz+fz8eabb/Ltb3+bJ598ku9///v85V/+JZdeeilf+MIX+M1vfjMseA7113/912RnZxMMBrnsssvYvXs3S5Ys4dZbb+W5555j7dq1tLe3k5qaylNPPcXBgwd56623SEhIoKWl5Yw/V7t27WLr1q2kpqbS3d3N7373O1JSUqiurub2229n586dbNmyhV/84he8/vrrpKWl0dLSQnZ2Nh6Ph7fffptVq1bxzDPPcM8995zx652JQqGIiIiIiMTEunXrhrVO+OY3v8nPf/5zAI4ePUp1dfWoULhgwQJWrVoFwJo1azh06NCYr33TTTdFxvzsZz8DYOvWrZHX37x5M1lZWWPe+5Of/ISnnnqKQCDA8ePH2bt3L8YY8vPzWbt2LQAZGRkAvPTSS3zqU58iIcGJVtnZ2Wd839dffz2pqamA0z/yoYce4u233yY+Pp73338/8rr33nsvaWlpw173vvvu45lnnuFrX/sazz33HG+88cYZv96ZKBSKiIiIiMwyH7SiN9TgCuFzD144IfNIT0+PfP7qq6/y0ksvsX37dtLS0rj44ovHbK2QnJwc+Tw+Pj5SPnq6cfHx8ZG9e9baM87p4MGDPPnkk+zYsYOsrCzuueceent7sdaOebLn6Z5PSEggFAoBjHofQ9/317/+dfLy8vjjH/9IKBQiJSXlA1/35ptvjqx4rlmzZlRoPhfaUygiIiIiIhPO7XbT0dFx2uttbW1kZWWRlpbG/v37+cMf/nDe57Bx40Z+8pOfAPDiiy9y6tSpUWPa29tJT0/H4/Fw8uRJtmzZAsCSJUs4duwYO3bsAKCjo4NAIMCmTZv4h3/4BwKBALWNnex6/wjg7CnctWsXAD/96U9PO6e2tjby8/OJi4vj2WefjRy6s2nTJp5++mm6u7sBImWpKSkpXHnllXz605/m3nvvPR8/LQqFIiIiIiIy8bxeL1VVVaxYsYJHH3101PXNmzcTCAQoLy/nL/7iL1i/fv15n8MTTzzBiy++SEVFBVu2bCE/Px+32z1szMqVK1m9ejXLly/nk5/8JFVVVQAkJSXx3HPP8fDDD7Ny5UquuOIKent7ue+++5g/fz7l5eVce/GF/Opn/xr5Wp/97Gf5yEc+Qnx8/Gnn9JnPfIYf/vCHrF+/nvfffz+yirh582auv/56KisrWbVqFU8++WTknjvvvBNjDJs2bTovPy9mPEuo011lZaXduXNnrKchIiIiIhIz+/btY+nSpWd1z0SXj062vr4+4uPjSUhIYPv27Xz605+OHHxzPtQ2dgJQkuM6b685lieffJK2tjb+6q/+aszrY/1aG2N2WWsrxxqvPYUiIiIiIjKmmRIGBx05coRbbrmFUChEUlIS3/ve92I9pbN24403Ultby8svv3zeXlOhUEREREREZoXS0lLeeuutWE/jQxk8PfV80p5CERERERGRWUyhUEREREREZBZTKBQREREREZnFFApFRERERGRsz1zj/JAZTaFQREREREQmXGtrK9/+9rfP+f5vfOMbkUbucn4pFIqIiIiIyISbCaEwEAjE9OtPFIVCERERERGZcI899hi1tbWsWrWKRx99FICvfvWrrF27lvLycp544gkAurq6uOaaa1i5ciUrVqzgueee45vf/CbHjh3jkksu4ZJLLhn12l/60pdYu3YtK1as4IEHHsBaC0BNTQ2XX345K1eupKKigtraWgC+8pWvUFZWxsqVK3nssccAuPjii9m5cycATU1NFBcXA/CDH/yAj3/841x33XVs2rSJzs5OLrvsMioqKigrK+OXv/xlZB4/f+6fueai9axcuZK77rqLjo4OFixYwMDAAADt7e0UFxdHHk8V6lMoIiIiIjLbbHkMTuw587gTu52P49lXOKcMrvryaS9/+ctf5p133uHtt98G4MUXX6S6upo33ngDay3XX389v//972lsbGTu3Ln8+te/BqCtrQ2Px8PXvvY1XnnlFXw+36jXfuihh3j88ccBuOuuu/j3f/93rrvuOu68804ee+wxbrzxRnp7ewmFQmzZsoVf/OIXvP7666SlpdHS0nLGt7Z9+3Z2795NdnY2gUCAn//852RkZNDU1MT69eu5/vrr2bt3L9/+xld57t9/x9olxbS0tOB2u7n44ov59a9/zQ033MCPf/xjbr75ZhITE8/88zmJtFIoIiIiIiKT7sUXX+TFF19k9erVVFRUsH//fqqrqykrK+Oll17i85//PP/1X/+Fx+M542u98sorXHDBBZSVlfHyyy/z7rvv0tHRQX19PTfeeCMAKSkppKWl8dJLL3HvvfeSlpYGQHZ29hlf/4orroiMs9byZ3/2Z5SXl3P55ZdTX1/PyZMnefnll9l83Q1ke33DXve+++7jmWeeAeCZZ57h3nvvPfufrAmmlUIRERERkdnmA1b0hhlcIbz31+d9CtZavvCFL/Dggw+OurZr1y5eeOEFvvCFL7Bp06bIKuBYent7+cxnPsPOnTuZN28eX/ziF+nt7Y2UkI71dY0xo55PSEggFApFXnOo9PT0yOc/+tGPaGxsZNeuXSQmJlJcXBz5emO9blVVFYcOHeI///M/CQaDrFix4rTvJVa0UigiIiIiIhPO7XbT0dEReXzllVfy9NNP09nZCUB9fT0NDQ0cO3aMtLQ0PvGJT/DII4/w5ptvjnn/oMEA5/P56Ozs5PnnnwcgIyODwsJCfvGLXwDQ19dHd3c3mzZt4umnn44cWjNYPlpcXMyuXbsAIq8xlra2NnJzc0lMTOSVV17h8OHDAFx22WW88MufcaqledjrAtx9993cfvvtU3KVELRSKCIiIiIik8Dr9VJVVcWKFSu46qqr+OpXv8q+ffu48MILAXC5XPzTP/0TNTU1PProo8TFxZGYmMh3vvMdAB544AGuuuoq8vPzeeWVVyKvm5mZyf33309ZWRnFxcWsXbs2cu3ZZ5/lwQcf5PHHHycxMZF//dd/ZfPmzbz99ttUVlaSlJTE1Vdfzd/8zd/wyCOPcMstt/Dss89y6aWXnvZ93HnnnVx33XVUVlayatUqlixZAsDy5cv5zP98lDtuuIrUpERWr17ND37wg8g9f/7nf87tt99+vn9azwtzumXVmaSystIOniQkIiIiIjIb7du3j6VLl57dTRNYPjoT1TY6q54lOa5hzz///PP88pe/5Nlnn52UeYz1a22M2WWtrRxrvFYKRURERERkbAqDH9rDDz/Mli1beOGFF2I9ldNSKBQREREREZkg3/rWt2I9hTPSQTMiIiIiIiKzmEKhiIiIiMgsMRvOE5ntzuXXWKFQRERERGQWSElJobm5WcFwBrPW0tzcTEpKylndpz2FIiIiIiKzQGFhIXV1dTQ2NsZ6KjNKMGQJBEMMhCwdvQHiDPQ3nV0oO59SUlIoLCw8q3sUCkVEREREZoHExEQWLFgQ62lMS9ZajrX1Un2yg5qGTmoaOqkOf2zrGYiMi48zzPWk8F+fP32fw6lIoVBERERERARn1e9oS3ck9FU3dFAbDn9d/cHIOG96EiW5Lq4tz6c014U/101pnouH//lNjDExfAfnRqFQRERERERmlYFgiMPNXVSfjK74VTd0UtvYSX8gFBmXl5FMaa6bj1fOw5/rCgdAF15X8pivOx0DISgUioiIiIjIDNU7EORAYxc1jZ3UnOwIr/51cqipi0AoeuBOYVYqpbkuNvq9lOa68ec54S8jJTGGs588CoUiIiIiIjKtdfUFqG3sHLLy5+z9O9LSzWD2izNQ7E2nJNfFpmV5lOa5KM11szAnnbSk2R2LZve7FxERERGRaaOte4Caxo5hZZ81DZ3Ut/ZExiTGGxb40lk+18PHVhU4ZZ95Lhb40klOiI/h7KcuhUIREREREZkyrLU0d/VTfTK64jdY9tnY0RcZl5wQhz/XxdriLG7PnRc57GV+dhqJ8WrHfjYUCkVEREREZNJZaznR3hsOf8PLPk91R9s8uJIT8Oe6uGhRDqXhVT9/jpuCrFTi46bnwS5TjUKhiIiIiIhMmFDIUneqZ8yyz86+QGRcZloipbkuNq/Ij5zyWZrnYk5GyrQ51fPx5kfDn22N6TzO1oSGQmPMZuBvgXjg+9baL4+4/nXgkvDDNCDXWpsZvvYV4BogDvgd8FlrrTXGvArkA4OFw5ustQ0T+T5EREREROSDBYIhDrd0jyr7rG3spHcg2uYhx51Maa6Lmyuc/X6DZZ/e9KRpE/5mmgkLhcaYeODvgSuAOmCHMeZX1tq9g2OstZ8bMv5hYHX48w1AFVAevrwVuAh4Nfz4Tmvtzomau4iIiIiIjK0vEORgU1ek7LMm3OT9YFMXA8Fom4eCzFT8uS7WL/QOK/v0pM2ONg/TyUSuFK4Daqy1BwCMMT8GPgbsPc3424Enwp9bIAVIAgyQCJycwLmKiIiIiMgQ3f0Bahu6hpV91jZ0cqi5K9LmwRgoyk7Dn+vi0iV5kfBXkuMiPVk71aaLifyVKgCODnlcB1ww1kBjTBGwAHgZwFq73RjzCnAcJxT+nbV235BbnjHGBIGfAv+vtdaOfE0RERERETmz9t4BZ8XvpLPiN1j2WXcq2uYhIc5Q7Etn8Rw315bn489z489xsTAnnZREtXmY7iYyFI5VEHy68HYb8Ly1NghgjPEDS4HC8PXfGWM+aq39PU7paL0xxo0TCu8C/nHUFzfmAeABgPnz53+oNyIiIiIiMt21dPVTfbJj2EEv1Q0dnGyPtnlISoijJMdFxfwsbqmcF1n5K/Kmq83DDDaRobAOmDfkcSFw7DRjbwP+dMjjG4E/WGs7AYwxW4D1wO+ttfUA1toOY8w/45SpjgqF1tqngKcAKisrtZIoIiIiIjOetZaGjr7IYS+D/f1qGzpp7uqPjEtLiqc010WV30dprjsS/gqz0tTmYRaayFC4Ayg1xiwA6nGC3x0jBxljFgNZwPYhTx8B7jfG/G+cFceLgG8YYxKATGttkzEmEbgWeGkC34OIiIiIyJQTClnqW3uoaRxd9tnRG23zkJGSQGmemyuW5YVP+nRRmucmPyOFOIU/CZuwUGitDRhjHgJ+i9OS4mlr7bvGmC8BO621vwoPvR348Yh9gc8DlwJ7cEpOf2Ot/TdjTDrw23AgjMcJhN+bqPcgIiIiIhJLgWCIo6d6ImWftQ3RPn89A8HIOJ8rCX+uixtWOW0eSnNd+PNc5LiS1eZBzmhCjwSy1r4AvDDiucdHPP7iGPcFgQfHeL4LWHN+ZykiIiIiElv9gRCHmqNtHgZX/g40dtEfjPb4y/ek4M91cdu6eU7ZZ54Lf46LrPSkGM5epjudEysiIiIiMkl6+oPUNjoN3avDZZ/VDZ0cbu4mGO7zYAzMy3LaPFy0KCdS8lmSk447RT3+5PxTKBQRERGZALd+1zku4bkHL4zxTCQWOvsCzorfyehev5qGTo6e6mZw01R8nKHIm0ZprourV+RH+vuV5LhITVKbB5k8CoUiIiIiIueotbvfOeFzRNnn8bbeyJik+DgW5qRTXujhpoqCSNlnsTedpAS1eZDYUygUEREREfkA1loaO/uoOdlJzZCyz5qGTpo6o20eUhPj8ee6WL/QGznspTTPzbysVBLU40+mMIVCERERERGc8HesrXfMss+2noHIOHdKAv5cF5cuyaU0140/fNhLQWaq2jzItKRQKCIiIiKzSjBkqTvVHV7xc0JfTXjlr6s/2uYhO91p83Bteb7T4iFc9pnrVpsHmVkUCkVERERkRhoIhjg8rM2D8+NAYyd9gWibh7yMZEpz3Xy8cl60x1+uC68rOYazF5k8CoUiIiIiMq31DgQ50NhFTWMnNeEm79UNnRxq6iIQbvMAUJiVSmmui41+b6TssyTHhSdVbR7k/Fie74n1FM6JQqGIiIiITAtdfYEh/f2iJZ9HWroZzH5xBoq86fhzXWxalhdu7u6mJDedtCR96ysyFv3JEBEREZEppa17gJrGjhF7/jqpb+2JjEmMNyzwpbN8rofrVxWET/p02jykJKrHn8jZUCgUERERkUlnraW5q9/Z7zei7LOxoy8yLjkhDn+ui8riLG7PnRc57GV+dhqJavMgcl4oFIqIiIhMgMebHw1/tjWm84g1ay0n2gfbPAwv+zzVHW3z4EpOoCTXxUWLciKrfv4cNwVZqcSrzYPIhFIoFBEREZEPLRSy1Lf2UN0wuuyzsy8QGZeZlkhprovNK/Ijp3yW5rmYk5GiNg8iMaJQKCIiIiLjFgiGONzSHW7zEG3wXtvYSe9AtM1DjjuZ0lwXN1cU4B/S48+bnqTwJzLFKBSKiIiIyCh9gSAHm6I9/pzw18HBpi4GgtE2DwWZqfhzXaxf6B1W9ulJU5sHkelCoVBERERkFuvuD1Db0DXstM/ahk4ONXdF2jwYA/Oz0yjNdXHpkrxI2WdJrgtXsr6dFJnu9KdYREREZBZo7x1wVvxOOit+g2WfdaeibR4S4gzFvnQWz3FzbXk+JbkuSnPdLMxRmweRmUyhUERERGQGaenqpzrc3mFo2efJ9mibh6SEOEpyXKyen8UtlfMiZZ9F3nS1eRCZhRQKRURERKYZay0NHX3hNg/R/n61DZ00d/VHxqUlxePPdVHl91Ga646Ufc7LTlObBxGJUCgUERERmaJCIcuxth5n1W9E2WdHb7TNQ0ZKAqV5bq5Ylhc+6dNFaZ6b/IwU4hT+ROQMFApFREREYiwYshxp6Y6s+tU2RPv89QwEI+N8riT8uS5uWOW0eSjNdeHPc5HjSlabBxE5ZwqFIiIiIpOkPxDiUHO0zcPgyt+Bpi76A9Eef/meFPy5Lm5bN88p+8xz4c9xkZWeFMPZi8hMpVAoIiIicp4FQ5ZDQR+HQrls+e17kfB3qLmbYLjPgzFQmJVKaa6bixblRMo+/bku3Cnq8Scik0ehUERERORDCIYsBxo72VPfxu66Nt6pb+PdY+30DNwPQPx/1lLkdXr8XbUiPxL8SnJcpCapzYOIxJ5CoYiIiMg4hUKWA01dvDMkAL5zrI3ufmffX2piPMvnZnDbunlk7/oWC+NPcPkXfkpygsKfiExdCoUiIiIiYwiFLIeau9hT38aeujb2hFcAO/ucUz9TEuNYlp/BLZXzWFHgobzQQ0mOK9Lq4d097wAoEIrIlKdQKCIiIrOetZbDzd1OAAyHwHfq2+gIB8DkhDiW5mdwU0VBJAD6c1wkqNG7iMwACoUiIiIyq1hrOdrS4+wBrG/lnXAIbA/3/UuKj2NpvpuPrZ5LWYGHsoJMSvNcJCoAisgMpVAoIiIiM5a1lrpTPc4ewPq2yF7Atp4BABLjDUvmZHDtysEA6GFRnpukBAVAEZk9FApFRERkRrDWcqytN7z/r5U99e3sqWvlVLcTABPiDIvnuLm6bI5TAlqQyaI5Lu35E5FZT6FQREREph1rLSfaeyMHwAzuA2zu6gcgPs6wKM/NpmVzWFHoobzAw+I5blISFQBFREZSKBQREZEp72Q4AA4tAW3q7AMgzsCiPDeXLsmlrNApAV2an6EAKCIyTgqFIiIiMqU0dPQO6wO4u66Nho5oAPTnurhoUQ5lBRmUFWayLD9DTeBFRD4EhUIRERGJmabOvmF9APfUtXGivRcAY6Akx8VGvy/SBmLZ3AzSkvTti4jI+aS/VUVERGRStHT1h4NfayQAHmvrjVxfmJPO+oXZ4QCYybK5GbiS9a2KiMhE09+0IiIict61djsBcGgJaH1rT+T6Al86lcXZThuIQg/L52bgTkmM4YxFRGYvhUIRERH5UNq6B3jn2JAAWN/K0ZZoACzyprF6fiZ3X1gUDoAePKkKgCIiU4VCoYiIiIxbe+8A7wzdA1jfxuHm7sj1edmplBdkcse6IsoLPayY68GTpgAoIjKVKRSKiIjImDp6B3j3WPuwAHiwqStyvSAzlfJCD7dUzosEwKz0pBjOWEREzoVCoYiIiNDVF+DdY+3srmsNl4A6AdBa5/pcTwplhR5uriigrDCTsgIP2QqAIiIzgkKhiIjILNPdH2DvsfYhewDbqG3sjATAORlOALxhVUGkGbzPlRzbSYuIyIRRKBQREZnBevqD7D3eHm4D0c6e+lZqGjoJhQNgrjuZ8kIP15bnOyWgBR5y3SmxnbSIiEwqhUIREZEZoncgyL7j7cOawVc3dBIMJ0CfywmAm1fkUx5uBZGXoQAoIjLbKRSKiIhMQ32BIPuPd7C7vo136pwS0OqTHQTCAdCbnkRZoYcrluVFegHOyUjBGBPjmYuIyFSjUCgiIjLF9QdCvHeiI3wCaCu769p4/2QHA0EnAGalJVJWmMmlS3IoK8ikrNDDXI8CoIiIjI9CoYiIyBQyEHQC4OABMHvq2njvRAf9wRAAntREygs93PeRhZQXOHsAC7NSFQBFROScKRSKiIjEyEAwRPXJznAAbGVPXRv7TnTQH3ACoDslgfJCD/duLKa8wGkDMS9bAXC6WJ7vifUURETGRaFQRERkEgSCIWoaOyMHwOyua2Pf8Xb6BgNgcgIrCjzcs6HY2QNY4KHIm6YAKCIiE06hUERkktz63e0APPfghTGeiUy0YMhSOywAtrL3eDu9A04ATE+KZ0WBh7vWF0X6ABZ704mLUwAUEZHJp1AoIiLyIQRDloNNnewOB8A9dW28e6ydnoEgAGlJ8ayY6+GOdUWRPoALfQqAIiIydSgUioiIjFMoZDnY3BVZAXQCYBtd/U4ATE2MZ/ncDG5dO4/y8ArgwhwX8QqAIiIyhSkUioiIjCEUshxu6WZ3XatzEEx4BbCzLwBAckIcy+dm8CdrCikrdA6BKclJJyE+LsYzFxEROTsKhSIiMutZaznS0s3uurZIAHznWBsdvU4ATEqIY1l+BjeuLojsASzNdSkAiojIjKBQKCIis4q1lrpTPdE9gOFWEO2DATA+jqX5bq5fOTeyB3BRnptEBUAREZmhFApFRGTGstZS39oT3QMY/tHaPQBAYrxhyZwMrimfG9kDuCjPTVKCAqCIiMweCoUiIpPk8eZHw59tjek8ZiprLcfbeqMloPXOx5aufgAS4gyL57jZvHxOpAR08Rw3yQnxMZ65iIhIbE1oKDTGbAb+FogHvm+t/fKI618HLgk/TANyrbWZ4WtfAa4B4oDfAZ+11lpjzBrgB0Aq8MLg8xP5PkREZGqx1nKyvS96CEw4ADZ1OgEwPs6wKM/N5UtzI4fALJnjJiVRAVBERGSkCQuFxph44O+BK4A6YIcx5lfW2r2DY6y1nxsy/mFgdfjzDUAVUB6+vBW4CHgV+A7wAPAHnFC4GdgyUe9DRERir6G9N9wEPloC2tjRB0CcgUV5bi5enBvZA7gsP0MBUEREJk4oCJ0nof04dByLfmx6D1KzYz27szaRK4XrgBpr7QEAY8yPgY8Be08z/nbgifDnFkgBkgADJAInjTH5QIa1dnv4Nf8RuAGFQhGRGaOxoy9yAuie+lb21Ldxsj0aAP25Lj5S6qO8wENZoYdl+R5SkxQARUTkPOnrhI7j0H7sNB+PQ+cJsKHh98UlgImHpPTYzPtDmMhQWAAcHfK4DrhgrIHGmCJgAfAygLV2uzHmFeA4Tij8O2vtPmNMZfh1hr5mwQTMXUREJkFzZ1+kCfxgCejxtl4AjIGSHBcbSnyURQJgBunJ2g4vIiLnIBSC7iYn3LUfG7LCNyLw9bWNvjfZAxn54M6HkiXOx4x8yCgIfz4X0nzww+sm/32dBxP5L6sZ47nT7f27DXjeWhsEMMb4gaVAYfj674wxHwV6xvuaxlvWhb4AACAASURBVJgHcMpMmT9//llMW0REJsKprv5I6aezF7Cd+tboX+sLc9JZtyDbCYAFHpYXeHApAIqIyHgM9I4IefUjSjuPQ8cJCA0Mv8/EgWuOE/C8fljw0WjIcw+GvjmQ7IrN+5okE/mvbR0wb8jjQuDYacbeBvzpkMc3An+w1nYCGGO2AOuBZ4kGxQ98TWvtU8BTAJWVlTqIRkRkErV29/NOfTu7wz0A99S3UXcqGgAX+NKpKMring3FrCjwsLwgg4yUxBjOWEREpiRrobtl+L699nDoG1zZ6zgGPadG35uY7oS7jHwoqgqv9M0d/jE9F+L1H5AT+TOwAyg1xiwA6nGC3x0jBxljFgNZwPYhTx8B7jfG/G+cFceLgG9Ya48bYzqMMeuB14G7gW9N4HsQEZEzaOsZ4N3wCaCDAfBIS3fkepE3jZXzMrlrfVFkBdCTqgAoIjLrBfrDK3in2b/XfsxZ3Qv2jbjRQHqOE+qyimD++rEDX3KGsxdBzmjCQqG1NmCMeQj4LU5Liqette8aY74E7LTW/io89HbgxyPaSjwPXArswSkP/Y219t/C1z5NtCXFFnTIjIjIpOnoHeCd+nb21LdG+gEeao4GwHnZqZQVeLh93XzKCjysKMggMy0phjMWEZFJZy30to4u3xwZ+LqbRt+bkBIt3yxcG17pmzu8pNM9B+L1n4vn04SulVprX8BpGzH0ucdHPP7iGPcFgQdP85o7gRXnb5YiIjKWzr4A70b2ADoB8EBTV+R6QaYTAD9eOS+yDzArXQFQRGRGCwbCrRiOjS7pHBr8BrpH35vmja7izV09JPDNjR7ikpql1b0YUAGtiIjQ1Rdg7/F2pw1EndMG4kBTF4M1HHM9Kawo8HBTRQErwgHQ60qO7aRFROT86uv44H177cehq2F0K4b4JGf1zj0X8sth0eZoyMsoiH6eoH83piqFQhGRWaanP8je4+E+gOE9gDWNnZEAOCfDCYAfW1UQLgH1kOPWP+QiItNWKAhdjR+wby8c/Po7Rt+b4om2XchbPnrfnnuuswIYFzf570vOG4VCEZEZrHcgyN7j7U4fwHAJaHVDB6FwAMxxJ1Ne4OGa8vxICWhuRkpsJy0iIuPX332Gg1rCrRiczm9RJj68upcPOUug5NIRrRjCH5PSYvO+ZFIpFIqIzBC9A0H2n+hgT51zCMye+jaqGzoJhhOgz5VEWYGHK1fMoazAQ3mhhzwFQBGRqSkUgu7mMfbtjejF1ztGo/Ukd7Rkc8FHRwe9jLnO6Z1x8ZP/vmRKUigUEZmG+gJB3jvRMawE9P2THQTCATA73QmAVyzLY0U4AM7JSMFo876ISOwF+sYu3xwa/DpOQLB/xI0GXHnhVgwLoGjDiH17g60Y3DF5WzJ9KRSKiExx/YEQ75/siKz+7alv5b0THQwEnQCYmZZIWYGHB5csdEpACzOZ61EAFBGZdNY6TdQHQ17koJYRwa+7efS9iWnRVbx5I/ruDe7pc+Wp0bpMCP2uEhGZQgaCTgDcEwmAbew/3kF/0DnpzZPqBMD7PrIwsgewMCtVAVBEZKIFB5zVu2Ehr350K4ZA7+h703OcUOcpgMLKIWWcg8FvrnOgi/4ulxhRKBQRiZFAMER1Q2ckAO6ub2Pf8Xb6A04AdKckUFbg4d6Nxc4ewIJM5mUrAIqInFfWQl/72Pv2hvbi62oE7PB745Odw1oyCqCgYuyDWtxz1IpBpjyFQhGRSRAKWQ4HfbwfzOcnv3yH3fVt7D3WTt9gAExOYHlBBvdsKHb2ABZ4mJ+dRlycAqCIyDkLBcON1sc6qCW8n6/9OAx0jb43NStavpm/8jStGLK1uiczgkKhiMgEOdrSzWs1TWytaWJ7bTPNXfcDkL6rjuUFHu5aX0RZoVMCWuxNVwAUETkbfZ2nb8UwuH+v88ToRutxCeEVvHDfPf8VzqresJLOfEhMjc37EokBhUIRkfOkpaufbbVNvFbTzGs1TRxp6QYg153MRYtymL/vuyyJr2fT//qlAqCIyOmEQtDd9MH79tqPQ98YrRiSPdFQV7Jk9L69jLmQ5lOjdZERFApFRM5Rd3+AHYdO8VpNE6/VNPHusXbAKQVdX+Llk1XFbCz1UZLjwhjDu3+zB0CBUERmr4HeEX32jg3ftzfYaD00MPw+EweuOU7A8/qd3ntj7d9LdsXmfYlMcwqFIiLjFAiG+GNdG9vCJaFvHjnFQNCSFB9HRVEmj2xaxAa/j/ICDwnx+l9oEZlFrIXulg/et9dxzGnXMFJienR1r6hq9L69jHxIz1UrBpEJpD9dIiKnYa2lpqGTreGVwNcPtNDRF8AYWD43g09WLaDK72NtcTapSfGxnq6IyMQI9IdX8E6zb6+9PtxovW/EjcZpxZCRD5nzYf4F4RW9EYEvOUOHtYjEmEKhiMgQx1p7eK2miW21zr7Ahg7nm5wibxrXrZrLRr+PCxd6yUpPivFMRUQ+JGuht210+ebI4NfVOPrehJRo2Wbh2tH79gZbMcQnTv77EpGzplAoIrNaW/cA2w80R/YFHmhyjiX3pidR5fdR5feyocTHvOy0GM9UROQsBANOK4ZhB7WMEfwGukffm+aNruLNXT18397g56lZWt0TmUEUCkVkVukdCLLr8Cm21jSxraaJPfVthCykJcVzwYJs7rhgPlV+H4vz3DoQRkSmpr6OEQ3W66OlnJFG6w1jtGJIjK7o5ZfDos3RvXyR1b18SEyJzfsSkZhRKBSRGS0Ysrx7rC2yL3DnoVP0BUIkxBlWz8/k4UtL2VjqY2VhJkkJOhxGRGIoFHRKNcfquzd0D19/x+h7UzzR8s285adptO5VKwYRGZNCoYjMKNZaDjZ1hctBm9lW20R7bwCAJXPcfGJ9ERv9PtYuyMaVrL8CRWSS9HefvtH64OcdJ8AGh99n4p29ee58yFkMJZeO3YohSSXuInLu9B2RiEx7DR29bKtpjqwGHm/rBaAgM5WrVuSzIbwvMMedHOOZisiMNtALzdXQsB8a90HDXgj0wpeLoLd19Pgkd7R8c2jfvaGBLz0H4nS6sYhMrDOGQmPMQ8CPrLVjNJYREZl8Hb0DvH6gxdkXWNvE+yc7AchMS2RDiZeH/D6qSnwUedMwOghBRM63QB8010DDPmjcH/3YciC6j8/EQ3wSJKbCiptGBL7BVgzu2L4PEZGw8awUzgF2GGPeBJ4GfmuttRM7LRGRqL5AkLeOtEaaxv+xro1gyJKSGMfa4mxuqihko9/HsvwMHQ4jIudPoB9aakeHv+baaJmniYPshZCzBJbf6HzMXQpePzx7kzPmmv8vdu9BRGQczhgKrbV/boz5C2ATcC/wd8aYnwD/v7W2dqInKCKzTyhk2XeiPbIv8I2DLfQMBIkzUF6YyacvKqHK76OiKJPkBJVViciHFBxwVvlGhb8aCAXCgwxkL4CcpbD0Oudj7hLwluq0ThGZ9sa1p9Baa40xJ4ATQADIAp43xvzOWvv/TOQERWR2ONLczWu1zkrg9tpmWrr6AfDnurh17Tw2lHi5YKEXT6oaIYvIOQoGnPDXuG/Ivr/B8DcQHmQgq9hZ7Vt8tfMxZwn4Sp1SUBGRGWg8ewr/B/DfgCbg+8Cj1toBY0wcUA0oFIrIWWvu7GNbbbhpfG0TR1t6AJiTkcLFi3PY6PexocTHHI/+B15EzlIoCC0Hxwh/1RDsj47LLHJC36Irh4S/RTrJU0RmnfGsFPqAm6y1h4c+aa0NGWOunZhpichM090f4PWDLeF9gc3sO94OgDslgQsXern/IwvZUOKjJCddh8OIyPiEgnDq0PCSz4b90PQ+BPui4zzznVJP/2XR8JezGJLSYzZ1EZGpZDyh8AWgZfCBMcYNLLPWvm6t3TdhMxORaW0gGGJ3XStbq5t5rbaJt46cYiBoSYqPY01RFo9euZgqv48VczNIiFczZRH5AKEQtB4eEf72OeEv0Bsdl1HohL+FF4XD31In/CW7Yjd3EZFpYDyh8DtAxZDHXWM8JyKznLWW9092hg+HaeIPB5rp6g9iDKyY6+G/b1xIld9LZVE2qUk6HEZExhAKQduRcMnn/uHhb6A7Oi6jwFntW/DR8KpfeOUvJSN2cxcRmcbGEwrN0BYU4bJRNb0XEepbeyIhcFttM40dTrnWAl86N6wuYKPfx4UlXjLTkmI8UxGZUqyFtqPD9/s17oPG92GgKzrOne8EvjX3RFs95CyGFE/Mpi4iMhONJ9wdCB82853w488AByZuSiIyVbV297O91ikHfa2mmYNNzjdvPlcSVeGG8Rv8XgqzdEiDiOCEv/b6McLfe9DfGR3nynNCX8Vdw8Nfalbs5i4iMouMJxR+Cvgm8OeABf4DeGAiJyUiU0PvQJCdh06xtaaJbbVN7Klvw1pIT4rngoVePrG+iI1+H4vyXDocRmQ2sxY6jo/u89f4HvS1R8el5zihb9UdQ8LfEkjLjt3cRURkXM3rG4DbJmEuIhJjwZBlT31bpCR05+FT9AdCJMQZKuZn8T8vW0SV38vKeZkk6nAYkdnHWug8OTr8NeyHvrbouDSvc8hL+S1Dwt9SSPfGbu4iInJa4+lTmAL8d2A5EGkYZq395ATOS0QmgbWW2sYuttU2sbXaORymvTcAwNL8DO5eX0RVqY91xdmkJ2srscisYS10Nozu89e4H3pbo+NSs53AV/Yn0VW/3KWQ7ovd3EVE5KyN57u8Z4H9wJXAl4A7AbWiEJmmTrb3hlcCncbxJ9qd49wLs1K5uiyfDX4fG0q8+FzJMZ6piEyKzsYxwt8+6DkVHZOS6YS95TeOCH85oNJxEZFpbzyh0G+t/bgx5mPW2h8aY/4Z+O1ET0xEzo/23gFeP9ASKQmtbnAOd8hKS2RDiY8qv4+Nfh/zvTocRmRG62oOh74hJZ+N+6C7OTom2eP0+Vt6/fDw58pT+BMRmcHGEwoHwh9bjTErgBNA8YTNSEQ+lL5AkDcPtzoloTVN7K5rIxiypCTGsW6Bl49XFrKhxMey/Azi4vRNnsiM090yusl7437oaoyOSc5wTvdcfPXw8OfOV/gTEZmFxhMKnzLGZOGcPvorwAX8xYTOSkTGLRSy7D3ezms1TgjccaiF3oEQ8XGGlYUePnNxCVV+H6vnZ5KcoKbxIjNGz6nRTd4b9zsHwQxKcjmBb9GVzkEvuUucjxlzFf5ERCTiA0OhMSYOaLfWngJ+DyyclFmJyGlZaznS0u20iahpZlttE6e6nQX90lwXt62dT5XfxwULs8lISYzxbGWo5flquC3noLdt9H6/hv3QeSI6JjHdWfnzXz681YOnUOFPRETO6ANDobU2ZIx5CPjJJM1HRMbQ1NnHttpmXqtu4rXaJupO9QCQ70nh0iV5bCz1sqHER15GyhleSUSmrN52p6/fyPDXcSw6JjENfIug5JIR4W8exKlNjIiInJvxlI/+zhjzCPAc0DX4pLW2ZcJmJTLLdfUFeONgS6QkdP+JDgAyUhK4sMTLgx9dyAa/j4W+dDWNF5lu+jqHhL8hh76010XHJKQ44W/BR4aHv8wihT8RETnvxhMKB/sR/umQ5ywqJRU5bwaCId4+2ho5IfStI60EQpakhDjWFmfx6JWL2ej3saLAQ7wOhxGZHvq7wuFvRJP3tiPRMfHJkLMIijY45Z+D4S+rGOK0B1hERCbHGUOhtXbBZExEZDax1vLeyQ62VjexrbaZ1w8009UfxBgoK/Bw/0cXUlXio7I4i5REfWMoMqX1d0PTe9GSz8b3nBDYejg6Jj7JWfmbtw7W3B0+9GWpwp+IiEwJZwyFxpi7x3reWvuP5386IjNX3aluttU0OwfE1DbT1NkHwEJfOjdVFFLl97J+oZfMtKQYz1RExjTQA03vjz705dRhnAIaIC4RfKVQsAZWfyJa+pm1AOLHU5wjIiIy+cbzL9TaIZ+nAJcBbwIKhSIf4FRXP9sPNEdKQg81dwOQ405mo99Lld9pHD83MzXGMxWRYQZ6obl6jPB3CGzIGROXAF4/5K+ClbdHw1/2QojXqb8iIjK9jKd89OGhj40xHuDZCZuRyDTV0x9k5+EWtoZD4LvH2rEWXMkJrF+Yzd0XFrOx1EdprkuHw4hMBYE+aK4Z3eS95UA0/Jl48JZA3goo+/iQ8FcCCVrVFxGRmeFcalm6gdLzPRGR6SYQDLGnvi28EtjMrsOn6A+GSIw3rJ6fxecuX0SV30t5YSaJ8TotUCRmAv3QUjs6/DXXgg06Y0y8s8qXuxSW3xRt8u71K/yJiMiMN549hf9GZLMEccAy1LdQZiFrLbWNnbwW3hf4hwPNdPQGAFiWn8E9VcVsKPGybkE2aUnaOyQy6YIDTtAb2eevpRZCzp9VTJyzvy93KSy9Pnrap68UEpJjO38REZEYGc93rk8O+TwAHLbW1p1usMhMcqKt11kJrHVKQk+2O4fDzM9O49ryfKr8Pi5c6MXr0jeTIpMmGHBKPEeGv+YaCA2EBxnnZM/cpbDkmuHhL1H7eEVERIYaTyg8Ahy31vYCGGNSjTHF1tpDEzqzme6Za5yP9/46tvOQYdp7B/hDbXOkaXxtYxcA2elJbCgJHw5T4mO+Ny3GMxWZBUJBaDk4RvirhmB/dFxmkRP6Fl05JPwtgiT9ORURERmP8YTCfwU2DHkcDD+3duzhItNH70CQN4+ciuwL3F3XSshCamI86xZkc9va+VT5fSyZ4yZOTeNFJkYo6JzsObLJe9P7EOyLjsuc7+zzK7083OdvMPylx2zqIiIiM8F4QmGCtTbyX7LW2n5jjHbdy7QUDFn2HmuPlIO+cbCFvkCI+DjDqnmZPHRpKVUlXlbPzyIpQYfDiJxXoRC0Hhrd5L3pfQj0Rsd55jmrfSUXDwl/iyHZFauZi4iIzGjjCYWNxpjrrbW/AjDGfAxomthpiZwf1loON3eHG8Y7TeNbu509R4vz3NxxwXw2+n2sW5CNO0W9xUTOi1AI2o6M7vPX+D4EeqLjMgqc8Lfgo9FWDzmLIdkdu7mLiIjMQuMJhZ8CfmSM+bvw4zrg7ombksiH09jRx7bwSuBrNc3UtzrfhM71pHDF0jyq/D42lHjJzUiJ8UxFpjlroe3o2OFvoCs6zp3vhL7Ke4eHvxRP7OYuIiIiEeNpXl8LrDfGuABjre2Y+GmJjF9nX4A3DjaztbqZbbVN7D/h/Bb1pCZy4UIvn7q4hI1+H8XeNDWNFzkX1kJ7/Rjh7z3o74yOc81xSj0r7hoS/pZAambs5i4iIiJnNJ4+hX8DfMVa2xp+nAX839baP5/oyc1k7x5vA2B5jOcxHfUHQrx9tDW8EtjE20dbCYQsyQlxrC3O5vObC6jye1k+10O8DocRGT9roeP46Cbvje9BX3t0XHquE/5W3Rlt8p6zGNKyYzd3EREROWfjKR+9ylr7Z4MPrLWnjDFXAwqFMilCIct7JzsibSLeONhCd3+QOANlhZk88NGFbPT7qCjKIiUxPtbTFZn6rIWOE6NbPTS+B31t0XFpPme1r/zWaPjLXarwJyIiMsOMJxTGG2OSrbV94PQpBNSpWybU0ZZuttU2sbWmmW01TTR3OQfgLsxJ50/WFLKhxGka70nT4TAip2UtdDaMEf72Q29rdFxqthP2yv4kWvKZuxTSfbGbu4iIiEya8YTCfwL+wxjzTPjxvcAPx/PixpjNwN8C8cD3rbVfHnH968Al4YdpQK61NtMYcwnw9SFDlwC3WWt/YYz5AXARMPjf2fdYa98ez3xk6mrp6md7bXPklNDDzd0A5LqT+eiiHKdpvN9Lvic1xjMVmaI6G8cIf/ug51R0TEqmE/aW3zgi/OWA9tuKiIjMWuM5aOYrxpjdwOWAAX4DFJ3pPmNMPPD3wBU4J5buMMb8ylq7d8hrf27I+IeB1eHnXwFWhZ/PBmqAF4e8/KPW2ufP+O5kyurpD/LGoRa2hUtC9x5vx1pwJSewfqGXezcUU+X34c916XAYkaG6msOhb0iT98Z90N0cHZPicUo9l30s2ucvZym4chX+REREZJTxrBQCnABCwC3AQeCn47hnHVBjrT0AYIz5MfAxYO9pxt8OPDHG838CbLHWdo9zrjIFBYIhdte38Vq1EwLfOtJKfzBEUnwcFUWZ/F+XL6Kq1Ed5gYeEeDWNF6G7ZchBL0MOfelqjI5JznBW+5ZcMzz8ueco/ImIiMi4nTYUGmMWAbfhhLVm4DmclhSXnO6eEQqAo0Me1wEXnOZrFQELgJfHuHwb8LURz/21MeZx4D+Axwb3O454zQeABwDmz58/zinL+WKtpaahk63hXoGvH2imoy+AMbAsP4N7q5yVwLXF2aQm6XAYmcV6To0u+WzYD10N0TFJbud0z0VXDg9/GXMV/kRERORD+6CVwv3AfwHXWWtrAIwxn/uA8SON9Z2KPc3Y24DnrbXBYS9gTD5QBvx2yNNfwFm5TAKeAj4PfGnUF7L2qfB1KisrT/d15Tw63tbDazXNkVYRDR1OVi/ypnHtyrls9Pu4sMRLdnpSjGcqEgMDPU5bh/5u2PJYNPx1noiOSUx3wl/pFcP7/HkKFf5ERESmg3t/HesZnJMPCoU344S1V4wxvwF+zNhB73TqgHlDHhcCx04z9jbgT8d4/hbg59bagcEnrLXHw5/2hQ+/eeQs5iTnUVv3ANsPNIdPCW3iQGMXAN70JDb4fWz0e9lQ4mNedlqMZyoyyayFU4egbifUvQF1O+DEHggFnOtvHnPCX8ml4VW/8A/PPIhT+bSIiIhMrtOGQmvtz4GfG2PSgRuAzwF5xpjv4AS1F093b9gOoNQYswCoxwl+d4wcZIxZDGQB28d4jdtxVgaHjs+31h43zukjNwDvnGEecp70DgR58/CpcEloE3vq2whZSEuK54IF2dyxbj5Vfh+L89zEqWm8zCZ9nXDsTSf81e10Pg7u/UtMh4IK2PA/4L0tzuP7fqfwJyIiIlPGeE4f7QJ+BPwofBLox4HHGH4a6Fj3BYwxD+GUfsYDT1tr3zXGfAnYaa39VXjo7cCPrbXDSjyNMcU4K43/OeKlf2SMycFZtXwb+NSZ3oOcm2DI8u6xNqdNRE0zOw610BcIkRBnWDUvk4cvLWVjqY+VhZkkJegbXJklQiFoqYWjb0RDYMO7YEPOdW8p+K+AeWuhcK2z9y8+/Fft0TecjwqEIiIiMoWM9/RRAKy1LcB3wz/GM/4F4IURzz0+4vEXT3PvIZzDakY+f+n4Zitny1rLwaYuXqtt5rXqJrYfaKatx6ncXTLHzSfWF1Hl97JugRdX8ln91hGZvnpaoX7XkFLQndHG78keKFwDSx51AmDBGkjLju18RURERM6SvrOf5Ro6etk25HCYY229ABRkpnLl8jyq/D42lPjIcSfHeKYikyAUdNo+1O1wfhzdAU3vhS8a5+CXZR9zAmDhWvAt0qqfiIiITHsKhbNMR+8AbxxsiewLfP9kJwCZaYlsKPHymRIfG/0+irxpahovM19XM9TvjJaC1r8J/R3OtdRsJ/iVf9z5OLcCUjJiO18RERGRCaBQOMP1B0K8deSUsxJY28zbR1sJhizJCXGsW5DNTRWFbPT7WJafocNhZGYLDsDJd6OrgHU7oOWAc83Ew5wVsPLW6Cpg9kK1gRAREZFZQaFwhgmFLPtOtLOtppmtNU28cbCFnoEgcQbKCzP51EULqfL7qJifRUqimsbLDNZxMtoOom6nswoY6HGuufKc4Ffx38KrgKsgKT228xURERGJEYXCGeBoS3ekHHRbbTMtXf0AlOSkc0tlIVV+Hxcs9OJJTYzxTEUmSKDP6QM49ETQtiPOtbhEyF8Ja+6JngjqmadVQBEREZEwhcJpqKWrn221TgjcWtPE0RZn9SMvI5mLF+dQVeKjyu9jjiclxjMVmQDWQlvd8DLQ43+EoPOfIWQUOuFv/aecADinHBL1Z0FERETkdBQKp4Hu/gBvHGwJnxDazN7j7QC4UxJYv9DLfRsXUuX3UpLj0uEwMvP0d8Pxt4efCNp5wrmWkAJzV8MF4QBYWAkZc2M7XxEREZFpRqFwCgoEQ/yxri2yEvjWkVMMBC1J8XGsKcrikU2LqPL7KCvwkBCv4/BlBrEWTh10yj8HS0FPvgOhgHM9awEs+KgTAOethbwVEK+yaBEREZEPQ6FwCrDWUt3QydbqJrbVNvGHAy109gUwBpbPzeCTGxew0e+jsiib1CQdDiMzSF+HcwDM0FLQ7mbnWpILCiqg6rPhxvCV4MqJ7XxFREREZiCFwhhpDLl5O1DM9378Fq/VNtPY0QdAsTeNj62aS5Xfx4ULvWSlJ8V4piLnSSgEzTXDTwRt2As25Fz3LYJFVzkloIVrnUbxcfpPEBEREZGJplAYA6GQ5X90fZJ2m4avpokN4YbxG/xeCrPSYj09kfOj5xTU73L2ANbtcJrE97Y511I8zsrfkmudMtCCNZCaFdv5ioiIiMxSCoUxEBdn+FzKr8mNa+Pq//UzHQ4j018oCA37oiuAdW9A0/vONRMHuctg+Y3RxvDeUojTflgRERGRqUChMEbWJdYAKBDK9NTVNHwfYP2b0N/pXEvzOsGv/NbwXsAKSHbHdr4iIiIicloKhSLywYIDzgmgR4eEwFMHnWsmHuaUwcrboyeCZi1QY3gRERGRaUShUESGaz8+fBXw2FsQ6HWuueY4wa/yXicE5q+CJO2DFREREZnOFApFZrNAHxz/45AQuBPajjrX4pMgfyVUfjK6F9BTqFVAERERkRlGoVBktrDWCXx1O6KloCd2Q7Dfue6Z7wS/9Z8JrwKWQ8L/ae/eo+wqyzuOf59cCbcEQgghAcIlEgPEJISoqJXSulRkgRfUsCwVFoq6RK2W1ssfaOmy1baraK2XYkXQugCLaCmXKq1orRcgCeEagYgXQgIJlySEkJDL0z/2jnNmmCGTMGf2zHm/n7Vm5Zz9vuec5/jyOvnlfffeAA8z9wAAEsZJREFUY5utWZIkSW1nKJQ61bMbq62frVtBNzxatY0aV10A5mXv67ox/L5Tmq1XkiRJjTAUSp0gE554sHsAfORuyG1V+/5HwBEndW0DnXwMjBzdZMWSJEkaIgyF0nC0aT2sXNJ9K+gzT1RtY/aubgb/yg93hcC9JjZbryRJkoYsQ6E01G3fXt0IvnUVcPUyIKv2A46Gmad0BcBJM2HEyEZLliRJ0vBhKJSGmo1PwMOLW0LgYti8rmrbY3wV/GadXp8LeDyMm9BsvZIkSRrWDIVSk7ZthTXLum8DffyBqi1GwIHHwLFvrm8MvwD2PxJGjGi2ZkmSJHUUQ6E0mDas6b4N9OElsOXpqm3PA6rwN+fM6s+D58LYfZqtV5IkSR3PUCi1y9Zn4dG7u4fAJ39TtY0YBQcdB3PfAdMWwLT5sN90bwwvSZKkQWcolAbK+pVd4e+h22DVUti6qWrbZ0q1+jf/3HoVcA6MHtdsvZIkSRKGQmn3bNkEq+7ovgq4/uGqbeQYmDIHTnhXtQI47QTYd6qrgJIkSRqSDIXSzmTC2t91D4Cr7oTtW6r2CYfCoS+rt4GeAAcdC6PGNluzJEmS1E+GQqmnZ5+Glbd3vyLo06urttF7wsHz4OXv77ov4D6Tm61XkjQ0nXN90xVIUr8YClW2THj8Vy2rgLfCo/dCbqva9z8SjjwZDqkD4IHHwEinjSRJkjqHf7tVWTatq24D0boV9Jknq7Yx+8C04+FVH6m2gk49Hvaa2Gy9kiRJUpsZCtW5tm+Hx+7rfkXQNb8EsmqfNBNmntq1DXTS0TBiZKMlS5IkSYPNUKjOsfEJWLGo5cbwi2Hz+qptjwlV8Dv2zdUVQQ+eB+MmNFuvJEmSNAQYChtyzJTxTZcwvG3bCqvv7b4N9PHlVVuMgMnHwHFn1KuAC2Dikd4SQpIkSeqFoVDDw4bV3beBrlwCWzZWbXtNqoLfnHfUN4afC2P3brZeSZIkaZgwFGro2fosPHJX91XAtb+t2kaMgoNmw9yz4JAF1VbQCYe5CihJkiTtJkOhmrfu4e4BcOVS2La5att3ahX8Fry7Wg2cMhtGj2u2XkmSJKmDGAo1uLY8A6vuqLeB3lpdGOaplVXbyLHV1s8F7+66Iuj4qc3WK0mSJHU4Q6HaJ7Pa9vlQyyrgI3fB9i1V+4TD4LATu7aBTj4ORo1ptmZJkiSpMIZCDZzNG2Dl7bDi1q5bQzy9pmobvWd1M/gTz+9aBdz7wGbrlSRJkmQo1G7KrG4B0XpF0NX3QG6v2iceBUe9ploBnHYCHDgLRvqfmyRJkjTU+Ld09c+mddXN4Fu3gm5aW7WN3bdaBXzVBdVW0KnHw577N1uvJEmSpH4xFOq5tm+DNffV4a/eCrrmPiCBgANfDLNO69oGesDRMGJE01VLkiRJ2g2GQsHTj8PDi1puC7EYnn2qahu3XxX8jj2j2go6dR7sMb7ZeiVJkiQNGENhabZtrc79e6jlYjBP/Kpqi5Ew+RiY/bb6iqAnwP5HeGN4SZIkqYMZCjvdU4923wa68nbYsrFq2+vAKvzNO6sKgAfPhTF7NVuvJEmSpEFlKOwkW5+FR+7sfkXQdb+r2kaMhimzYd47u64IOuFQVwElSZKkwhkKh6tMWP9w922gq+6AbZur9n2nVeHvpe+pVgMPmg2j92i2ZkmSJElDjqFwuNjyDKxc2n0r6FOrqrZRe1RbP196XtcVQfc9uNl6JUmSJA0LhsKhKBOe/HXXCuCK2+CRu2D71qp9v+kw/ZUwbUG1Gjj5WBg1ptGSJUmSJA1PhsKhYPMGWLmk+1bQjY9VbaP3qm4DceIHu1YB957UbL2SJEmSOoahsAmZsOFR2PwUfPkVsPpeyO1V28QZ8KLXdl0MZtKLYaTDJEmSJKk9TBtNiIB1D8G2LTB5Fsx8Q7UVdOo82HP/pquTJEmSVBBDYVMmz4aRo+Gs7zZdiSRJkqSCGQqb4oVhJEmSJA0BI5ouQJIkSZLUnLaGwoh4XUTcFxHLI+JjvbRfHBFL65/7I2JtffwPW44vjYhNEfHGuu3wiLglIh6IiKsiwiU3SZIkSdpNbQuFETES+CLwemAWcGZEzGrtk5kfzsw5mTkH+AJwTX385pbjJwMbgR/UL/sscHFmzgCeBM5t13eQJEmSpE7XzpXCBcDyzHwwM58FrgROf57+ZwJX9HL8DODGzNwYEUEVEq+u2y4H3jiANUuSJElSUdoZCqcCD7U8X1Efe46IOAw4HPhhL80L6QqLE4G1mbl1Z+8pSZIkSdq5dobC6OVY9tF3IXB1Zm7r9gYRU4DjgO/v6ntGxHkRsSgiFq1Zs6afJUuSJElSWdoZClcAh7Q8nwas7KNv62pgq7cB383MLfXzx4AJEbHjVhp9vmdmXpKZ8zNz/qRJk3a5eEmSJEkqQTtD4W3AjPpqoWOogt+1PTtFxNHAfsDPe3mPbucZZmYCN1OdZwjwTuA/BrhuSZIkSSpG20Jhfd7f+VRbP5cB387MeyLioog4raXrmcCVdeD7vYiYTrXS+OMeb/1R4CMRsZzqHMOvtecbSJIkSVLnG7XzLrsvM28Abuhx7MIezz/Vx2t/Qy8XkcnMB6mubCpJkiRJeoHaevN6SZIkSdLQZiiUJEmSpIIZCiVJkiSpYIZCSZIkSSpYWy80I0lqcc71TVcgSZL0HK4USpIkSVLBDIWSJEmSVDBDoSRJkiQVzFAoSZIkSQUzFEqSJElSwQyFkiRJklQwQ6EkSZIkFcxQKEmSJEkFMxRKkiRJUsEMhZIkSZJUMEOhJEmSJBXMUChJkiRJBTMUSpIkSVLBDIWSJEmSVDBDoSRJkiQVzFAoSZIkSQUzFEqSJElSwQyFkiRJklQwQ6EkSZIkFcxQKEmSJEkFMxRKkiRJUsEMhZIkSZJUMEOhJEmSJBXMUChJkiRJBTMUSpIkSVLBDIWSJEmSVDBDoSRJkiQVzFAoSZIkSQUzFEqSJElSwQyFkiRJklQwQ6EkSZIkFcxQKEmSJEkFMxRKkiRJUsEMhZIkSZJUMEOhJEmSJBXMUChJkiRJBTMUSpIkSVLBDIWSJEmSVDBDoSRJkiQVzFAoSZIkSQUzFEqSJElSwQyFkiRJklQwQ6EkSZIkFcxQKEmSJEkFMxRKkiRJUsEMhZIkSZJUMEOhJEmSJBXMUChJkiRJBTMUSpIkSVLBDIWSJEmSVDBDoSRJkiQVzFAoSZIkSQVrayiMiNdFxH0RsTwiPtZL+8URsbT+uT8i1ra0HRoRP4iIZRFxb0RMr49fFhG/bnndnHZ+B0mSJEnqZKPa9cYRMRL4IvAaYAVwW0Rcm5n37uiTmR9u6f8BYG7LW3wD+HRm3hQRewPbW9r+IjOvblftg+Kc65uuQJIkSZLaulK4AFiemQ9m5rPAlcDpz9P/TOAKgIiYBYzKzJsAMnNDZm5sY62SJEmSVKR2hsKpwEMtz1fUx54jIg4DDgd+WB96EbA2Iq6JiNsj4u/rlccdPh0Rd9bbT8f28Z7nRcSiiFi0Zs2aF/5tJEmSJKkDtTMURi/Hso++C4GrM3Nb/XwU8CrgAuAE4Ajg7Lrt48DM+vj+wEd7e8PMvCQz52fm/EmTJu3WF5AkSZKkTtfOULgCOKTl+TRgZR99F1JvHW157e311tOtwPeAeQCZuSorm4GvU21TlSRJkiTthnaGwtuAGRFxeESMoQp+1/bsFBFHA/sBP+/x2v0iYscS38nAvXX/KfWfAbwRuLtt30CSJEmSOlzbrj6amVsj4nzg+8BI4NLMvCciLgIWZeaOgHgmcGVmZstrt0XEBcD/1OFvMfDVuvlbdVgMYCnw3nZ9B0mSJEnqdNGSxTrW/Pnzc9GiRU2XIUmSJEmNiIjFmTm/t7a23rxekiRJkjS0GQolSZIkqWCGQkmSJEkqmKFQkiRJkgpmKJQkSZKkghkKJUmSJKlghkJJkiRJKlgR9ymMiDXAb5uuoxcHAI81XYQa4diXy7Evk+NeLse+XI59uYbq2B+WmZN6aygiFA5VEbGorxtIqrM59uVy7MvkuJfLsS+XY1+u4Tj2bh+VJEmSpIIZCiVJkiSpYIbCZl3SdAFqjGNfLse+TI57uRz7cjn25Rp2Y+85hZIkSZJUMFcKJUmSJKlghsI2i4hLI2J1RNzdR3tExD9FxPKIuDMi5g12jWqPfoz9SRGxLiKW1j8XDnaNGngRcUhE3BwRyyLinoj4UC99nPcdqJ9j77zvQBGxR0TcGhF31GP/V730GRsRV9Xz/paImD74lWqg9XPsz46INS3z/l1N1KqBFxEjI+L2iLiul7ZhNedHNV1AAS4D/hn4Rh/trwdm1D8vBb5c/6nh7zKef+wBfpKZpw5OORokW4E/z8wlEbEPsDgibsrMe1v6OO87U3/GHpz3nWgzcHJmboiI0cD/RcSNmfmLlj7nAk9m5lERsRD4LPD2JorVgOrP2ANclZnnN1Cf2utDwDJg317ahtWcd6WwzTLzf4EnnqfL6cA3svILYEJETBmc6tRO/Rh7daDMXJWZS+rHT1H9spjao5vzvgP1c+zVgeq5vKF+Orr+6XnRhtOBy+vHVwN/FBExSCWqTfo59upAETENeAPwr310GVZz3lDYvKnAQy3PV+BfIkry8nrLyY0RcUzTxWhg1VtF5gK39Ghy3ne45xl7cN53pHob2VJgNXBTZvY57zNzK7AOmDi4Vaod+jH2AG+pTxe4OiIOGeQS1R6fA/4S2N5H+7Ca84bC5vX2Lwb+C1MZlgCHZeZLgC8A32u4Hg2giNgb+A7wZ5m5vmdzLy9x3neInYy9875DZea2zJwDTAMWRMSxPbo47ztUP8b+P4HpmTkb+G+6Vo80TEXEqcDqzFz8fN16OTZk57yhsHkrgNZ/MZoGrGyoFg2izFy/Y8tJZt4AjI6IAxouSwOgPq/kO8C3MvOaXro47zvUzsbeed/5MnMt8CPgdT2afj/vI2IUMB5PMegofY19Zj6emZvrp18Fjh/k0jTwXgGcFhG/Aa4ETo6If+vRZ1jNeUNh864F/rS+GuHLgHWZuarpotR+EXHQjr3lEbGAaj4+3mxVeqHqMf0asCwz/7GPbs77DtSfsXfed6aImBQRE+rH44A/Bn7Zo9u1wDvrx2cAP0xvFj3s9Wfse5wzfhrV+cYaxjLz45k5LTOnAwup5vOf9Og2rOa8Vx9ts4i4AjgJOCAiVgCfpDoJmcz8CnADcAqwHNgInNNMpRpo/Rj7M4D3RcRW4Blg4VD+Pwv12yuAs4C76nNMAD4BHArO+w7Xn7F33nemKcDlETGSKuh/OzOvi4iLgEWZeS3VPxh8MyKWU60WLGyuXA2g/oz9ByPiNKorFD8BnN1YtWqr4Tznw99FkiRJklQut49KkiRJUsEMhZIkSZJUMEOhJEmSJBXMUChJkiRJBTMUSpIkSVLBDIWSJEmSVDBDoSSpSBGREfHNluejImJNRFw3gJ9xdv2et0fEAxHx/Yg48QW835yIOKXl+aci4oKBqVaSVCpDoSSpVE8Dx0bEuPr5a4CH2/A5V2Xm3MycAXwGuCYiXryb7zUHOGWnvSRJ2gWGQklSyW4E3lA/PhO4YkdDRCyIiJ/Vq3w/i4ij6+MfiYhL68fHRcTdEbFnfz4sM28GLgHOq19/ZET8V0QsjoifRMTM+vhlEfGV+tj9EXFqRIwBLgLeHhFLI+Lt9dvOiogfRcSDEfHBF/4/iSSpNIZCSVLJrgQWRsQewGzglpa2XwJ/kJlzgQuBv6mPfw44KiLeBHwdeE9mbtyFz1wCzKwfXwJ8IDOPBy4AvtTSbzrwaqrQ+hWq39kXUq08zsnMq+p+M4HXAguAT0bE6F2oRZIkRjVdgCRJTcnMOyNiOtUq4Q09mscDl0fEDCCB0fVrtkfE2cCdwL9k5k938WMDICL2Bk4E/j0idrSNben37czcDjwQEQ/SFSR7uj4zNwObI2I1MBlYsYs1SZIKZiiUJJXuWuAfgJOAiS3H/xq4OTPfVAfHH7W0zQA2AAfvxufNBZZRrfytzcw5ffTLnTzfYXPL4234u12StIvcPipJKt2lwEWZeVeP4+PpuvDM2TsORsR44PPAHwATI+KM/n5QRLya6nzCr2bmeuDXEfHWui0i4iUt3d8aESMi4kjgCOA+4Clgn135cpIk7YyhUJJUtMxckZmf76Xp74C/jYifAiNbjl8MfCkz7wfOBT4TEQc+z0fsuDDM/cAngLdk5rK67R3AuRFxB3APcHrL6+4Dfkx1MZz3ZuYm4GaqC8u0XmhGkqQXJDL72o0iSZKaEBGXAddl5tVN1yJJ6nyuFEqSJElSwVwplCTpBYqIc4AP9Tj808x8fxP1SJK0KwyFkiRJklQwt49KkiRJUsEMhZIkSZJUMEOhJEmSJBXMUChJkiRJBTMUSpIkSVLB/h+imE/sIlFJ+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.errorbar(maxdepth_settings, lahat_training.mean(axis=1),\n",
    "             yerr=lahat_training.std(axis=1), label=\"training accuracy\")\n",
    "plt.errorbar(maxdepth_settings, lahat_test.mean(axis=1),\n",
    "             yerr=lahat_test.std(axis=1), label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Max_Depth\")\n",
    "plt.legend()\n",
    "bestdepth=np.argmax(lahat_test.mean(axis=1))+1\n",
    "print(\"Highest Average Test Set Achieved = %f\" % np.amax(lahat_test.mean(axis=1)))\n",
    "print(\"Max_Depth = %d\" %bestdepth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:42:05.651772Z",
     "start_time": "2021-01-10T19:42:05.647307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([            0,             1,             2,             3,\n",
       "                   4,  'add_(0, 1)', 'diff_(0, 1)',  'div_(0, 1)',\n",
       "        'mul_(0, 1)',  'add_(0, 2)', 'diff_(0, 2)',  'div_(0, 2)',\n",
       "        'mul_(0, 2)',  'add_(0, 3)', 'diff_(0, 3)',  'div_(0, 3)',\n",
       "        'mul_(0, 3)',  'add_(0, 4)', 'diff_(0, 4)',  'div_(0, 4)',\n",
       "        'mul_(0, 4)',  'add_(1, 2)', 'diff_(1, 2)',  'div_(1, 2)',\n",
       "        'mul_(1, 2)',  'add_(1, 3)', 'diff_(1, 3)',  'div_(1, 3)',\n",
       "        'mul_(1, 3)',  'add_(1, 4)', 'diff_(1, 4)',  'div_(1, 4)',\n",
       "        'mul_(1, 4)',  'add_(2, 3)', 'diff_(2, 3)',  'div_(2, 3)',\n",
       "        'mul_(2, 3)',  'add_(2, 4)', 'diff_(2, 4)',  'div_(2, 4)',\n",
       "        'mul_(2, 4)',  'add_(3, 4)', 'diff_(3, 4)',  'div_(3, 4)',\n",
       "        'mul_(3, 4)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:42:05.666442Z",
     "start_time": "2021-01-10T19:42:05.653217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1292787948319561, 'diff_(0, 4)'),\n",
       " (0.10168567010618015, 'diff_(0, 2)'),\n",
       " (0.08400916989403667, 'add_(0, 1)'),\n",
       " (0.08154019236623156, 'div_(0, 2)'),\n",
       " (0.07161529648151493, 'diff_(0, 3)'),\n",
       " (0.06841048414717524, 0),\n",
       " (0.05669209938333182, 'diff_(0, 1)'),\n",
       " (0.04120290867548418, 'add_(0, 4)'),\n",
       " (0.03995237456193179, 'add_(0, 3)'),\n",
       " (0.03529326430686815, 'div_(0, 3)'),\n",
       " (0.02824444073337055, 'div_(0, 4)'),\n",
       " (0.024309699403778345, 'add_(2, 3)'),\n",
       " (0.02262722261315606, 'mul_(2, 3)'),\n",
       " (0.01839858571849264, 'add_(2, 4)'),\n",
       " (0.0183014738371062, 'add_(3, 4)'),\n",
       " (0.017283283527955433, 'diff_(2, 4)'),\n",
       " (0.014585193602113545, 'diff_(1, 4)'),\n",
       " (0.012604996068057565, 'diff_(2, 3)'),\n",
       " (0.01224413284004215, 3),\n",
       " (0.012036485993943271, 'div_(2, 4)'),\n",
       " (0.010677982821942492, 'diff_(1, 2)'),\n",
       " (0.010278387692417663, 'mul_(3, 4)'),\n",
       " (0.00920084824622357, 2),\n",
       " (0.00813981920786492, 'add_(1, 2)'),\n",
       " (0.007719547128048608, 'diff_(1, 3)'),\n",
       " (0.00746364999216731, 4),\n",
       " (0.006908811840642879, 'diff_(3, 4)'),\n",
       " (0.006782648962208776, 'add_(1, 4)'),\n",
       " (0.006700290828565774, 'div_(1, 4)'),\n",
       " (0.006125766984306646, 'mul_(0, 2)'),\n",
       " (0.00423803100185635, 'div_(2, 3)'),\n",
       " (0.003366411127467082, 'mul_(2, 4)'),\n",
       " (0.0033079785956133994, 'mul_(0, 1)'),\n",
       " (0.0031444551620188766, 'add_(0, 2)'),\n",
       " (0.0027466442591817623, 'add_(1, 3)'),\n",
       " (0.0022175491644719925, 'mul_(1, 2)'),\n",
       " (0.0015697673095019338, 'div_(3, 4)'),\n",
       " (0.0014448215046609146, 1),\n",
       " (0.0014365218815774123, 'div_(0, 1)'),\n",
       " (0.0013692273241933364, 'mul_(1, 3)'),\n",
       " (0.001206366101144226, 'div_(1, 3)'),\n",
       " (0.0011596628396215855, 'mul_(1, 4)'),\n",
       " (0.0009494801533183941, 'mul_(0, 4)'),\n",
       " (0.0008458124048417777, 'div_(1, 2)'),\n",
       " (0.0006837483734160919, 'mul_(0, 3)')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(i,j) for i,j in zip(reg.feature_importances_, df_features.columns)], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:42:05.784238Z",
     "start_time": "2021-01-10T19:42:05.667823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 0, 0])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:42:05.787936Z",
     "start_time": "2021-01-10T19:42:05.785647Z"
    }
   },
   "outputs": [],
   "source": [
    "XX = df_features\n",
    "y = y_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:42:05.946772Z",
     "start_time": "2021-01-10T19:42:05.789512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12768     0   284     2     9]\n",
      " [ 1169     0    82     0     1]\n",
      " [ 1713     0   909    32    23]\n",
      " [   85     0   188    83    19]\n",
      " [   75     0   123    22   150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.808     0.977     0.884     13063\n",
      "           1      0.000     0.000     0.000      1252\n",
      "           2      0.573     0.340     0.426      2677\n",
      "           3      0.597     0.221     0.323       375\n",
      "           4      0.743     0.405     0.524       370\n",
      "\n",
      "    accuracy                          0.784     17737\n",
      "   macro avg      0.544     0.389     0.432     17737\n",
      "weighted avg      0.709     0.784     0.733     17737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_true = y_val_labels\n",
    "y_pred = reg.predict(df_features)\n",
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untouched Precision Recall F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:04:19.943273Z",
     "start_time": "2021-01-07T09:04:19.902720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7829028983872787, 0.7829028983872787, 0.7829028983872787, None)\n",
      "(0.517461496555691, 0.42250464611229105, 0.44358317115476015, None)\n",
      "(0.7226330765330707, 0.7829028983872787, 0.7381765527261192, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y_test_labels\n",
    "y_pred = y_test_predict\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:04:23.481367Z",
     "start_time": "2021-01-07T09:04:23.436671Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12645    33   328    15    35]\n",
      " [ 1119    11    91     6     4]\n",
      " [ 1536    14   902    95    66]\n",
      " [   91     1   178   122    40]\n",
      " [   57     0    97    44   204]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.819     0.969     0.887     13056\n",
      "           1      0.186     0.009     0.017      1231\n",
      "           2      0.565     0.345     0.429      2613\n",
      "           3      0.433     0.282     0.342       432\n",
      "           4      0.585     0.507     0.543       402\n",
      "\n",
      "    accuracy                          0.783     17734\n",
      "   macro avg      0.517     0.423     0.444     17734\n",
      "weighted avg      0.723     0.783     0.738     17734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of top model BALANCED ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model using different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:07:17.937027Z",
     "start_time": "2021-01-10T20:07:17.934159Z"
    }
   },
   "outputs": [],
   "source": [
    "#PCC\n",
    "#Specificity\n",
    "#Other S measure\n",
    "#Precision\n",
    "#Recall\n",
    "#Weighterd P and R\n",
    "#F1\n",
    "#regarding accuracy, sensitivity, and specificity \n",
    "#are 87.83%, 77.81%, and 93.88%, respectively, \n",
    "#which are better than 86.10%, 73.24%, and 93.81%, "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:06:05.460455Z",
     "start_time": "2021-01-06T16:06:05.457183Z"
    }
   },
   "source": [
    "Prepare test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:38.590555Z",
     "start_time": "2021-01-10T20:07:17.941635Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test image name and image vector dictionary\n",
    "test_data = (data.filter(lambda x: x['name'] in x_test.values)\n",
    "            .map(lambda x: {x['name']: np.asarray(x['value'])})\n",
    "            .fold(binop=lambda x, y: join_dict(x, y),\n",
    "                  combine=lambda x, y: join_dict(x, y))\n",
    "            .compute())\n",
    "\n",
    "#Test labels\n",
    "test_labels = pd.Series(y_test.values, index=x_test.values).to_dict()\n",
    "y_test_labels = [test_labels[i] for i in test_data.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:38.672306Z",
     "start_time": "2021-01-10T20:09:38.592085Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_toplayers():\n",
    "    \"\"\"Create top layer using inceptionb resnet v2 base\"\"\"\n",
    "    # Create input layer based on the output of the convolutional base\n",
    "    lyr_input = Input(shape=(base.layers[-1].output.shape.as_list()[1], ))\n",
    "\n",
    "    # Add Dense\n",
    "    lyr_dense1 = Dense(1024, activation='relu')(lyr_input)\n",
    "    lyr_dense2 = Dense(512, activation='relu')(lyr_dense1)\n",
    "    lyr_dense3 = Dense(256, activation='relu')(lyr_dense2)\n",
    "\n",
    "\n",
    "    # Create output layer\n",
    "    output = Dense(5, activation='softmax')(lyr_dense3)\n",
    "\n",
    "    model = Model(inputs=[lyr_input], outputs=[output])\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model_fp = 'tune/add_more_dense4_sigmoid.h5'\n",
    "model = create_toplayers()\n",
    "model.load_weights(model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:42.407780Z",
     "start_time": "2021-01-10T20:09:38.673761Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - 3s 5ms/step - loss: 0.9224 - categorical_accuracy: 0.7801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9224074482917786, 0.7801398634910583]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(list(test_data.values())), to_categorical(y_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:44.304604Z",
     "start_time": "2021-01-10T20:09:42.413626Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_predict = np.argmax(model.predict(np.array(list(test_data.values()))), \n",
    "                           axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T15:35:57.931469Z",
     "start_time": "2021-01-06T15:35:57.929395Z"
    }
   },
   "source": [
    "### PCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:44.320966Z",
     "start_time": "2021-01-10T20:09:44.311297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pcc : 20.00%\n",
      "Accuracy (1.25Pcc) required : 25.00%\n"
     ]
    }
   ],
   "source": [
    "values = np.bincount(y_train)\n",
    "Pcc = ((values/values.sum())**2).sum()\n",
    "print('Pcc : %.2f'%(Pcc*100) +'%')\n",
    "print('Accuracy (1.25Pcc) required : %.2f'%(Pcc*100*1.25) +'%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Assuming all are 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:44.488926Z",
     "start_time": "2021-01-10T20:09:44.326592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "np.array(list(Counter(y_train).values())) / sum(list(Counter(y_train).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Recall F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:44.539674Z",
     "start_time": "2021-01-10T20:09:44.494583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7801398443667531, 0.7801398443667531, 0.7801398443667531, None)\n",
      "(0.4873174984561026, 0.4129279093159141, 0.43712654675869966, None)\n",
      "(0.7036224783381455, 0.7801398443667531, 0.7308471774122896, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y_test_labels\n",
    "y_pred = y_test_predict\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T20:09:44.586648Z",
     "start_time": "2021-01-10T20:09:44.541854Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12706     0   305    14    31]\n",
      " [ 1136     0    91     0     4]\n",
      " [ 1652     0   804   116    41]\n",
      " [  100     0   164   143    25]\n",
      " [   67     0    94    59   182]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.811     0.973     0.885     13056\n",
      "           1      0.000     0.000     0.000      1231\n",
      "           2      0.551     0.308     0.395      2613\n",
      "           3      0.431     0.331     0.374       432\n",
      "           4      0.643     0.453     0.531       402\n",
      "\n",
      "    accuracy                          0.780     17734\n",
      "   macro avg      0.487     0.413     0.437     17734\n",
      "weighted avg      0.704     0.780     0.731     17734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p37)",
   "language": "python",
   "name": "conda_tensorflow_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "518px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
